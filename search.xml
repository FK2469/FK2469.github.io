<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[走向分布式（四）- GFS]]></title>
    <url>%2FGFS%2F</url>
    <content type="text"><![CDATA[GFS——Google分布式存储的基石。 GFS是构建在廉价服务器上的大型分布式系统。它将服务器故障视为正常现象，通过集成持续的监控、错误侦测、灾难冗余以及自动恢复的机制以实现自动容错。由于文件非常巨大，重新考虑了I/O 操作和 Block 的尺寸。针对海量文件的访问模式，客户端对数据块缓存是没有意义的，为了性能优化和原子性保证，GFS中绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。应用程序和文件系统 API 的协同设计提高了整个系统的灵活性。在保证系统可靠性和稳定性的同时，大幅降低了系统成本。 设计假设 系统是构建在很多廉价的、普通的组件上，组件会经常发生故障。它必须不间断监控自己、侦测错误，能够容错和快速恢复。系统存储了适当数量的大型文件，GFS预期几百万个，每个通常是100MB或者更大，即使是GB级别的文件也需要高效管理。也支持小文件，但是不需要着重优化。 系统主要面对两种读操作：大型流式读和小型随机读。在大型流式读中，单个操作会读取几百KB，也可以达到1MB或更多。相同客户端发起的连续操作通常是在一个文件读取一个连续的范围。小型随机读通常在特定的偏移位置上读取几KB。重视性能的应用程序通常会将它们的小型读批量打包、组织排序，能显著的提升性能。 也会面对大型的、连续的写，将数据append到文件。append数据的大小与一次读操作差不多。一旦写入，几乎不会被修改。不过在文件特定位置的小型写也是支持的，但没有着重优化。 系统必须保证多客户端对相同文件并发append的高效和原子性。GFS的文件通常用于制造者消费者队列或者多路合并。几百个机器运行的制造者，将并发的append到一个文件。用最小的同步代价实现原子性是关键所在。文件被append时也可能出现并发的读。 持久稳定的带宽比低延迟更重要。GFS更注重能够持续的、大批量的、高速度的处理海量数据，对某一次读写操作的回复时间要求没那么严格。接口GFS提供了一个非常亲切的文件系统接口，尽管它没有全量实现标准的POSIX API。像在本地磁盘中一样，GFS按层级目录来组织文件，一个文件路径（path）能作为一个文件的唯一ID。GFS支持常规文件操作，比如create、delete、open、close、read和write。 除了常规操作，GFS还提供快照和record append操作。快照可以用很低的花费为一个文件或者整个目录树创建一个副本。record append允许多个客户端并发的append数据到同一个文件，而且保证它们的原子性。这对于实现多路合并、制造消费者队列非常有用，大量的客户端能同时的append，也不用要考虑锁等同步问题。 系统架构GFS节点的三种角色: Master ChunkServer——CS ClientGFS文件被划分为固定大小的chunk，由Master在创建时分配一个64位、全局唯一的chunk句柄。CS以普通的Linux文件的形式将chunk存储在磁盘中。为保证可靠性，chunk在不同的机器中复制多份，默认为三份。 主控服务器中维护了系统的元数据，包括文件及chunk命名空间、文件到chunk之间的映射、chunk位置信息。它也负责整个系统的全局控制，如chunk租约管理、垃圾回收无用chunk、chunk复制等。Master会定期与CS通过心跳的方式交换信息。 GFS 客户端代码以库的形式被链接到客户程序里。客户端代码实现了 GFS 文件系统的 API 接口函数、应用程序与 Master 节点和 Chunk 服务器通讯、以及对数据进行读写操作。客户端和 Master 节点的通信只获取元数据，所有的数据操作都是由客户端直接和 Chunk 服务器进行交互的。GFS不提供 POSIX 标准的 API 的功能，因此，GFS API调用不需要深入到Linux vnode 级别。客户端访问GFS时，首先访问Master，获取与之交互的CS信息，然后直接访问这些CS，完成数据存取工作。 GFS中的Client不缓存文件数据，只缓存Master中获取的元数据，这是由GFS的应用特点决定的。GFS的主要应用就是MapReduce与Bigtable。对于MapReduce,GFS client使用方式为顺序读写，没有缓存文件数据的必要;而Bigtable作为分布式表格系统，内部实现了一套缓存机制。何况，如何维护客户端缓存与实际数据之间的一致性是一个极其复杂的问题。chunkserver不需要缓存文件数据因为chunk被存储为本地文件，Linux提供的OS层面的buffer缓存已经保存了频繁访问的文件。 关键问题租约机制GFS数据追加以记录为单位，每个记录的大小为几十KB到几MB不等，如果每次记录追加都需要请求Master，那么Master显然会成为系统的性能瓶颈。因此GFS通过租约(lease)机制将chunk写操作授权给chunk。拥有租约授权的CS称为主CS，其他副本所在的CS称为备CS。租约授权针对单个chunk，在租约有效期内，对该chunk的写操作都由主ChunkServer负责，从而减轻Master的负载。一般来说，租约的有效期比较长，比如60秒，只要没有出现异常，主CS可以不断向Master请求延长租约的有效期直到整个chunk写满。 假设chunk A在GFS中保存了三个副本——A1(主)、A2、A3。若A2所在CS下线后由重新上线，并且在A2下线的过程中，副本A1和A3有更新，那么A2需要被Master当成垃圾回收掉。GFS通过对每个chunk维护一个版本号来解决，每次给CS进行租约授权或者主CS重新延长有效期时，Master会将chunk的版本号加1。对应该场景，A2的版本号太低，从而被标记成可删除的chunk。 一致性模型GFS 支持一个宽松的一致性模型，这个模型能够很好的支撑GFS的高度分布的应用，同时还保持了相对简单且容易实现的优点。本节GFS讨论 GFS 的一致性的保障机制，以及对应用程序的意义。 GFS一致性保障机制文件命名空间的修改（例如，文件创建）是原子性的。它们仅由 Master 节点的控制：命名空间锁提供了原子性和正确性的保障；Master 节点的操作日志定义了这些操作在全局的顺序。数据修改后文件 region的状态取决于操作的类型、成功与否、以及是否同步修改。表 1 总结了各种操作的结果。 先来看看两种操作结果: 如果所有客户端，无论从哪个副本读取，读到的数据都一样，那么我们认为文件 region 是“一致的”； 如果对文件的数据修改之后，region 是一致的，并且客户端能够看到写入操作全部的内容，那么这个 region是“已定义的”。 当一个数据修改操作成功执行，并且没有受到同时执行的其它写入操作的干扰，那么影响的 region 就是已定义的（隐含了一致性）：所有的客户端都可以看到写入的内容。 并行修改操作成功完成之后，region 处于一致的、未定义的状态：所有的客户端看到同样的数据，但是无法读到任何一次写入操作写入的数据。通常情况下，文件 region 内包含了来自多个修改操作的、混杂的数据片段。失败的修改操作导致一个 region 处于不一致状态（同时也是未定义的）：不同的客户在不同的时间会看到不同的数据。后面我们将描述应用如何区分已定义和未定义的 region。应用程序没有必要再去细分未定义 region 的不同类型。 再看看两种操作:GFS主要是为了追加(append)而不是改写(overwrite)而设计的。一方面是因为改写的需求比较少，或者可以通过追加来实现。比如可以只使用GFS的追加功能构建分布式表格系统Bigtable;另一方面是因为追加的一致性模型相比改写更加简单有效。若改写失败，之后读操作可能读到不正确的数据；若追加失败，读操作只是读到过期而不是错误的数据。因为改写操作指定一个变动发生的偏移量，而追加记录操作不会。在前一种情况下，变动的位置是预先确定的，而后一种情况下，是由系统决定的。并发写入到相同位置是不可序列化的，还可能导致文件区域的损坏。 对于追加记录操作，GFS保证追加操作会至少发生一次且是原子的(即作为一个连续的字节序列)，但系统并不保证块的所有副本相同。若出现异常，失败的副本可能会出现一些可识别的填充(padding)记录或者重复数据。这些数据占据的文件 region 被认定是不一致的，这些数据通常比用户数据小的多。这里的复制策略是考虑到GFS的应用程序和服务可以容忍一致性放松后的语义，是针对特定领域的，且削弱了一致性的保证。 追加流程 客户机向 Master 节点询问哪一个 Chunk 服务器持有当前的租约，以及其它副本的位置。如果没有一个Chunk 持有租约，Master 节点就选择其中一个副本建立一个租约（这个步骤在图上没有显示）。 Master 节点将主 Chunk 的标识符以及其它副本（又称为 secondary 副本、二级副本）的位置返回给客户机。客户机缓存这些数据以便后续的操作。只有在主 Chunk 不可用，或者主 Chunk 回复信息表明它已不再持有租约的时候，客户机才需要重新跟 Master 节点联系。 客户机把数据推送到所有的副本上。客户机可以以任意的顺序推送数据。Chunk 服务器接收到数据并保存在它的内部 LRU 缓存中，一直到数据被使用或者过期交换出去。由于数据流的网络传输负载非常高，通过分离数据流和控制流，GFS可以基于网络拓扑情况对数据流进行规划，提高系统性能，而不用去理会哪个Chunk 服务器保存了主 Chunk。 当所有的副本都确认接收到了数据，客户机发送写请求到主 Chunk 服务器。这个请求标识了早前推送到所有副本的数据。主 Chunk 为接收到的所有操作分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。它以序列号的顺序把操作应用到它自己的本地状态中（alex 注：也就是在本地执行这些操作，这句话按字面翻译有点费解，也许应该翻译为“它顺序执行这些操作，并更新自己的状态”）。 主 Chunk 把写请求传递到所有的二级副本。每个二级副本依照主 Chunk 分配的序列号以相同的顺序执行这些操作。 所有的二级副本回复主 Chunk，它们已经完成了操作。 主 Chunk 服务器20回复客户机。任何副本产生的任何错误都会返回给客户机。在出现错误的情况下，写入操作可能在主 Chunk 和一些二级副本执行成功。（如果操作在主 Chunk 上失败了，操作就不会被分配序列号，也不会被传递。）客户端的请求被确认为失败，被修改的 region 处于不一致的状态。GFS的客户机代码通过重复执行失败的操作来处理这样的错误。在从头开始重复执行之前，客户机会先从步骤3到步骤7做几次尝试。 如果应用程序一次写入的数据量很大，或者数据跨越了多个 Chunk，GFS 客户机代码会把它们分成多个写操作。这些操作都遵循前面描述的控制流程，但是可能会被其它客户机上同时进行的操作打断或者覆盖。因此，共享的文件 region 的尾部可能包含来自不同客户机的数据片段，尽管如此，由于这些分解后的写入操作在所有的副本上都以相同的顺序执行完成，Chunk 的所有副本都是一致的。这使文件 region 处于一致的、但是未定义的状态。 提高网络效率分离控制流和数据流！ 目标:充分利用每台机器的带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时。 为了充分利用每台机器的带宽:数据沿着一个Chunk服务器链顺序的推送，而不是以其他拓扑形式分散推送(如，树形拓扑结构)。线性推送模式下，每台机器所有的出口带宽都用于以最快的速度传输数据，而不是在多个接收者之间分配带宽。 为了尽可能地避免出现网络瓶颈和高延迟的链接:每台机器都尽量地在网络拓扑中选择一台还没有接收到数据的、离自己最近的机器作为目标推送数据。由于网络拓扑简单，通过IP地址就可以计算出节点的”距离”。 最小化延迟:利用基于TCP连接的、管道式数据推送方式来减少延时。当一个ChunkServer接收到一些数据，它就立即开始转发。由于采取全双工网络，立即发送数据并不会降低接收数据的速率。抛开网络阻塞，传输B个字节到R个副本的理想时间是B/T+RL，T是网络吞吐量，L是在两台机器数据传输的延迟。比如网速100Mbps,L远小于1ms的情况下，1MB数据80ms左右就能分发出去。 容错机制和检验高可用Google使用两条简单但有效的策略保证整个系统的高可用性:快速恢复和复制。 快速恢复master和chunkserver都被设计成都能在秒级别重启。 Master复制通过操作日志和checkpoint的方式进行，并且有一台称为”Shadow Master”的实时热备。 master的operation log和checkpoint都会复制到多台机器上，要保证这些机器的写都成功了，才认为是成功。只有一台master在来做garbage collection等后台操作。当master挂掉后，它能在很多时间内重启；当master所在的机器挂掉后，监控会在其他具有operation log的机器上重启启动master。 新启动的master只提供读服务，因为可能在挂掉的一瞬间，有些日志记录到primary master上，而没有记录到secondary master上（这里GFS没有具体说同步的流程）。 Chunk复制每个chunk在多个机架上有副本，副本数量由用户来指定。当chunkserver不可用时，GFS master会自动的复制副本，保证副本数量和用户指定的一致。 数据完整性每个chunkserver都会通过checksum来验证数据是否损坏的。 每个chunk被分成多个64KB的block，每个block有32位的checksum，checksum在内存中和磁盘的log中都有记录。 对于读请求，chunkserver会检查读操作所涉及block的所有checksum值是否正确，如果有一个block的checksum不对，那么会报错给client和master。client这时会从其他副本读数据，而master会clone一个新副本，当新副本clone好后，master会删除掉这个checksum出错的副本。 诊断工具主要是通过log，包括重要事件的log(chunkserver上下线)，RPC请求，RPC响应等。 Master设计Master内存占用Master在内存中维护了系统中的全部元数据，包括文件及chunk命名空间、文件到chunk之间的映射、chunk副本的位置信息。其中前两者需要以记录变更的方式持久化到磁盘，chunk副本位置信息不需要持久化，可通过ChunkServer汇报获取。 貌似将元数据全部保存在内存中的方法有问题，因为Chunk数量以及整个系统的承载能力都受限于Master服务器所拥有的内存大小。但实际应用中，Master对命名空间进行了压缩存储。压缩后每个文件在文件命名空间的元数据也不超过64字节，由于GFS中的文件一般都申大文件，因此文件命名空间占用内存不多。故，Master内存容量不会称为GFS的系统瓶颈。 即便需要支持更大的系统，为Master服务器增加额外内存的费用是很少的，而通过增加有限的费用，我们就能把元数据全部保存在内存里，增强了系统的简洁性、可靠性、高性能和灵活性。 创建、重新复制、负载均衡GFS中副本分布策略需要考虑多种因素，如网络拓扑、机架分布、磁盘利用率等。为了提高系统的可用性，GFS会避免将同一个chunk的所有副本都存放在同一个机架的情况。 需要创建chunk副本的情况有三种:chunk创建、chunk复制、以及负载均衡。 chunk创建GFS在创建chunk的时候，选择chunkserver时考虑的因素包括： 磁盘空间使用率低于平均值的chunkserver。 限制每台chunkserver的最近的创建chunk的次数，因为创建chunk往往意味着后续需要写大量数据，所以，应该把写流量尽量均摊到每台chunkserver上。 chunk的副本放在处于不同机架的chunkserver上。chunk复制当一个chunk的副本数量少于预设定的数量时，需要做复制的操作，例如，chunkserver宕机，副本数据出错，磁盘损坏，或者设定的副本数量增加。 chunk的复制的优先级是按照下面的因素来确定的： 丢失两个副本的chunk比丢失一个副本的chunk的复制认为优先级高 文件正在使用比文件已被删除的chunk的优先级高 阻塞了client进程的chunk的优先级高 chunk复制的时候，选择新chunkserver要考虑的点： 磁盘使用率 单个chunkserver的复制个数限 多个副本需要在多个机架 集群的复制个数限制 限制每个chunkserver的复制网络带宽，通过限制读流量的速率来限制负载均衡周期性地检查副本分布情况，然后调整到更好的磁盘使用情况和负载均衡。GFS master对于新加入的chunkserver，逐渐地迁移副本到上面，防止新chunkserver带宽打满。垃圾回收在GFS删除一个文件后，并不会马上就对文件物理删除，而是在后面的定期清理的过程中才真正的删除。 具体地，对于一个删除操作，GFS仅仅是写一条日志记录，然后把文件命名成一个对外部不可见的名称，这个名称会包含删除的时间戳。GFS master会定期的扫描，当这些文件存在超过3天后，这些文件会从namespace中删掉，并且内存的中metadata会被删除。 在对chunk namespace的定期扫描时，会扫描到这些文件已经被删除的chunk，然后会把metadata从磁盘中删除。 在与chunkserver的heartbeat的交互过程中，GFS master会把不在metadata中的chunk告诉chunkserver，然后chunkserver就可以删除这些chunk了。 采用这种方式删除的好处： 利用心跳方式交互，在一次删除失败后，还可以通过下次心跳继续重试操作 删除操作和其他的全局扫描metadata的操作可以放到一起做 坏处： 有可能有的应用需要频繁的创建和删除文件，这种延期删除方式会导致磁盘使用率偏高，GFS提供的解决方案是，对一个文件调用删除操作两次，GFS会马上做物理删除操作，释放空间。快照Snapshot的整个流程如下： client向GFS master发送Snapshot请求。 GFS master收到请求后，会回收所有这次Snapshot涉及到的chunk的lease。 当所有回收的lease到期后，GFS master写入一条日志，记录这个信息。然后，GFS会在内存中复制一份snapshot涉及到的metadata。 当snapshot操作完成后，client写snapshot中涉及到的chunk C的流程如下: client向GFS master请求primary chunkserver和其他chunkserver。 GFS master发现chunk C的引用计数超过1，即snapshot和本身。它会向所有有chunk C副本的chunkserver发送创建一个chunk C的拷贝请求，记作是chunk C’，这样，把最新数据写入到chunk C’即可。本质上是copy on write。 ChunkServer设计ChunkServer管理大小约为64MB的chunk，存储的时候需要保证chunk尽可能均匀地分布在不同的磁盘之中，需要考虑的因素包括:磁盘空间、最近新建chunk数等。另外，Linux文件系统删除64MB大文件消耗的时间太长且没有必要，因此，删除chunk的时候可只将其对应的chunk文件移动到磁盘回收站。以后新建chunk时可重用。 ChunkServer是一个磁盘和网络I/O密集型应用，为了最大限度地发挥机器性能，需要能够做到将磁盘和网络操作异步化。 观感评价GFS是一个具有良好可扩展性并能够在软件层面自动处理各种异常情况的系统。由于Google的系统一开始能很好地解决可扩展性的问题，所以后续的系统能够构建在前一个系统之上并且一步一步引入新的功能。Bigtable在GFS之上将海量数据组织成表格形式，Megastore、Spanner又进一步在Bigtable之上融合一些关系型数据库的功能。 自动化容错是GFS的亮点所在，在设计GFS时认为节点失效是常态，通过在软件层面进行故障检测，并且通过chunk复制操作将原有故障节点的服务迁移到新的节点。系统还会根据一定的策略，如磁盘使用情况、机器负载等执行负载均衡操作。由于软件层面的自动化容错，底层可采用廉价的硬件，从而大大降低云服务的成本。 单Master设计是可行的。事实上，绝大多数分布式存储系统都和GFS一样依赖单总控节点。其中用于满足Master元数据管理的写时复制B树实现相当复杂。 问答根据《大规模分布式存储系统》中的一组问题，来检验一下学习成果！ 1）为什么存储三个副本？而不是两个或者四个？两个副本不足以保证高可用性。四个副本成本较高。 2）Chunk的大小为何选择64MB？这个选择主要基于哪些考虑？优点 可以减少GFS client和GFS master的交互次数，chunk size比较大的时候，多次读可能是一块chunk的数据，这样，可以减少GFS client向GFS master请求chunk位置信息的请求次数。 对于同一个chunk，GFS client可以和GFS chunkserver之间保持持久连接，提升读的性能。 chunk size越大，chunk的metadata的总大小就越小，使得chunk相关的metadata可以存储在GFS master的内存中。 缺点 chunk size越大时，可能对部分文件来讲只有1个chunk，那么这个时候对该文件的读写就会落到一个GFS chunkserver上，成为热点。 64MB应该是google得出的一个比较好的权衡优缺点的经验值。 3）GFS主要支持追加（append）、改写（overwrite）操作比较少。为什么这样设计？如何基于一个仅支持追加操作的文件系统构建分布式表格系统Bigtable？该特性是Google根据现有应用需求而确定的。 4）为什么要将数据流和控制流分开？如果不分开，如何实现追加流程？更有效地利用网络带宽。 5）GFS有时会出现重复记录或者补零记录（padding），为什么？ padding出现场景： last chunk的剩余空间不满足当前写入量大小，需要把last chunk做padding，然后告诉客户端写入下一个chunk append操作失败的时候，需要把之前写入失败的副本padding对齐到master 重复记录出现场景： append操作部分副本成功，部分失败，然后告诉客户端重试，客户端会在成功的副本上再次append，这样就会有重复记录出现 6）租约（Lease）是什么？在GFS起什么作用？它与心跳（heartbeat）有何区别？lease是gfs master把控制写入顺序的权限下放给chunkserver的机制，以减少gfs master在读写流程中的参与度，防止其成为系统瓶颈。心跳是gfs master检测chunkserver是否可用的标志。 7）GFS追加操作过程中如果备副本（Secondary）出现故障，如何处理？如果主副本（Primary）出现故障，如何处理？对于备副本故障，写入的时候会失败，然后primary会返回错误给client。按照一般的系统设计，client会重试一定次数，发现还是失败，这时候client会把情况告诉给gfs master，gfs master可以检测chunkserver的情况，然后把最新的chunkserver信息同步给client，client端再继续重试。 对于主副本故障，写入的时候会失败，client端应该是超时了。client端会继续重试一定次数，发现还是一直超时，那么把情况告诉给gfs master，gfs master发现primary挂掉，会重新grant lease到其他chunkserver，并把情况返回给client。 8）GFS Master需要存储哪些信息？Master数据结构如何设计？namespace、文件到chunk的映射以及chunk的位置信息 namespace采用的是B-Tree，对于名称采用前缀压缩的方法，节省空间；（文件名，chunk index）到chunk的映射，可以通过hashmap；chunk到chunk的位置信息，可以用multi_hashmap，因为是一对多的映射。 9）假设服务一千万个文件，每个文件1GB,Master中存储的元数据大概占用多少内存？1GB/64MB = 1024 / 64 = 16。总共需要16 10000000 64 B = 10GB 10）Master如何实现高可用性？ metadata中namespace，以及文件到chunk信息持久化，并存储到多台机器 对metadata的做checkpoint，保证重启后replay消耗时间比较短，checkpoint可以直接映射到内存使用，不用解析 在primary master发生故障的时候，并且无法重启时，会有外部监控将secondary master，并提供读服务。secondary master也会监控chunkserver的状态，然后把primary master的日志replay到内存中 11）负载的影响因素有哪些？如何计算一台机器的负载值？主要是考虑CPU、内存、网络和I/O，但如何综合这些参数并计算还是得看具体的场景，每部分的权重随场景的不同而不同。 12）Master新建chunk时如何选择ChunkServer？如果新机器上线，负载值特别低，如何避免其他ChunkServer同时往这台机器迁移chunk？ 13）如果某台ChunkServer报废，GFS如何处理？RE-BALANCE。当有 Chunk服务器离线了，或者通过 Chksum 校验（参考5.2节）发现了已经损坏的数据，Master节点通过克隆已有的副本保证每个 Chunk 都被完整复制 。 14）如果ChunkServer下线后过一会重新上线，GFS如何处理？因为是过一会，所以假设chunk re-replication还没有执行，那么在这期间，可能这台chunkserver上有些chunk的数据已经处于落后状态了，client读数据的时候或者chunkserver定期扫描的时候会把这些状态告诉给master，master告诉上线后的chunkserver从其他机器复制该chunk，然后master会把这个chunk当作是垃圾清理掉。 对于没有落后的chunk副本，可以直接用于使用。 15）如何实现分布式文件系统的快照操作？ 16）ChunkServer数据结构如何设计？chunkserver主要是存储64KB block的checksum信息，需要由chunk+offset，能够快速定位到checksum，可以用hashmap。 17）磁盘可能出现“位翻转”错误，ChunkServer如何应对？利用checksum机制，分读和写两种情况来讨论： 对于读，要检查所读的所有block的checksum值。对于写，分为append和write。对于append，不检查checksum，延迟到读的时候检查，因为append的时候，对于最后一个不完整的block计算checksum时候采用的是增量的计算，即使前面存在错误，也能在后来的读发现。对于overwrite，因为不能采用增量计算，要覆盖checksum，所以，必须要先检查只写入部分数据的checksum是否不一致，否则，数据错误会被隐藏。 18）ChunkServer重启后可能有一些过期的chunk,Master如何能够发现？chunkserver重启后，会汇报chunk及其version number，master根据version number来判断是否过期。如果过期了，那么会做以下操作： 过期的chunk不参与数据读写流程 master会告诉chunkserver从其他的最新副本里拷贝一份数据 master将过期的chunk假如garbage collection中 问题：如果chunkserver拷贝数据的过程过程中，之前拷贝的数据备份又发生了变化，然后分为两种情况讨论： 如果期间lease没变，那么chunkserver不知道自己拷贝的数据是老的，应该会存在不一致的问题？ 如果期间lease改变，那么chunkserver因为还不能提供读服务，那么version number应该不会递增，继续保持stable状态，然后再发起拷贝。 继任者——Colossus Master将单一主控服务器改造为多主控服务器构成的集群，将所有管理数据进行数据分片后分配到不同的主控服务器中。这样水平扩展性得到极大增强。 Chunk Server通过应用纠删码算法，在减少备份数目的情况下达到类似的高可用性要求。 Client使得客户端可以管理备份数据的存储地点，对于提高I/O效率很有帮助。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走向分布式（三）- Go&Concurrency]]></title>
    <url>%2FGo%26Concurrency%2F</url>
    <content type="text"><![CDATA[Goroutine和Channel擦出的火花，使Go得以轻松应对数以万计的并发逻辑。 并发简略介绍Golang 最大卖点就是原生支持并发。和传统基于 OS 线程和进程实现不同，Go 语言的并发是基于用户态的并发，这种并发方式就变得非常轻量，能够轻松运行几万甚至是几十万的并发逻辑。因此使用 Go 开发的服务端应用采用的就是“协程模型”，每一个请求由独立的协程处理完成。 比进程线程模型高出几个数量级的并发能力，而相对基于事件回调的服务端模型，Go 开发思路更加符合人的逻辑处理思维，因此即使使用 Go 开发大型的项目，也很容易维护。 详细特性 goroutine是Go语言运行库的功能，不是操作系统提供的功能，goroutine不是用线程实现的。具体可参见Go语言源码里的pkg/runtime/proc.c goroutine就是一段代码，一个函数入口，以及在堆上为其分配的一个堆栈。所以它非常廉价，我们可以很轻松的创建上万个goroutine，但它们并不是被操作系统所调度执行。 除了被系统调用阻塞的线程外，Go运行库最多会启动$GOMAXPROCS个线程来运行goroutine。 goroutine是协作式调度的，如果goroutine会执行很长时间，而且不是通过等待读取或写入channel的数据来同步的话，就需要主动调用Gosched()来让出CPU。 和所有其他并发框架里的协程一样，goroutine里所谓“无锁”的优点只在单线程下有效，如果$GOMAXPROCS &gt; 1并且协程间需要通信，Go运行库会负责加锁保护数据，这也是为什么sieve.go这样的例子在多CPU多线程时反而更慢的原因。 Web等服务端程序要处理的请求从本质上来讲是并行处理的问题，每个请求基本独立，互不依赖，几乎没有数据交互，这不是一个并发编程的模型，而并发编程框架只是解决了其语义表述的复杂性，并不是从根本上提高处理的效率，也许是并发连接和并发编程的英文都是concurrent吧，很容易产生“并发编程框架和coroutine可以高效处理大量并发连接”的误解。 Go语言运行库封装了异步IO，所以可以写出貌似并发数很多的服务端，可即使我们通过调整$GOMAXPROCS来充分利用多核CPU并行处理，其效率也不如我们利用IO事件驱动设计的、按照事务类型划分好合适比例的线程池。在响应时间上，协作式调度是硬伤。即便它更符合我们的我们的处理思维。 goroutine最大的价值是其实现了并发协程和实际并行执行的线程的映射以及动态扩展，随着其运行库的不断发展和完善，其性能一定会越来越好，尤其是在CPU核数越来越多的未来，终有一天我们会为了代码的简洁和可维护性而放弃那一点点性能的差别。 并发模型Go的Goroutine 和 Channel属于 CSP 并发模型的一种实现，CSP 并发模型的核心概念是：“不要通过共享内存来通信，而应该通过通信来共享内存”。在1978发表的 CSP 论文中有一段使用 CSP 思路解决问题的描述: “Problem: To print in ascending order all primes less than 10000. Use an array of processes, SIEVE, in which each process inputs a prime from its predecessor and prints it. The process then inputs an ascending stream of numbers from its predecessor and passes them on to its successor, suppressing any that are multiples of the original prime.” 要找出10000以内所有的素数，这里使用的方法是筛法，即从2开始每找到一个素数就标记所有能被该素数整除的所有数。直到没有可标记的数，剩下的就都是素数。下面以找出10以内所有素数为例，借用 CSP 方式解决这个问题。 从上图中可以看出，每一行过滤使用独立的并发处理程序，上下相邻的并发处理程序传递数据实现通信。通过4个并发处理程序得出10以内的素数表，对应的 Go 实现代码如下： 12345678910111213141516171819202122232425262728func main()&#123; origin, wait := make(chan int), make(chan struct&#123;&#125;) Processor(origin, wait) for num := 2; num &lt; 10000; num++ &#123; origin &lt;- num &#125; close(origin) &lt;- wait&#125;func Processor(seq chan int, wait chan struct&#123;&#125;)&#123; go func()&#123; prime, ok := &lt;-seq if !ok &#123; close(wait) return &#125; fmt.Println(prime) out := make(chan int) Processor(out, wait) for num := range seq&#123; if num%prime != 0&#123; out &lt;- num &#125; &#125; close(out) &#125;()&#125; 当然，如同之前我们讨论过的MapReduce一样，这些编程模型只代表了一种理念和思维方式，具体实现以后是语言的特性或项目的解决方案，使用时仍然应该按照具体场景来分析。 事实上Go从来也不排斥共享内存，当然也不鼓励滥用Channel。Go的wiki就有一页专门说这件事。 其中有句话让我很受启发，与君共勉：Go is pragmatic in letting you use the tools that solve your problem best and not forcing you into one style of code. Channel的应用场景：传递数据的所有权,分配工作单位,通信异步结果。 Mutex的应用场景：缓存,状态。 另一个重要的同步原语是sync.WaitGroup。它允许合作的goroutines共同等待一个阈值事件, 然后再独立地继续。这在两种情况下很有用：第一个，“清理”的场景。一个sync.WaitGroup 可以用来确保所有的goroutines(包括主协程)等待，直到所有协程终止干净。第二个更一般的情况是一个循环算法, 它涉及一组 goroutines, 它们都独立工作一段时间, 然后在一个屏障上等待, 然后再独立进行。此模式可能重复多次。数据在障碍事件可能被交换。该策略是批量同步并行 (BSP-Bulk Synchronous Parallelism) 的基础。 以上内容是我整理的Go语言机制及其背后并发模型的影子。 对于并发模型这个概念，我目前还是停留在很简单的语用层面。从代数的层面讲，其实Communicating Sequential Processes 是一门语言，有它的形式语法等。而Go只不过是借鉴了一些概念，和 Plan9 libthread, DirectShow … 一样。但用 Golang 写的程序是不能用 CSP 工具分析的, Golang 编译器也没这样的分析, 所以依然无法杜绝死锁/活锁 (Go 编译器有一个 -race 检测, 不过和 CSP 的分析是两回事)。 既然都提到这么多并发模型了，那不如再聊聊Actors模型。它主要使用消息机制来实现并发，目标是让开发者不再考虑进程/线程，每个Actor最多只能同时进行一样工作，Actor内部可以有自己的变量和数据。 Actors模型避免了由操作系统进行任务调度的问题，在操作系统进程之上，多个Actor可能运行在同一个进程(或线程)中。这就节省了大量的Context切换。 在Actors模型中，每个Actor都有一个专属的命名”邮箱”, 其他Actor可以随时选择一个Actor通过邮箱收发数据,对于“邮箱”的维护，通常是使用发布订阅的机制实现的，比如我们可以定义发布者是自己，订阅者可以是某个Socket接口，另外的消息总线或者直接是目标Actor。 目前akka库是比较流行的Actors编程模型实现，支持Scala和Java语言。 CSP和Actor的区别: CSP进程通常是同步的(即任务被推送进Channel就立即执行，如果任务执行的线程正忙，则发送者就暂时无法推送新任务)，Actor进程通常是异步的(消息传递给Actor后并不一定马上执行)。 CSP中的Channel通常是匿名的, 即任务放进Channel之后你并不需要知道是哪个Channel在执行任务，而Actor是有“身份”的，你可以明确的知道哪个Actor在执行任务。 在CSP中，我们只能通过Channel在任务间传递消息, 在Actor中我们可以直接从一个Actor往另一个Actor传输数据.CSP中消息的交互是同步的，Actor中支持异步的消息交互。 而Actor有而CSP没有的一个很棒的特性就是其容错机制。根据在 Erlang 中设计的 OTP 规范将 Actors 组织成一个监督层次结构, 我们可以在应用程序中构建一个失败域。从而将错误模型化，不同层次的错误该如何应对都可以建模。 并发控制当并发成为语言的原生特性之后，在实践过程中就会频繁地使用并发来处理逻辑问题，尤其是涉及到网络I/O的过程，例如 RPC 调用，数据库访问等。下述内容整理自今日头条技术团队的博文——《今日头条Go建千亿级微服务的实践》 下图是一个微服务处理请求的抽象描述： 如图所示，当 Request 到达 GW 之后，GW 需要整合下游5个服务的结果来响应本次的请求，假定对下游5个服务的调用不存在互相的数据依赖问题。那么这里会同时发起5个 RPC 请求，然后等待5个请求的返回结果。为避免长时间的等待，这里会引入等待超时的概念。超时事件发生后，为了避免资源泄漏，会发送事件给正在并发处理的请求。在实践过程中，今日头条技术团队得出两种抽象的模型。 Wait Cancel Wait和Cancel两种并发控制方式，在使用 Go 开发服务的时候到处都有体现，只要使用了并发就会用到这两种模式。在上面的例子中，GW 启动5个协程发起5个并行的 RPC 调用之后，主协程就会进入等待状态，需要等待这5次 RPC 调用的返回结果，这就是 Wait 模式。另一中 Cancel 模式，在5次 RPC 调用返回之前，已经到达本次请求处理的总超时时间，这时候就需要 Cancel 所有未完成的 RPC 请求，提前结束协程。Wait 模式使用会比较广泛一些，而对于 Cancel 模式主要体现在超时控制和资源回收。 在 Go 语言中，分别有 sync.WaitGroup 和 context.Context 来实现这两种模式。 超时控制合理的超时控制在构建可靠的大规模微服务架构显得非常重要，不合理的超时设置或者超时设置失效将会引起整个调用链上的服务雪崩。 图中被依赖的服务G由于某种原因导致响应比较慢，因此上游服务的请求都会阻塞在服务G的调用上。如果此时上游服务没有合理的超时控制，导致请求阻塞在服务G上无法释放，那么上游服务自身也会受到影响，进一步影响到整个调用链上各个服务。 在 Go 语言中，Server 的模型是“协程模型”，即一个协程处理一个请求。如果当前请求处理过程因为依赖服务响应慢阻塞，那么很容易会在短时间内堆积起大量的协程。每个协程都会因为处理逻辑的不同而占用不同大小的内存，当协程数据激增，服务进程很快就会消耗大量的内存。 协程暴涨和内存使用激增会加剧 Go 调度器和运行时 GC 的负担，进而再次影响服务的处理能力，这种恶性循环会导致整个服务不可用。今日头条团队在使用 Go 开发微服务的过程中，曾多次出现过类似的问题，并称之为协程暴涨。 有没有好的办法来解决这个问题呢？通常出现这种问题的原因是网络调用阻塞过长。即使在我们合理设置网络超时之后，偶尔还是会出现超时限制不住的情况，对 Go 语言中如何使用超时控制进行分析，首先我们来看下一次网络调用的过程。 第一步，建立 TCP 连接，通常会设置一个连接超时时间来保证建立连接的过程不会被无限阻塞。 第二步，把序列化后的 Request 数据写入到 Socket 中，为了确保写数据的过程不会一直阻塞，Go 语言提供了 SetWriteDeadline 的方法，控制数据写入 Socket 的超时时间。根据 Request 的数据量大小，可能需要多次写 Socket 的操作，并且为了提高效率会采用边序列化边写入的方式。因此在 Thrift 库的实现中每次写 Socket 之前都会重新 Reset 超时时间。 第三步，从 Socket 中读取返回的结果，和写入一样， Go 语言也提供了 SetReadDeadline 接口，由于读数据也存在读取多次的情况，因此同样会在每次读取数据之前 Reset 超时时间。 分析上面的过程可以发现影响一次 RPC 耗费的总时间的长短由三部分组成：连接超时，写超时，读超时。而且读和写超时可能存在多次，这就导致超时限制不住情况的发生。为了解决这个问题，今日头条团队在其内部框架——kite中引入了并发超时控制的概念，并将功能集成到 kite 框架的客户端调用库中。 并发超时控制模型如上图所示，在模型中引入了“Concurrent Ctrl”模块，这个模块属于微服务熔断功能的一部分，用于控制客户端能够发起的最大并发请求数。并发超时控制整体流程是这样的: 首先，客户端发起 RPC 请求，经过“Concurrent Ctrl”模块判断是否允许当前请求发起。如果被允许发起 RPC 请求，此时启动一个协程并执行 RPC 调用，同时初始化一个超时定时器。然后在主协程中同时监听 RPC 完成事件信号以及定时器信号。如果 RPC 完成事件先到达，则表示本次 RPC 成功，否则，当定时器事件发生，表明本次 RPC 调用超时。这种模型确保了无论何种情况下，一次 RPC 都不会超过预定义的时间，实现精准控制超时。 1234567891011121314151617181920import( "context")func Handler(r *Request)&#123; timeout := r.Value("timeout") ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() done := make(chan struct&#123;&#125;, 1) go func()&#123; RPC(ctx,...) done &lt;- struct&#123;&#125; &#125;() select&#123; case &lt;- done: // nice case &lt;- ctx.Done() // timeout &#125;&#125; Go 语言在1.7版本的标准库引入了“context”，这个库几乎成为了并发控制和超时控制的标准做法，随后1.8版本中在多个旧的标准库中增加对“context”的支持，其中包括“database/sql”包。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走向分布式（二）- RPC&Threads]]></title>
    <url>%2FRPC%26Threads%2F</url>
    <content type="text"><![CDATA[远程进程之间的通信的两把利器：线程和RPC。 线程为什么线程？ 允许您利用并发性，这是分布式系统中很自然的要求。 I/O并发性：在等待来自另一个服务器的响应时，处理下一个请求。 多核：线程在几个核心上并行运行。 线程==“执行线程”(thread of execution)线程允许一个程序一次（逻辑上讲）执行许多事情。线程共享内存。 每个线程都包含一些线程状态：程序计数器，寄存器，堆栈。 程序中应该有多少个线程？ 尽可能多的“有用”的应用程序。 Go鼓励你创造许多线程。通常比核心更多的线程。Go运行时会在可用CPU上安排它们。 Go线程是不是免费的，但你应该认为他们是。创建线程比方法调用更昂贵。 线程中的挑战： 共享数据 一个线程读取另一个线程正在改变的数据？ 可能导致竞赛状况。 -&gt;不要共享或协调共享（例如，互斥体） 线程之间的协调等待所有的Map线程完成可能会导致死锁（通常比竞争更易于注意）-&gt;使用Go channel或WaitGroup。 并发性的粒度 粗粒度 - &gt;简单，但很少并发/并行性。 细粒度 - &gt;更多的并发性，更多的竞争和死锁。 实例练习 Crawler 安排I/O并发在获取URL的同时，处理另一个URL 一次获取每个URL要避免浪费网络带宽要对远程服务器友好=&gt;需要某种方式来跟踪哪些网址被访问了。[在不同核心上处理不同的URL]少认同是重要的。 解决方案消除深度—使用fetched代替 顺序方案：将fetched map传递给递归调用当fetcher需要很长时间时不会重叠I/O不利用多个核心 1234567891011121314151617func CrawlSerial(url string, fetcher Fetcher, fetched map[string]bool) &#123; if fetched[url] &#123; return &#125; fetched[url] = true body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) for _, u := range urls &#123; CrawlSerial(u, fetcher, fetched) &#125; return&#125; 使用Go routines和 shared fetched map 为每个URL创建一个线程我们不通过你呢？（竞争） 为什么锁？（删除它们，似乎每一个还会工作！）没有锁可能会出什么问题？ URL的检查和标记不是原子的 所以可能会发生，我们获取相同的网址两次。T1检查提取[url]，T2检查提取[url]都看到该网址还没有被提取两者都返回假和两个取，这是错误的这叫做”竞赛条件” 该错误只显示一些线程交错 很难找到，很难推理Go可以检测到你的竞争（go run -race crawler.go）请注意，访问的检查和标记必须是原子的。 我们怎样才能决定我们处理一个页面？waitGroup 1234567891011121314151617181920212223242526272829303132333435363738394041424344type fetchState struct &#123; mu sync.Mutex fetched map[string]bool&#125;func (f *fetchState) CheckAndMark(url string) bool &#123; //defer f.mu.Unlock() //f.mu.Lock() if f.fetched[url] &#123; return true &#125; f.fetched[url] = true return false&#125;func mkFetchState() *fetchState &#123; f := &amp;fetchState&#123;&#125; f.fetched = make(map[string]bool) return f&#125;func CrawlConcurrentMutex(url string, fetcher Fetcher, f *fetchState) &#123; if f.CheckAndMark(url) &#123; return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) var done sync.WaitGroup for _, u := range urls &#123; done.Add(1) go func(u string) &#123; defer done.Done() CrawlConcurrentMutex(u, fetcher, f) &#125;(u) // Without the u argument there is a race &#125; done.Wait() return&#125; 使用Channel Channels：general-purse机制来协调线程消息的有限缓冲区多个线程可以在channel上发送和接收 （Go运行时在内部使用锁） 发送或接收可能会被阻止当channel已满时当channel是空的 通过主线程分派每个URL获取没有竞争获取map，因为它不共享！ 123456789101112131415161718192021222324252627282930313233343536func dofetch(url1 string, ch chan []string, fetcher Fetcher) &#123; body, urls, err := fetcher.Fetch(url1) if err != nil &#123; fmt.Println(err) ch &lt;- []string&#123;&#125; &#125; else &#123; fmt.Printf("found: %s %q\n", url1, body) ch &lt;- urls &#125;&#125;func master(ch chan []string, fetcher Fetcher) &#123; n := 1 fetched := make(map[string]bool) for urls := range ch &#123; for _, u := range urls &#123; if _, ok := fetched[u]; ok == false &#123; fetched[u] = true n += 1 go dofetch(u, ch, fetcher) &#125; &#125; n -= 1 if n == 0 &#123; break &#125; &#125;&#125;func CrawlConcurrentChannel(url string, fetcher Fetcher) &#123; ch := make(chan []string) go func() &#123; ch &lt;- []string&#123;url&#125; &#125;() master(ch, fetcher)&#125; 什么是最好的解决方案？ 所有并发的比串行的更难一些Go设计师认为避免共享内存即只使用channel 我们的解决方案是使用许多并发功能 加锁: 当共享是自然的时候例如，几个共享map的服务器线程 channels: 当线程间需要协调的时候例如，生产者/消费者风格的并发 使用Go的竞争探测器：https://golang.org/doc/articles/race_detector.htmlgo test -race mypkg 远程过程调用（RPC） 分布式系统的关键部分! RPC的概念代表着分布式计算的重大突破，同时也使分布式编程和传统编程相似，实现了分布透明性。该概念由Birrell和Nelson在1984年首次提出，为许多分布式系统的编程铺平了道路，一直到现在。 目标：易于编程的网络通信 隐藏客户端/服务器通信的大部分细节(对参数和结果的解码和编码、消息传递以及保留调用要求的语义) 客户调用很像普通的程序调用 服务器处理程序很像普通程序RPC被广泛使用！ RPC理想地使网络通信看起来就像fn调用：1234567Client: z = fn(x, y)Server: fn(x, y) &#123; compute return z &#125; RPC旨在达到这种透明度。 实例研究：kv.gokv.go 客户端“拨号”服务器并调用Call（） 调用类似于常规函数调用 服务器在单独的线程中处理每个请求 并发！因此，锁住了keyvalue字段。 RPC消息图：123Client Server request---&gt; &lt;---response 软件结构1234client app handlers stubs dispatcher RPC lib RPC lib net ------------ net 一些细节： 哪个服务器函数（处理程序）要调用？在Go中特指Call() 编组(Marshalling)：将数据格式化成数据包棘手的数组，指针，对象，＆CGo的RPC库非常强大！有些事情你不能通过：例如，channel，函数 绑定(Binding)：客户如何知道与谁交谈？也许客户端提供服务器主机名也许名称服务将服务名称映射到最佳服务器主机 RPC问题：如何处理失败？ 比如：丢包，网络断线，服务器运行缓慢，服务器崩溃。 错误对RPC客户端意味着什么？ 客户端从不会看到来自服务器的响应。 客户端不知道服务器是否看到了请求！也许服务器/网络在发送回复之前失败。 最简单的方案：“至少一次”(at least once)语义 RPC库等待响应一段时间，如果没有到达，重新发送请求。 这样做几次后，若仍然没有回应则将错误返回给应用程序。 采用“至少一次”调用语义，调用者可能收到返回的结果，也可能收到一个异常。在收到返回结果的情况下，调用者知道该方法至少执行过一次，而异常信息则通知它没有接收到执行结果。“至少一次”调用语义可以通过重发请求消息来达到，它屏蔽了调用或结果消息的遗漏故障。但可能会遇到下列类型的故障： 由于包含远程对象服务器故障而引起的系统崩溃 随机故障。重发调用消息时，远程对象可能会接收到这一消息并多次执行某一方法，结果导致存储或返回了错误的值。 问：应用程序能够轻松处理“至少一次”吗？ 简单的问题”写”至少一次： 客户发送“从银行账户扣除 $10” 问：这个客户端程序有什么问题？ Put（“k”，10） - 一个RPC调用在数据库服务器中设置键值对。 Put（“k”，20） - 客户端对同一个键设置其他值。 问：至少有一次可以吗？ 是的：如果可以重复操作，例如只读操作。 是的：如果应用程序有自己的应对重复计划(MIT 6.824 Lab1 中就用到该方案)。换句话说，如果能设计服务器中的操作使其服务接口中的所有方法都是幂等操作的话，那么“至少一次”调用语义是可以接受的。 更好的RPC行为：“最多一次”(at most once)想法：服务器RPC代码检测到重复的请求，返回以前的答复，而不是重新运行处理程序。 问：如何检测重复的请求？客户端包含每个请求的唯一ID（XID）;使用相同的XID重新发送。1234567server: if seen[xid]: r = old[xid] else r = handler() old[xid] = r seen[xid] = true 一些”最多一次”的复杂性 如何确保XID是唯一的？很大的随机数？将唯一的客户端ID（IP地址？）和序列号组合起来？ 服务器必须最终放弃有关旧RPC的信息什么时候放弃安全？理念： 唯一的客户端ID 每客户端RPC序列号 客户端包括“seen all replies &lt;= X” 很像TCP序列号和ack 或者一次只允许客户端一个未完成的RPC，到达的是seq+1，那么忽略其他小于seq的请求。 或者客户端最多可以尝试5次，服务器会忽略大于5次的请求。 当原来的请求还在执行，怎么样处理相同seq的请求？服务器还不知道答复; 不想跑两次 想法：给每个执行的RPC“挂起”标志; 等待或忽略 如果“最多一次”服务器崩溃并重新启动，该怎么办？ 如果在内存中至多有一次重复的信息，服务器将会忘记，并在重新启动后接受重复的请求。 也许它应该将重复信息写入磁盘？ 也许副本服务器也应该复制重复信息？ 那“仅一次”呢？ 最多一次 + 无限重试 + 容错服务。 Go RPC是“最多一次” 打开TCP连接; 写请求到TCP连接。 TCP可能会重新传输，但服务器的TCP将会过滤掉重复的内容。 Go代码没有重试（即不会创建第二个TCP连接）。 Go RPC代码返回一个错误，如果它没有得到答复： 也许是TCP连接的超时 也许服务器没有看到请求 也许服务器处理请求，但服务器/网络在回复之前失败 对于实验1来说，Go RPC的”最多一次”是不够的 它仅只适用于一个RPC调用。如果worker没有回应，master重新发送给另一名worker，但原worker可能没有失败，并且也在努力。 Go RPC不能检测到这种重复： 实验1中没有问题，因为在应用程序级别处理了。实验3将明确检测重复。 带有故障模拟的RPC实现分析labrpc——MIT 6.824中一个简单的RPC库，它很像Go的RPC包，但是带有模拟网络。它用channel模拟了network、endpoint、server、dispatch等内容，同时提供注入故障功能（网络重传、延时、丢包等）。这个模拟的网络会延迟请求和回复、会丢失请求和回复、会重新排序请求和回复。在揭示RPC原理的同时，对于学习分布式系统如何测试也有一定的借鉴意义。 注意：实验一 MapReduce实现使用的是Go的RPC包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778type JunkArgs struct &#123; X int&#125;type JunkReply struct &#123; X string&#125;type JunkServer struct &#123; mu sync.Mutex log1 []string log2 []int&#125;func (js *JunkServer) Handler1(args string, reply *int) &#123; js.mu.Lock() defer js.mu.Unlock() js.log1 = append(js.log1, args) *reply, _ = strconv.Atoi(args)&#125;func (js *JunkServer) Handler2(args int, reply *string) &#123; js.mu.Lock() defer js.mu.Unlock() js.log2 = append(js.log2, args) *reply = "handler2-" + strconv.Itoa(args)&#125;func (js *JunkServer) Handler3(args int, reply *int) &#123; js.mu.Lock() defer js.mu.Unlock() time.Sleep(20 * time.Second) *reply = -args&#125;// args is a pointerfunc (js *JunkServer) Handler4(args *JunkArgs, reply *JunkReply) &#123; reply.X = "pointer"&#125;// args is a not pointerfunc (js *JunkServer) Handler5(args JunkArgs, reply *JunkReply) &#123; reply.X = "no pointer"&#125;func TestBasic(t *testing.T) &#123; runtime.GOMAXPROCS(4) rn := MakeNetwork() e := rn.MakeEnd("end1-99") js := &amp;JunkServer&#123;&#125; svc := MakeService(js) rs := MakeServer() rs.AddService(svc) rn.AddServer("server99", rs) rn.Connect("end1-99", "server99") rn.Enable("end1-99", true) &#123; reply := "" e.Call("JunkServer.Handler2", 111, &amp;reply) if reply != "handler2-111" &#123; t.Fatalf("wrong reply from Handler2") &#125; &#125; &#123; reply := 0 e.Call("JunkServer.Handler1", "9099", &amp;reply) if reply != 9099 &#123; t.Fatalf("wrong reply from Handler1") &#125; &#125;&#125; 由该测试框架可以看出，一次RPC需要用到如下的API: MakeNetwork: 创建网络，由Client和Server组成。 MakeServer: 创建Server，Server里面有不同的Service。 MakeService: 创建Service，Service里面定义了不同的handle。 MakeEnd: 创建Client。 Connect: Client调用Server前先要执行Connect。 Call: Client调用Server端的过程，通过参数执行要调用的handle，handle需要的参数。 同时，整个RPC过程中涉及到以下基本结构: 服务器（Server）， 支持含有多个服务srv := MakeServer() srv.AddService(svc) 客户端（ClientEnd），调用Call()函数，发生一个RPC请求并等待结果 reply := end.Call(“Raft.AppendEntries”, args, &amp;reply) 网络（Network），每个结构都持有一个sync.Mutex 再剧透一下其中的关键函数: Server.AddService() 由于它可能在多个goroutine中调用，所以上锁。而deter的作用是在函数退出前，调用之后的代码，就是添加完新服务后，做解锁操作。 Server.dispatch() 分发请求。为什么持有锁？因为跟AddService并发访问可能冲突。 ClientEnd.Call() 使用反射查找参数类型，使用“gob”序列化参数（Go的自带编解码包gob.NewEncoder）; ClientEnd.ch（chan reqMsg）是用于发送请求的通道e.ch &lt;- req; 需要一个通道从接收回复(&lt;- req.replyCh)。 Network.MakeEnd，创建客户端，使用一个线程或者goroutine模拟网络，每个请求分别在不同的goroutine处理。一个端点是否可以拥有多个未处理的请求？为什么使用rn.mu.Lock()？锁保护了什么？保护Network中的活跃数组信息。 Network.ProcessReq，如果网络不可靠，可能会延迟或者丢失请求，在一个新的线程中分发请求。通过读取e.ch等待回复直到时间过去100毫秒。100毫秒只是来看看服务器是否崩溃。最后返回回复（req.replyCh &lt;- reply）。ProcessReq没有持有rn锁，是否安全？ Service.dispatch，为请求找到合适的处理方法。 RPC基本结构Network1234567891011type Network struct &#123; mu sync.Mutex reliable bool longDelays bool // pause a long time on send on disabled connection longReordering bool // sometimes delay replies a long time ends map[interface&#123;&#125;]*ClientEnd // ends, by name enabled map[interface&#123;&#125;]bool // by end name servers map[interface&#123;&#125;]*Server // servers, by name connections map[interface&#123;&#125;]interface&#123;&#125; // endname -&gt; servername endCh chan reqMsg&#125; Network中各字段含义如下: servers：该网络中的所有Server ends：该网络中的所有Client connections：Client到Server的所有链接 endCh：golang的channel，用来模拟传送数据的网络 enabled：模拟Server是否宕机 reliable：用来模拟网络是否可靠 longDelays：用来模拟慢Server longReording：用来模拟网络的乱序 Server12345type Server struct &#123; mu sync.Mutex services map[string]*Service count int // incoming RPCs&#125; Server各字段含义如下:service: Server所包含的Servicecount: 到达Server的RPC数 Client1234type ClientEnd struct &#123; endname interface&#123;&#125; // this end-point's name ch chan reqMsg // copy of Network.endCh &#125; Client各字段含义如下: endname：客户端名称 reqMsg：发送消息的模拟网络，和Network的endCh是同一个channel RPC API实现MakeNetwork12345678910111213141516171819// 本模块主要是初始化Network数据结构，然后启动一个goroutine来处理Client的Call调用请求。func MakeNetwork() *Network&#123; rn := &amp;Network&#123;&#125; rn.reliable = true rn.ends = map[interface&#123;&#125;]*ClientEnd&#123;&#125; rn.enabled = map[interface&#123;&#125;]bool&#123;&#125; rn.servers = map[interface&#123;&#125;]*Server&#123;&#125; rn.connections = map[interface&#123;&#125;](interface&#123;&#125;)&#123;&#125; rn.endCh = make(chan reqMsg) //single goroutine to handle all ClientEnd.Call()s go func()&#123; for xreq := range rn.endCh&#123; go rn.ProcessReq(xreq) &#125; &#125;() return rn&#125; MakeEnd123456789101112131415161718// 本模块主要是在Network中添加Client,并把其enabled和connections设置成空。func (rn *Network) MakeEnd(endname interface&#123;&#125;) *ClientEnd&#123; rn.mu.Lock() defer rn.mu.Unlock() if _, ok := rn.ends[endname]; ok&#123; log.Fatalf("MakeEnd: %v already exsists\n", endname) &#125; e := &amp;ClientEnd&#123;&#125; e. endname := endname e.ch = rn.endCh rn.ends[endname] = e rn.enabled[endname] = false rn.connections[endname] = nil return e&#125; MakeServer123456//本模块初始化Server结构体的service字段为空的hashmap。func MakeServer() *Server&#123; rs := &amp;Server&#123;&#125; rs.services = map[string]*Service&#123;&#125; return rs&#125; AddServer1234567// 在Network的servers中添加server。func (rn *Network) AddServer(servername interface&#123;&#125;, rs *Server)&#123; rn.mu.Lock() defer rn.mu.Unlock() rn.servers[servername] = rs&#125; MakeService12345678910111213141516171819202122232425262728293031func MakeService(rcvr interface&#123;&#125;) *Service&#123; svc := &amp;Service&#123;&#125; svc.typ = reflect.TypeOf(rcvr) svc.rcvr = reflect.ValueOf(rcvr) svc.name = reflect.Indirect(svc.rcvr).Type().Name() svc.methods = map[string]relect.Method&#123;&#125; for m := 0; m &lt; svc.typ.NumMethod(); m++ &#123; method := svc.typ.Method(m) mtype := method.Type mname := method.Name //fmt.Printf("%v pp %v ni %v 1k %v 2k %v no %v\n", // mname, method.PkgPath, mtype.NumIn(), mtype.In(1).Kind(), mtype.In(2).Kind(), mtype.NumOut()) if method.PkgPath != "" || // capitalized? mtype.NumIn() != 3 || //mtype.In(1).Kind() != reflect.Ptr || mtype.In(2).Kind() != reflect.Ptr || mtype.NumOut() != 0 &#123; // the method is not suitable for a handler //fmt.Printf("bad method: %v\n", mname) &#125; else &#123; // the method looks like a handler svc.methods[mname] = method &#125; &#125; return svc&#125; rcvr是一个golang的接口，其上定义了一系列的方法，每个方法对应RPC的一个调用函数。整个处理方式流程如下： 创建Service结构体 通过golang的reflection方式，获取结构体的所有方法，通过reflect.TypeOf(rcvr).NumMethod()来获取 检测结构体中所有的method的参数是否符合RPC的标准。 把符合的方法添加到Service中，作为handle Connect1234567// 在Network结构体中的connections设置endname的连接为servernamefunc (rn *Network) Connect(endname interface&#123;&#125;, servername interface&#123;&#125;)&#123; rn.mu.Lock() defer rn.mu.Unlock() rn.connections[endname] = servername&#125; Enable1234567// 设置此Client对应的Server是否宕机func (rn *Network) Enable(endname interface&#123;&#125;, enabled bool)&#123; rn.mu.Lock() defer rn.mu.Unlock() rn.enabled[endname] = enabled&#125; Call123456789101112131415161718192021222324252627282930313233343536// send an RPC, wait for the reply.// the return value indicates success; false means the// server couldn't be contacted.func (e *ClientEnd) Call(svcMeth string, args interface&#123;&#125;, reply interface&#123;&#125;) bool &#123; req := reqMsg&#123;&#125; req.endname = e.endname req.svcMeth = svcMeth req.argsType = reflect.TypeOf(args) req.replyCh = make(chan replyMsg) //把Client要发送的数据进行encode，即序列化 qb := new(bytes.Buffer) qe := gob.NewEncoder(qb) qe.Encode(args) req.args = qb.Bytes() //发送请求的数据到channel上，即模拟的网络上 e.ch &lt;- req //Client等待Server端返回数据 rep := &lt;-req.replyCh // Server端响应！ //Client收到数据后，按照以下流程处理。反序列化Server端的响应，最终返回结果给应用端。 if rep.ok &#123; rb := bytes.NewBuffer(rep.reply) rd := gob.NewDecoder(rb) if err := rd.Decode(reply); err != nil &#123; log.Fatalf("ClientEnd.Call(): decode reply: %v\n", err) &#125; return true &#125; else &#123; return false &#125;&#125; 在Client端发送数据后, Server端的处理流程如下:12345go func()&#123; for xreg := range rn.endCh&#123; go rn.ProcessReq(xreq) &#125;&#125; Server端检测到endCh中的数据，然后调用ProcessReq处理请求。123456789101112if enabled &amp;&amp; servername != nil &amp;&amp; server != nil &#123; if reliable == false &#123; // short delay ms := (rand.Int() % 27) time.Sleep(time.Duration(ms) * time.Millisecond) &#125; if reliable == false &amp;&amp; (rand.Int()%1000) &lt; 100 &#123; // drop the request, return as if timeout req.replyCh &lt;- replyMsg&#123;false, nil&#125; return &#125; 如果要模拟网络不是可靠的请求下，会按照如下流程处理: 随机等待一小段时间 等待完后，以一定地概率不处理结果，直接返回Client失败 接着把请求分发到相应的handle处理12345ech := make(chan replyMsg)go func()&#123; r := server.dispatch(req) ech &lt;- r&#125;() 其中server.dispatch实现如下:1234567891011121314151617181920212223242526func (rs *Server) dispatch(req reqMsg) replyMsg &#123; rs.mu.Lock() rs.count += 1 // split Raft.AppendEntries into service and method dot := strings.LastIndex(req.svcMeth, ".") serviceName := req.svcMeth[:dot] methodName := req.svcMeth[dot+1:] service, ok := rs.services[serviceName] rs.mu.Unlock() if ok &#123; return service.dispatch(methodName, req) &#125; else &#123; choices := []string&#123;&#125; for k, _ := range rs.services &#123; choices = append(choices, k) &#125; log.Fatalf("labrpc.Server.dispatch(): unknown service %v in %v.%v; expecting one of %v\n", serviceName, serviceName, methodName, choices) return replyMsg&#123;false, nil&#125; &#125;&#125; 具体到service.dispatch，定位到具体需要处理的函数并调用它，流程如下:123456789101112131415161718// decode the argument.ab := bytes.NewBuffer(req.args)ad := gob.NewDecoder(ab)ad.Decode(args.Interface()) // allocate space for the reply.replyType := method.Type.In(2)replyType = replyType.Elem()replyv := reflect.New(replyType) // call the method.function := method.Funcfunction.Call([]reflect.Value&#123;svc.rcvr, args.Elem(), replyv&#125;) // encode the reply.rb := new(bytes.Buffer)re := gob.NewEncoder(rb)re.EncodeValue(replyv) 之后server对RPC请求进行反序列化，调用对应的函数处理，最后把生成的结果序列化:12345678910111213141516171819202122232425262728293031 serverDead = rn.IsServerDead(req.endname, servername, server) if replyOK == false || serverDead == true &#123; // server was killed while we were waiting; return error. req.replyCh &lt;- replyMsg&#123;false, nil&#125; &#125; else if reliable == false &amp;&amp; (rand.Int()%1000) &lt; 100 &#123; // drop the reply, return as if timeout req.replyCh &lt;- replyMsg&#123;false, nil&#125; &#125; else if longreordering == true &amp;&amp; rand.Intn(900) &lt; 600 &#123; // delay the response for a while ms := 200 + rand.Intn(1+rand.Intn(2000)) time.Sleep(time.Duration(ms) * time.Millisecond) req.replyCh &lt;- reply &#125; else &#123; req.replyCh &lt;- reply &#125; &#125; else &#123; // simulate no reply and eventual timeout. ms := 0 if rn.longDelays &#123; // let Raft tests check that leader doesn't send // RPCs synchronously. ms = (rand.Int() % 7000) &#125; else &#123; // many kv tests require the client to try each // server in fairly rapid succession. ms = (rand.Int() % 100) &#125; time.Sleep(time.Duration(ms) * time.Millisecond) req.replyCh &lt;- replyMsg&#123;false, nil&#125;&#125; 根据一系列的配置，决定是否返回结果以及何时返回结果，用来模拟故障情况: 如果enabled为false，则模拟Server挂掉的情况，则直接返回失败。 如果reliable为false，则模拟网络不可靠情况，有概率返回失败 如果longreording为true，则以一定概率等待一定时间返回结果，以模拟网络包乱序地情况 如果是longDelays为true，则会等待一段事件再返回结果，模拟高时延的情况]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发并行;同步异步;阻塞非阻塞]]></title>
    <url>%2F1%2F</url>
    <content type="text"><![CDATA[容易被不规范讨论的几组概念。 并发&amp;并行2018/1/8 0:33 阅读 goroutine背后的系统知识与Concurrency is not parallelism有感。对并发、并行这一老生常谈的问题进行二次挖掘，但仍然停留在扩展概念和完善解释的层面。对Concurrency后面的big picture(如Actor模型等)仍认识不足。需待以后补全。 一组定义 并发(Concurrency)：一个同时具有多个活动的系统； 两个或多个事件在同一时间间隔内发生。(操作系统教材) Concurrency is the composition of independently executing processes. Concurrency is about dealing with lots of things at once. Concurrency is about structure. Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable. 一个逻辑流（一系列PC的值，唯一地对应于包含在程序的可执行目标文件中的指令/包含在运行时动态链接到程序的共享对象中的指令）的执行在时间上与另一个流重叠，这两个流称为被称为并发地运行。多个流并发地执行的一般现象成为并发。 并行(Parallelism)：用并发使一个系统运行得更快。 两个或多个事件在同一时刻发生。(操作系统教材) Parallelism is the simultaneous execution of (possibly related) computations. Parallelism is about doing lots of things at once. Parallelism is about execution. 并发是指程序的逻辑结构。非并发的程序就是一根竹竿捅到底，只有一个逻辑控制流，也就是顺序执行的(Sequential)程序，在任何时刻，程序只会处在这个逻辑控制流的某个位置。而如果某个程序有多个独立的逻辑控制流，也就是可以同时处理(deal)多件事情，我们就说这个程序是并发的。这里的“同时”，并不一定要是真正在时钟的某一时刻(那是运行状态而不是逻辑结构)，而是指：如果把这些逻辑控制流画成时序流程图，它们在时间线上是可以重叠的。 并行是指程序的运行状态。如果一个程序在某一时刻被多个CPU流水线同时进行处理，那么我们就说这个程序是以并行的形式在运行。（严格意义上讲，我们不能说某程序是“并行”的，因为“并行”不是描述程序本身，而是描述程序的运行状态。以下说到“并行”的时候，就是指代“以并行的形式运行”）显然，并行一定是需要硬件支持的。 一组推论基于上述概念，我们不难得到如下的推论： 并发是并行的必要条件，如果一个程序本身就不是并发的，也就是只有一个逻辑控制流，那么我们不可能让其被并行处理。 并发不是并行的充分条件，一个并发的程序，如果只被一个CPU流水线进行处理(通过分时)，那么它就不是并行的。 并发只是更符合现实问题本质的表达方式，并发的最初目的是简化代码逻辑，而不是使程序运行的更快； 这几段略微抽象，我们可以用一个最简单的例子来把这些概念实例化：用C语言写一个最简单的HelloWorld，它就是非并发的，如果我们建立多个线程，每个线程里打印一个HelloWorld，那么这个程序就是并发的，如果这个程序运行在老式的单核CPU上，那么这个并发程序还不是并行的，如果我们用多核多CPU且支持多任务的操作系统来运行它，那么这个并发程序就是并行的。 还有一个略微复杂的例子，更能说明并发不一定可以并行，而且并发不是为了效率，就是Go语言例子里计算素数的sieve.go。我们从小到大针对每一个因子启动一个代码片段，如果当前验证的数能被当前因子除尽，则该数不是素数，如果不能，则把该数发送给下一个因子的代码片段，直到最后一个因子也无法除尽，则该数为素数，我们再启动一个它的代码片段，用于验证更大的数字。这是符合我们计算素数的逻辑的，而且每个因子的代码处理片段都是相同的，所以程序非常的简洁，但它无法被并行，因为每个片段都依赖于前一个片段的处理结果和输出。 并发的构造方式 显式地定义并触发多个代码片段，也就是逻辑控制流，由应用程序或操作系统对它们进行调度。它们可以是独立无关的，也可以是相互依赖需要交互的。线程只是实现并发的其中一个手段，除此之外，运行库或是应用程序本身也有多种手段来实现并发。 隐式地放置多个代码片段，在系统事件发生时触发执行相应的代码片段，也就是事件驱动的方式，譬如某个端口或管道接收到了数据(多路IO的情况下)，再譬如进程接收到了某个信号(signal)。 构建并发程序的机制：进程、I/O多路复用、线程。 进程： 进程由内核自动调度，因为他们有各自独立的虚拟地址空间，所以要实现数据共享必须要有显式的的IPC机制。构造“进程”这一抽象的概念，能设计出同时执行多个程序的系统。 I/O多路复用： 事件驱动程序创建他们自己的并发逻辑流，这些逻辑流被模型化为状态机，用I/O多路复用来显示地调度这些流。因为程序运行在单一线程中，所以共享数据很快而且很容易。 线程： 线程是上述方法的综合。同基于进程的流一样，线程也是由内核自动调度的。同基于I/O多路复用的流一样，线程是运行在一个单一进程的上下文中的，因此可以更快地访问数据。构造“线程”这一概念，能够在一个进程中执行多个控制流。 从进程到线程到协程 碰着I/O访问，阻塞了后面所有的计算。空着也是空着，就把CPU切换到其他进程。 进程数高的时候，进程切换会耗费大量系统资源。于是线程的概念，大致意思就是，这个地方阻塞了，但我还有其他地方的逻辑流可以计算，这些逻辑流是共享一个地址空间的，不用特别麻烦的切换页表、刷新TLB，只要把寄存器刷新一遍就行，能比切换进程开销少点。 如果连时钟阻塞、 线程切换这些功能我们都不需要了，自己在进程里面写一个逻辑流调度的东西。那么我们即可以利用到并发优势，又可以避免反复系统调用，还有进程切换造成的开销，分分钟给你上几千个逻辑流不费力。这就是用户态线程。 从上面可以看到，实现一个用户态线程有两个必须要处理的问题：一是碰着阻塞式I\O会导致整个进程被挂起；二是由于缺乏时钟阻塞，进程需要自己拥有调度线程的能力。如果一种实现使得每个线程需要自己通过调用某个方法，主动交出控制权。那么我们就称这种用户态线程是协作式的，即是协程。本质上，协程就是用户空间下的线程。 并行的四个层面 多台机器。自然我们就有了多个CPU流水线，譬如Hadoop集群里的MapReduce任务。 多CPU。不管是真的多颗CPU还是多核还是超线程，总之我们有了多个CPU流水线。 多CPU：将多个CPU(称为”核”)集成到一个集成电路芯片上。以Intel Core i7为例，其中微处理器芯片有4个CPU核，每个核都有自己的L1和L2高速缓存，以及到主存的接口。 超线程：有时称为同时多线程(simultaneout multi-threading),是个控制流的技术。它涉及CPU某些硬件有多个备份，比如程序计数器和寄存器文件；而其他的硬件部分只有一份，比如执行浮点算数运算的单元。常规的处理器需要大约20000个时钟周期做不同线程间的转换，而超线程的处理器可以在单个周期的基础上决定要执行哪一个线程。这使得CPU能够更好地利用它的处理资源。例如，假设一个线程必须等到某些数据被装载到高速缓存中，那CPU就可以继续去执行另一个线程。举例来说，Intel Core i7处理器可以让一个核执行两个线程，所以一个4核的系统实际上可以并行地执行8个线程。 单CPU核里的ILP(Instruction-level parallelism)，指令级并行。 通过复杂的制造工艺和对指令的解析以及分支预测和乱序执行，现在的CPU可以在单个时钟周期内执行多条指令，从而，即使是非并发的程序，也可能是以并行的形式执行。早起的微处理器，如1978年的Intel 8086，需要多个(通常是3-10个)时钟周期来执行一条指令。比较先进的处理器可以保持每个时钟周期2-4条指令的执行效率。其实每条指令从开始到结束需要长得多的时间，大约要20个时间周期，为了执行多达百条的指令，通过流水线将每条指令分解成几个步骤，这些步骤可以同时进行，这样就能并行地处理不同指令的不同部分。如果处理器可以达到比一个周期一条语句更快的执行速率，就称之为超标量(superscalar)处理器。 单指令多数据(Single instruction, multiple data. SIMD)。 为了多媒体数据的处理，现在的CPU的指令集支持单条指令对多条数据进行操作。 其中，1牵涉到分布式处理，包括数据的分布和任务的同步等等，而且是基于网络的。3和4通常是编译器和CPU的开发人员需要考虑的。这里我们说的并行主要针对第2种：单台机器内的多核CPU并行。 在CMU那本著名的《Computer Systems: A Programmer’s Perspective》里的这张图也非常直观清晰： 同步&amp;异步同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。典型的异步编程模型比如Node.js。 阻塞与&amp;阻塞阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态.阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。这里阻塞与非阻塞与是否同步异步无关。如果是关心blocking IO/ asynchronous IO, 参考 Unix Network Programming View Book]]></content>
  </entry>
  <entry>
    <title><![CDATA[走向分布式（一）- MapReduce]]></title>
    <url>%2Fmapreduce%2F</url>
    <content type="text"><![CDATA[没有完美的系统，任何系统的出现都是为了解决当时最主要的问题。 走向分布式 分布式： 一个业务分拆为多个子业务，部署在不同的服务器上。各个模块间通过远程服务调用(RPC)进行通信，整个计算机集合共同对外提供服务，但对于用户来说，就像是一台计算机在提供服务一样。 一个系统走向分布式，追求的无非是： 扩展性(Scalability) 高可用(High Availability) 高性能(High Performence) 获取相应特性的同时，便注定要接受一些牺牲: 牺牲效率 牺牲AP弹性 牺牲运维的便捷性 为了对应用掩盖“分布”，产生了三大抽象: 存储(Storage) 通信(Communication) 计算(Computation) 云计算不过是分布式这个旧瓶子装的新酒：分布式技术 + 服务化技术 + 虚拟化技术(资源隔离和管理) 服务模式(Service models)美国国家标准和技术研究院的云计算定义中明确了三种服务模式： 软件即服务（SaaS - Software as a service）平台即服务（PaaS - Platform as a service）基础设施即服务（IaaS - Infrastructure as a service） 而另外两位新成员分别是：移动后端即服务(MBaaS - Mobile “backend” as a service)和Serverless computing。 虚拟化包括资源虚拟化、统一分配检测资源、向资源池中添加资源。 实现远程服务调用(RPC), 线程(threads), 并发控制(concurrency control) 性能 愿景: 彻底的可扩展性N台服务器就有N倍的吞吐量。应对更多的负载只需要堆更多的机器就好了。 事实上:随着N的增长，扩展的难度越大。 负载不均衡(Load im-balance, stragglers) 不可并行化的代码(Non-parallelizable code) 共享资源(比如网络)带来的性能瓶颈容错数以千计的机器、复杂的网络意味着总会有错误产生，而我们希望这些错误对应用是透明的。 我们想要的是: 可用性(Availability)尽管机器出了毛病，应用不至于崩溃。 耐久性(Durability)当错误修复的时候，应用又能活蹦乱跳。 一个点子诞生了: 复制服务器如果一个服务器崩溃了，用户可以转而使用其他的。 一致性 通常，基础架构需要“良好定义的行为”(well-defined behavior)。比如”Get(k)获取到的值，产生自最近的Put(k,v).” 但实现良好行为相当困难！ 备份服务器(Replica servers)之间很难保证一致。 客户端可能在多步骤上传的中途崩溃。 服务器可能在执行后、响应前崩溃。 竞争网络资源可能造成服务器的死锁。 多主服务器的风险(split brain)。 一致性和高性能不可兼得。保证一致性需要通信，强一致性会影响系统性能。高可用常常意味着应用的“弱一致性”。 实例学习: MapReduceMapReduce 概述现实需求 处理大量的原始数据比如文档抓取、Web 请求日志； 处理各种类型的衍生数据比如倒排索引、Web 文档的图结构的各种表示形式、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合。潜在问题 如何处理并行计算、如何分发数据、如何处理错误？ 有问题，试着用抽象的方法解决如何？一种新的计算模型——MapReduce应运而生。MapReduce抽象出来大多数的运算都包含的操作：在输入数据的记录上应用 Map 操作得出一个中间 key/value pair 集合。然后在所有具有相同 key 值的 value 值上应用 Reduce 操作，从而达到合并中间的数据，得到结果。值得一提的是，它的灵感来自 Lisp 和许多其他函数式语言的 Map 和 Reduce 的原语。 MapReduce 原理简图1234567input is divided into M files Input1 -&gt; Map -&gt; a,1 b,1 c,1 Input2 -&gt; Map -&gt; b,1 Input3 -&gt; Map -&gt; a,1 c,1 | | | | -&gt; Reduce -&gt; c,2 -----&gt; Reduce -&gt; b,2 用户自定义的 Map 函数接受一个输入的key/value pair值，然后产生一个中间key/value pair 值的集合。MapReduce 库把所有具有相同中间数据的值集合在一起后传递给 reduce 函数。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般地，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通过一个迭代器把中间 value 值提供给 Reduce 函数，以此处理无法全部放入内存中的大量的 value 值的集合。 例子: Word Count1234567input is thousands of text filesMap(k, v) split v into words for each word w emit(w, "1")Reduce(k, v) emit(len(v)) MapReduce 特性MapReduce 隐藏了许多令人痛苦的细节 启动服务器上的软件 追踪任务进展 数据移动 错误恢复 MapReduce 可扩展性良好 N台服务器可以有N倍的吞吐量。因为Map()操作之间并不需要交互，所以Map()可以并行处理。同理Reduce()操作也可以。 新模型解放了程序员，他们不再需要为每个应用进行专门的并行优化了。毕竟程序员的时间比机器更宝贵! 可能限制性能的地方? 我们想知道这里还有什么可优化的地方。CPU? 内存? 硬盘? 网络?论文的作者认为网络带宽是瓶颈所在，所以他们尽量避免数据在网络间的移动。 MapReduce 详述运行流程 用户程序首先调用 MapReduce 库将输入文件分成 M 个数据片，每个数据片的大小一般从16MB 到 64MB(通过可选的参数可配置每个数据片的大小)。然后在集群中启动大量用户程序的副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 或 Reduce 任务分配给一个空闲的 worker。 被分配了 Map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给Reduce worker。 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对MapReduce 调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这 R 个输出文件合并成一个文件–他们经常把这些文件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。 我们此前提到的Word_Count例子也可用该流程图表达。 如何降低缓慢网络的影响? Map 输入从 GFS replica的本地磁盘中读取，而不是网络中。 中间数据经过仅经过网络一次。 Map worker写入本地磁盘，而不是GFS。 中间数据分成多个键的文件。 大规模的网络运输更有效。 如何获得负载均衡?有些应用执行所需的时间本身就比其他应用要长。解决方案：任务数多于worker数。Master分配新任务给完成先前任务的worker。所以，理想中没有因任务太大而占据太多完成时间的情况出现。更快的服务器做更多的工作，最终在相同时间完成。 如何进行容错?例如, 如果一台服务器在MR任务中崩溃了怎么办？隐藏失败是简化编程的一大组成部分!MR只需要重新运行失败的Map和Reduce任务。 为什么不需要从头开始整个工作？究其根源，是MR要求各个任务是纯函数: 他们不在调用里保持状态 他们不读或写文件, 除了预期的 MR 输入/输出 任务之间没有隐藏的通信。 如此便保证了重新执行(re-execution)会产生相同的结果。注意事项：对纯函数的要求是MR与其他并行编程方案相比主要的限制，但这对 MR 的简洁性至关重要。 故障恢复的细节 Map worker崩溃:master 发现 worker 不再回应 pings，即意识到worker崩溃。若崩溃的worker的中间Map输出损坏，但这部分数据可能被所有Reduce任务需要。此时master重新运行，将任务分配到其他GFS副本中。有些 Reduce workers 可能已经读取失败的worker的中间数据，这里我们需要有函数式(functional)和可确定(deterministic)的Map()!如果 Reduces已经获取了所有中间数据，master 不需要重新执行 Map。尽管若之后Reduce崩溃仍会导致失败的Map的重新执行。 Reduce worker 崩溃：已完成的任务不受影响，因它们已经被存储在了GFS上，并且有副本。master 重启 worker的未完成的任务在其他worker上。 Reduce worker 在构建输出文件的中途崩溃：GFS具有原子重命名功能，可防止输出在完成之前可见。所以master在其他地方重新运行Reduce任务是安全的。 其他问题 如果master分配两个workers同样的Map()任务怎么办?大概master错以为其中一个worker宕机了，它只会告诉 Reduce workers 其中的一个. 如果master分配两个workers同样的Reduce()任务怎么办?他们都将尝试写入同样的输出文件到GFS上。GFS的原子重命名可防止混淆;先完成的文件将是可见的。 如果一个 worker 十分缓慢怎么办(straggler)?也许是由于硬件的问题，master启动前几个任务的第二个副本。 如果一个 worker 由于 h/w 或 s/w 计算出错误的输出怎么办?MR 采取失效停止(fail-stop)策略评估CPUs和软件。 如果 master 崩溃怎么办?master可以周期性地建立备份，当master宕机后，可以从这些checkpoint中恢复过来，然而必须终止当期的mapreduce活动，用户需要重新开始任务。 对于哪些应用MapReduce效果不好?不是一切应用都适合map/shuffle/reduce模式。 数据量很小的时候。 小数据更新到大数据之中。 不可预料的读操作(Map和Reduce都不能选择输入) 多重洗牌(Multiple shuffles)。比如 page-rank，虽然可以用多重MR但不是最有效的。有更灵活的系统适用于该场景，当然模型也更复杂。 观感结论MapReduce凭一己之力让集群计算广受欢迎。 或许不是最有效、最灵活的。 可扩展性良好。 易于编程。错误和数据移动对程序员透明。 启发 限定编程模型使得并行和分布式计算非常容易，也易于构造容错的计算环境； 网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽； 多次执行相同的任务可以减少性能缓慢的机器带来的负面影响（即硬件配置的不平衡），同时解决了由于机器失效导致的数据丢失问题。 继承者MapReduce 这套分布式计算框架实现的主要局限在于： 用 MapReduce 写复杂的分析 pipeline 太麻烦。 它怎么改进都还是一个基于 batch mode 的框架。 MapReduce 的计算模型特别简单，只要分析任务稍微复杂一点，你就会发现一趟 MapReduce 是没法把事情做完了，你就得设计多个互相依赖的 MapReduce 任务，这就是所谓 pipeline。在数据流复杂的分析任务中，设计好的 pipeline 达到最高运行效率很困难，至于给 pipeline 调错更是复杂。 这时就需要用到 Flume 了。Flume 提供了一个抽象层次更高的 API，然后一个 planner 把 Flume 程序转换成若干个 MapReduce 任务去跑。其实Flume的思路没有多独特，它的编程模型很像微软的LINQ，本质上是一个编译器，把一个复杂的分析程序编译成一堆基本的 MapReduce 执行单元。多说一句，DryadLINQ 的计划算法也跟 Flume 异曲同工。另外其实自从有了 Dremel, 很多分析任务都可以直接用交互式查询来完成，写分析 pipeline 的时候也少了很多。 Google 还有很多这种基于 MapReduce 的封装： Tenzing : 把复杂的 SQL 查询转换（编译）成 MapReduce. Sawzall : 直接基于 MapReduce 模型的专用语言。 MillWheel:解决流计算的问题了。 MillWheel 的所谓流计算则跟函数式编程里的懒惰求值大有渊源。比如计算：1(map (fn [x] (* x 2)) (map (fn [x] (+ x 1)) data-list)) 最笨的做法就是先把 data-list 每项加 1，输出一个列表作为每项乘 2 的 map 任务的输入，然后再输出另一个列表，这就是传统的MapReduce实现。Clojure 利用 LazySeq 实现了对 map 的懒惰求值，可以做到「要一个算一个」：当要取上述结果的第一项时，它才去取 data-list 中的第一项，作加 1 和乘 2 操作然后输出，如此类推，就不是做完一个 map 再做另一个 map 了。 MillWheel 做的则是方向正好反过来的「来一个算一个」，data-list 里来一个输入就输出一个结果，每一步都不需要等上一步全部完成（数据流往往是无限的，没有「全部完成」的概念）。 比如计算：1(reduce + 0 (map (fn [x] (* x 2)) data-stream)) 在 MillWheel 里，就可以随着 data-stream 数据的涌入，实时显示当前的数据总和，而不是到 data-stream 结束时才输出一个结果，而且这样 x * 2 的中间结果也压根用不着存储下来。可以看到，具体怎么实现上述运算，是个具体实现的底层优化的问题，在概念上计算模型还是基本的 map 和 reduce，就好比同一条 SQL 查询语句可用于不同的执行引擎。 作为常用计算模型的 MapReduce 并没有什么被淘汰的可能。当然，MapReduce 不是唯一可用的计算模型，MillWheel 可以很方便地实现其他计算模型：基于图的计算框架 Pregel。 每个系统都有自己的历史地位，一篇论文，一个系统带给我们更多的是一种思路，以及更深层次的哲学层面的东西。而不是一个具体的系统实现。具体的系统实现可能会迭代、优化和被遗弃，但其背后的思想将作为人类文明中的宝贵财富传承下去。 MIT 6.824 Lab 1Preamble: Getting familiar with the sourceMapreduce包提供了一个简单的Map / Reduce库。应用程序通常应该调用Distributed()[位于master.go中]来启动一个作业，但是也可以调用Sequential()[也在master.go中]来获得调试的顺序执行。 执行流程与论文中的运行流程无异，但有了具体的函数名看起来更亲切一些。 该应用程序提供大量输入文件, 一个Map函数, 一个Reduce函数, nReduce个reduce tasks。 RPC服务器启动, 等该worker注册(Register)至RPC中. 任务处于可执行状态时, 调度器(Schedule)决定如何把task分配给worker和如何容错。 Master把每个输入文件作为一个Map任务, 每个任务至少调用一次doMap. 这些任务或者直接执行或者由DoTask RPC发射, 每个对doMap()的调用读取合适的文件, 对每个文件调用map, 对每个map文件最后产生nReduce个文件。 Master对每个reduce人物至少调用一次doReduce(). doReduce()收集由map产生的nReduce文件, 然后对这些文件运行reduce函数. 最终产生nReduce个中间文件。 Master调用mr.merge(), 合并所有的中间文件为一个输出文件。 Master发送Shutdown RPC关闭workers, 然后关闭RPC服务器。 Part I: Map/Reduce input and output本部分要实现DoMap()和DoReduce()两个函数。 DoMap功能描述:通过inFile文件名读取文件中的内容, 将内容传入Map函数中, 返回得到Key-value对数组。将Key-value pair数组通过Split函数(实验中的Split函数为ihash()), 平均分配到nReduce个中间文件中, nReduce名字可以通过reduceName构造出来。 注意事项：Key-Value写入文件需要用Json进行序列化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func doMap( jobName string, mapTaskNumber int, inFile string, nReduce int, mapF func(file string, contents string) []KeyValue,) &#123; file, err := os.Open(inFile) if err != nil&#123; log.Fatal("Open file error: ", err) &#125; fileInfo, err := file.Stat() if err != nil&#123; log.Fatal("Get file info error: ", err) &#125; fileSize := fileInfo.Size() buf := make([]byte, fileSize) _, err = file.Read(buf) if err != nil&#123; log.Fatal("Read error: ", err) &#125; res := mapF(inFile, string(buf)) rSize := len(res) file.Close() // Generate Intermediate files for i := 0; i &lt; nReduce; i++ &#123; fileName := reduceName(jobName, mapTaskNumber, i) file, err := os.Create(fileName) if err != nil&#123; log.Fatal("Create intermediate file error:", err) &#125; enc := json.NewEncoder(file) for r := 0; r &lt; rSize; r++ &#123; kv := res[r] if ihash(kv.Key) % nReduce == i&#123; err := enc.Encode(&amp;kv) if err != nil&#123; log.Fatal("Encode error: ", kv) &#125; &#125; &#125; file.Close() &#125;&#125; DoReduce功能描述: Map生成nReduce个中间文件后, DoReduce遍历读取这些中间文件, 通过序列化器拿出所有的Key-Value pair, 然后将Key-value放入新的数据结构。为将所有key相同的value值合并, 自然想到使用Map数据结构。对所有的key进行排序, 生成有序的key数组. 然后对key-values(注意此处key对应的值为一个数组)进行Reduce操作, 并将结果写入新的文件(文件名由mergeName获得). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func doReduce( jobName string, reduceTaskNumber int, outFile string, nMap int, reduceF func(key string, values []string) string,) &#123; keyValues := make(map[string][]string) for i := 0; i &lt; nMap; i++ &#123; fileName := reduceName(jobName, i, reduceTaskNumber) file, err := os.Open(fileName) if err != nil&#123; log.Fatal("Open Error: ",fileName) &#125; decoder := json.NewDecoder(file) for&#123; var kv KeyValue err := decoder.Decode(&amp;kv) if err != nil&#123; break &#125; _, ok := keyValues[kv.Key] if !ok&#123; keyValues[kv.Key] = make([]string, 0) &#125; keyValues[kv.Key] = append(keyValues[kv.Key], kv.Value) &#125; file.Close() &#125; var keys []string for k, _ := range keyValues &#123; keys = append(keys, k) &#125; sort.Strings(keys) mergeFileName := mergeName(jobName, reduceTaskNumber) file, err := os.Create(mergeFileName) if err != nil&#123; log.Fatal("Create file error: ", err) &#125; enc := json.NewEncoder(file) for _, k := range keys&#123; res := reduceF(k, keyValues[k]) enc.Encode(&amp;KeyValue&#123;k, res&#125;) &#125; file.Close() &#125; Part II: Single-worker word count本部分将完成一个简单的Map-Reduce词频统计的任务。任务描述：对输入文件内容进行分词, 然后将词发射出去(词频默认为1), Reduce将values进行求和即可。 1234567891011121314151617181920212223func mapF(filename string, contents string) []mapreduce.KeyValue &#123; var res []mapreduce.KeyValue values := strings.FieldsFunc(contents, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, v := range values&#123; res = append(res, mapreduce.KeyValue&#123;v,"1"&#125;) &#125; return res&#125;func reduceF(key string, values []string) string &#123; var count int = 0 for _,v := range values&#123; intValue, err := strconv.Atoi(v) if err != nil&#123; fmt.Printf("%v make error: %v\n", v, err) &#125; count += intValue &#125; return strconv.Itoa(count)&#125; Part III: Distributing MapReduce tasks本部分将实现分布式MapReduce的调度模块。执行流程:启动一个Master RPC服务器, 服务器调用schedule来调用Map/Reduce任务;启动多个Worker RPC服务器, 并将Worker的端口信息注册到Master服务器的数据结构(channel)中。 Schedule: 负责整个MapReduce任务的调度, 查找当前可用Worker, 然后通过worker来执行Map/Reduce任务。 注意事项: 应该保证Schedule中所有的goroutine全部完成后才能返回. 所以应该使函数阻塞直到所有的goroutine完成。 Part IV: Handling worker failuresMaster来处理失败的workers, 当某worker上的Map/Reduce任务失败后, 需要将这个任务转移给其他worker来执行。 在设计调度任务函数schedule()的时候考虑容错性, 判断在Worker上调用RPC是否成功, 若失败则重新分配一个新的worker服务器来处理task。整个容错逻辑可以放到一个for循环中, 只有当任务成功调用才break跳出循环。 1234567891011121314151617181920212223for i := 0; i &lt; ntasks; i++&#123; waitGroup.Add(1) go func(TaskNumber int, n_other int, phase jobPhase)&#123; defer waitGroup.Done() for &#123; worker := &lt;- registerChan var args DoTaskArgs args.JobName = jobName args.File = mapFiles[TaskNumber] args.Phase = phase args.TaskNumber = TaskNumber args.NumOtherPhase = n_other ok := call(worker, "Worker.DoTask", &amp;args, new(struct&#123;&#125;)) if ok &#123; go func()&#123; registerChan &lt;- worker &#125;() break &#125; &#125; &#125;(i, n_other, phase) &#125; Part V: Inverted index generation本部分将构建一个倒排索引Map/Reduce任务。 Map任务描述:拿到一个网页URL和URL对应网页文本, 对网页文本进行分词, 将每个词作为key, 网页URL作为value发射出去。 123456789func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; values := strings.FieldsFunc(value, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, word := range values&#123; res = append(res, mapreduce.KeyValue&#123;word, document&#125;) &#125; return res&#125; Reduce任务描述：拿到一个关键词key, 和关键词对应的URL集合, 首先对URL进行去重(可能一个URL中出现多次关键词), 然后对URL进行排序(可以不排序), 根据需要的结构对整个URL集合作拼接(URL集合的长度即为URL中出现关键词的URL个数), 最后将关键词和拼接字符串发射出去。 123456789101112131415func reduceF(key string, values []string) string &#123; var buffer bytes.Buffer values = deleteDuplicates(values) sort.Strings(values) size := len(values) for index, value := range values&#123; buffer.WriteString(value) if index != (size - 1)&#123; buffer.WriteString(",") &#125; &#125; return strconv.Itoa(size) + " " + buffer.String()&#125; 去重的算法大家当然是轻车熟路了： 12345678910111213func deleteDuplicates(values []string) []string&#123; var res []string valuesMap := make(map[string]bool) for _, v := range values&#123; if _, ok := valuesMap[v]; !ok&#123; valuesMap[v] = true res = append(res, v) &#125;else&#123; continue &#125; &#125; return res&#125;]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
</search>
