<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[走向分布式（一）- MapReduce]]></title>
    <url>%2Fmapreduce%2F</url>
    <content type="text"><![CDATA[没有完美的系统，任何系统的出现都是为了解决当时最主要的问题。 走向分布式 分布式： 一个业务分拆为多个子业务，部署在不同的服务器上。各个模块间通过远程服务调用(RPC)进行通信，整个计算机集合共同对外提供服务，但对于用户来说，就像是一台计算机在提供服务一样。 一个系统走向分布式，追求的无非是： 扩展性(Scalability) 高可用(High Availability) 高性能(High Performence) 获取相应特性的同时，便注定要接受一些牺牲: 牺牲效率 牺牲AP弹性 牺牲运维的便捷性 为了对应用掩盖“分布”，产生了三大抽象: 存储(Storage) 通信(Communication) 计算(Computation) 云计算不过是分布式这个旧瓶子装的新酒：分布式技术 + 服务化技术 + 虚拟化技术(资源隔离和管理) 服务模式(Service models)美国国家标准和技术研究院的云计算定义中明确了三种服务模式： 软件即服务（SaaS - Software as a service）平台即服务（PaaS - Platform as a service）基础设施即服务（IaaS - Infrastructure as a service） 而另外两位新成员分别是：移动后端即服务(MBaaS - Mobile “backend” as a service)和Serverless computing。 虚拟化包括资源虚拟化、统一分配检测资源、向资源池中添加资源。 实现远程服务调用(RPC), 线程(threads), 并发控制(concurrency control) 性能 愿景: 彻底的可扩展性N台服务器就有N倍的吞吐量。应对更多的负载只需要堆更多的机器就好了。 事实上:随着N的增长，扩展的难度越大。 负载不均衡(Load im-balance, stragglers) 不可并行化的代码(Non-parallelizable code) 共享资源(比如网络)带来的性能瓶颈容错数以千计的机器、复杂的网络意味着总会有错误产生，而我们希望这些错误对应用是透明的。 我们想要的是: 可用性(Availability)尽管机器出了毛病，应用不至于崩溃。 耐久性(Durability)当错误修复的时候，应用又能活蹦乱跳。 一个点子诞生了: 复制服务器如果一个服务器崩溃了，用户可以转而使用其他的。 一致性 通常，基础架构需要“良好定义的行为”(well-defined behavior)。比如”Get(k)获取到的值，产生自最近的Put(k,v).” 但实现良好行为相当困难！ 备份服务器(Replica servers)之间很难保证一致。 客户端可能在多步骤上传的中途崩溃。 服务器可能在执行后、响应前崩溃。 竞争网络资源可能造成服务器的死锁 多主服务器的风险(split brain) 一致性和高性能不可兼得。保证一致性需要通信，强一致性会影响系统性能。高可用常常意味着应用的“弱一致性”。 实例学习: MapReduceMapReduce 概述现实需求 处理大量的原始数据比如文档抓取、Web 请求日志； 处理各种类型的衍生数据比如倒排索引、Web 文档的图结构的各种表示形式、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合。潜在问题 如何处理并行计算、如何分发数据、如何处理错误？ 有问题，试着用抽象的方法解决如何？一种新的计算模型——MapReduce应运而生。MapReduce抽象出来大多数的运算都包含的操作：在输入数据的记录上应用 Map 操作得出一个中间 key/value pair 集合。然后在所有具有相同 key 值的 value 值上应用 Reduce 操作，从而达到合并中间的数据，得到结果。值得一提的是，它的灵感来自 Lisp 和许多其他函数式语言的 Map 和 Reduce 的原语。 MapReduce 原理简图1234567input is divided into M files Input1 -&gt; Map -&gt; a,1 b,1 c,1 Input2 -&gt; Map -&gt; b,1 Input3 -&gt; Map -&gt; a,1 c,1 | | | | -&gt; Reduce -&gt; c,2 -----&gt; Reduce -&gt; b,2 用户自定义的 Map 函数接受一个输入的key/value pair值，然后产生一个中间key/value pair 值的集合。MapReduce 库把所有具有相同中间数据的值集合在一起后传递给 reduce 函数。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般地，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通过一个迭代器把中间 value 值提供给 Reduce 函数，以此处理无法全部放入内存中的大量的 value 值的集合。 例子: Word Count1234567input is thousands of text filesMap(k, v) split v into words for each word w emit(w, "1")Reduce(k, v) emit(len(v)) MapReduce 特性MapReduce 隐藏了许多令人痛苦的细节 启动服务器上的软件 追踪任务进展 数据移动 错误恢复 MapReduce 可扩展性良好 N台服务器可以有N倍的吞吐量。因为Map()操作之间并不需要交互，所以Map()可以并行处理。同理Reduce()操作也可以。 新模型解放了程序员，他们不再需要为每个应用进行专门的并行优化了。毕竟程序员的时间比机器更宝贵! 可能限制性能的地方? 我们想知道这里还有什么可优化的地方。CPU? 内存? 硬盘? 网络?论文的作者认为网络带宽是瓶颈所在，所以他们尽量避免数据在网络间的移动。 MapReduce 详述运行流程 用户程序首先调用 MapReduce 库将输入文件分成 M 个数据片，每个数据片的大小一般从16MB 到 64MB(通过可选的参数可配置每个数据片的大小)。然后在集群中启动大量用户程序的副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 或 Reduce 任务分配给一个空闲的 worker。 被分配了 Map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给Reduce worker。 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对MapReduce 调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这 R 个输出文件合并成一个文件–他们经常把这些文件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。 我们此前提到的Word_Count例子也可用该流程图表达。 如何降低缓慢网络的影响? Map 输入从 GFS replica的本地磁盘中读取，而不是网络中。 中间数据经过仅经过网络一次。 Map worker写入本地磁盘，而不是GFS。 中间数据分成多个键的文件。 大规模的网络运输更有效。 如何获得负载均衡?有些应用执行所需的时间本身就比其他应用要长。解决方案：任务数多于worker数。Master分配新任务给完成先前任务的worker。所以，理想中没有因任务太大而占据太多完成时间的情况出现。更快的服务器做更多的工作，最终在相同时间完成。 如何进行容错?例如, 如果一台服务器在MR任务中崩溃了怎么办？隐藏失败是简化编程的一大组成部分!MR只需要重新运行失败的Map和Reduce任务。 为什么不需要从头开始整个工作？究其根源，是MR要求各个任务是纯函数: 他们不在调用里保持状态 他们不读或写文件, 除了预期的 MR 输入/输出 任务之间没有隐藏的通信。 如此便保证了重新执行(re-execution)会产生相同的结果。注意事项：对纯函数的要求是MR与其他并行编程方案相比主要的限制，但这对 MR 的简洁性至关重要。 故障恢复的细节 Map worker崩溃:master 发现 worker 不再回应 pings，即意识到worker崩溃。若崩溃的worker的中间Map输出损坏，但这部分数据可能被所有Reduce任务需要。此时master重新运行，将任务分配到其他GFS副本中。有些 Reduce workers 可能已经读取失败的worker的中间数据，这里我们需要有函数式(functional)和可确定(deterministic)的Map()!如果 Reduces已经获取了所有中间数据，master 不需要重新执行 Map。尽管若之后Reduce崩溃仍会导致失败的Map的重新执行。 Reduce worker 崩溃：已完成的任务不受影响，因它们已经被存储在了GFS上，并且有副本。master 重启 worker的未完成的任务在其他worker上 Reduce worker 在构建输出文件的中途崩溃：GFS具有原子重命名功能，可防止输出在完成之前可见。所以master在其他地方重新运行Reduce任务是安全的。 其他问题 如果master分配两个workers同样的Map()任务怎么办?大概master错以为其中一个worker宕机了，它只会告诉 Reduce workers 其中的一个. 如果master分配两个workers同样的Reduce()任务怎么办?他们都将尝试写入同样的输出文件到GFS上。GFS的原子重命名可防止混淆;先完成的文件将是可见的。 如果一个 worker 十分缓慢怎么办(straggler)?也许是由于硬件的问题，master启动前几个任务的第二个副本。 如果一个 worker 由于 h/w 或 s/w 计算出错误的输出怎么办?MR 采取失效停止(fail-stop)策略评估CPUs和软件。 如果 master 崩溃怎么办? 对于哪些应用MapReduce效果不好?不是一切应用都适合map/shuffle/reduce模式。 数据量很小的时候。 小数据更新到大数据之中。 不可预料的读操作(Map和Reduce都不能选择输入) 多重洗牌(Multiple shuffles)。比如 page-rank，虽然可以用多重MR但不是最有效的。有更灵活的系统适用于该场景，当然模型也更复杂。 观感结论MapReduce凭一己之力让集群计算广受欢迎。 或许不是最有效、最灵活的。 可扩展性良好。 易于编程。错误和数据移动对程序员透明。 启发 限定编程模型使得并行和分布式计算非常容易，也易于构造容错的计算环境； 网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽； 多次执行相同的任务可以减少性能缓慢的机器带来的负面影响（即硬件配置的不平衡），同时解决了由于机器失效导致的数据丢失问题。 继承者MapReduce 这套分布式计算框架实现的主要局限在于： 用 MapReduce 写复杂的分析 pipeline 太麻烦。 它怎么改进都还是一个基于 batch mode 的框架。 MapReduce 的计算模型特别简单，只要分析任务稍微复杂一点，你就会发现一趟 MapReduce 是没法把事情做完了，你就得设计多个互相依赖的 MapReduce 任务，这就是所谓 pipeline。在数据流复杂的分析任务中，设计好的 pipeline 达到最高运行效率很困难，至于给 pipeline 调错更是复杂。 这时就需要用到 Flume 了。Flume 提供了一个抽象层次更高的 API，然后一个 planner 把 Flume 程序转换成若干个 MapReduce 任务去跑。其实Flume的思路没有多独特，它的编程模型很像微软的LINQ，本质上是一个编译器，把一个复杂的分析程序编译成一堆基本的 MapReduce 执行单元。多说一句，DryadLINQ 的计划算法也跟 Flume 异曲同工。另外其实自从有了 Dremel, 很多分析任务都可以直接用交互式查询来完成，写分析 pipeline 的时候也少了很多。 Google 还有很多这种基于 MapReduce 的封装： Tenzing : 把复杂的 SQL 查询转换（编译）成 MapReduce. Sawzall : 直接基于 MapReduce 模型的专用语言。 MillWheel:解决流计算的问题了。 MillWheel 的所谓流计算则跟函数式编程里的懒惰求值大有渊源。比如计算：1(map (fn [x] (* x 2)) (map (fn [x] (+ x 1)) data-list)) 最笨的做法就是先把 data-list 每项加 1，输出一个列表作为每项乘 2 的 map 任务的输入，然后再输出另一个列表，这就是传统的MapReduce实现。Clojure 利用 LazySeq 实现了对 map 的懒惰求值，可以做到「要一个算一个」：当要取上述结果的第一项时，它才去取 data-list 中的第一项，作加 1 和乘 2 操作然后输出，如此类推，就不是做完一个 map 再做另一个 map 了。 MillWheel 做的则是方向正好反过来的「来一个算一个」，data-list 里来一个输入就输出一个结果，每一步都不需要等上一步全部完成（数据流往往是无限的，没有「全部完成」的概念）。 比如计算：1(reduce + 0 (map (fn [x] (* x 2)) data-stream)) 在 MillWheel 里，就可以随着 data-stream 数据的涌入，实时显示当前的数据总和，而不是到 data-stream 结束时才输出一个结果，而且这样 x * 2 的中间结果也压根用不着存储下来。可以看到，具体怎么实现上述运算，是个具体实现的底层优化的问题，在概念上计算模型还是基本的 map 和 reduce，就好比同一条 SQL 查询语句可用于不同的执行引擎。 作为常用计算模型的 MapReduce 并没有什么被淘汰的可能。当然，MapReduce 不是唯一可用的计算模型，MillWheel 可以很方便地实现其他计算模型：基于图的计算框架 Pregel。 每个系统都有自己的历史地位，一篇论文，一个系统带给我们更多的是一种思路，以及更深层次的哲学层面的东西。而不是一个具体的系统实现。具体的系统实现可能会迭代、优化和被遗弃，但其背后的思想将作为人类文明中的宝贵财富传承下去。 MIT 6.824 Lab 1Preamble: Getting familiar with the sourceMapreduce包提供了一个简单的Map / Reduce库。应用程序通常应该调用Distributed()[位于master.go中]来启动一个作业，但是也可以调用Sequential()[也在master.go中]来获得调试的顺序执行。 执行流程与论文中的运行流程无异，但有了具体的函数名看起来更亲切一些。 该应用程序提供大量输入文件, 一个Map函数, 一个Reduce函数, nReduce个reduce tasks。 RPC服务器启动, 等该worker注册(Register)至RPC中. 任务处于可执行状态时, 调度器(Schedule)决定如何把task分配给worker和如何容错。 Master把每个输入文件作为一个Map任务, 每个任务至少调用一次doMap. 这些任务或者直接执行或者由DoTask RPC发射, 每个对doMap()的调用读取合适的文件, 对每个文件调用map, 对每个map文件最后产生nReduce个文件。 Master对每个reduce人物至少调用一次doReduce(). doReduce()收集由map产生的nReduce文件, 然后对这些文件运行reduce函数. 最终产生nReduce个中间文件。 Master调用mr.merge(), 合并所有的中间文件为一个输出文件。 Master发送Shutdown RPC关闭workers, 然后关闭RPC服务器。 Part I: Map/Reduce input and output本部分要实现DoMap()和DoReduce()两个函数。 DoMap功能描述:通过inFile文件名读取文件中的内容, 将内容传入Map函数中, 返回得到Key-value对数组。将Key-value pair数组通过Split函数(实验中的Split函数为ihash()), 平均分配到nReduce个中间文件中, nReduce名字可以通过reduceName构造出来。 注意事项：Key-Value写入文件需要用Json进行序列化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func doMap( jobName string, mapTaskNumber int, inFile string, nReduce int, mapF func(file string, contents string) []KeyValue,) &#123; file, err := os.Open(inFile) if err != nil&#123; log.Fatal("Open file error: ", err) &#125; fileInfo, err := file.Stat() if err != nil&#123; log.Fatal("Get file info error: ", err) &#125; fileSize := fileInfo.Size() buf := make([]byte, fileSize) _, err = file.Read(buf) if err != nil&#123; log.Fatal("Read error: ", err) &#125; res := mapF(inFile, string(buf)) rSize := len(res) file.Close() // Generate Intermediate files for i := 0; i &lt; nReduce; i++ &#123; fileName := reduceName(jobName, mapTaskNumber, i) file, err := os.Create(fileName) if err != nil&#123; log.Fatal("Create intermediate file error:", err) &#125; enc := json.NewEncoder(file) for r := 0; r &lt; rSize; r++ &#123; kv := res[r] if ihash(kv.Key) % nReduce == i&#123; err := enc.Encode(&amp;kv) if err != nil&#123; log.Fatal("Encode error: ", kv) &#125; &#125; &#125; file.Close() &#125;&#125; DoReduce功能描述: Map生成nReduce个中间文件后, DoReduce遍历读取这些中间文件, 通过序列化器拿出所有的Key-Value pair, 然后将Key-value放入新的数据结构。为将所有key相同的value值合并, 自然想到使用Map数据结构。对所有的key进行排序, 生成有序的key数组. 然后对key-values(注意此处key对应的值为一个数组)进行Reduce操作, 并将结果写入新的文件(文件名由mergeName获得). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func doReduce( jobName string, reduceTaskNumber int, outFile string, nMap int, reduceF func(key string, values []string) string,) &#123; keyValues := make(map[string][]string) for i := 0; i &lt; nMap; i++ &#123; fileName := reduceName(jobName, i, reduceTaskNumber) file, err := os.Open(fileName) if err != nil&#123; log.Fatal("Open Error: ",fileName) &#125; decoder := json.NewDecoder(file) for&#123; var kv KeyValue err := decoder.Decode(&amp;kv) if err != nil&#123; break &#125; _, ok := keyValues[kv.Key] if !ok&#123; keyValues[kv.Key] = make([]string, 0) &#125; keyValues[kv.Key] = append(keyValues[kv.Key], kv.Value) &#125; file.Close() &#125; var keys []string for k, _ := range keyValues &#123; keys = append(keys, k) &#125; sort.Strings(keys) mergeFileName := mergeName(jobName, reduceTaskNumber) file, err := os.Create(mergeFileName) if err != nil&#123; log.Fatal("Create file error: ", err) &#125; enc := json.NewEncoder(file) for _, k := range keys&#123; res := reduceF(k, keyValues[k]) enc.Encode(&amp;KeyValue&#123;k, res&#125;) &#125; file.Close() &#125; Part II: Single-worker word count本部分将完成一个简单的Map-Reduce词频统计的任务。任务描述：对输入文件内容进行分词, 然后将词发射出去(词频默认为1), Reduce将values进行求和即可。 1234567891011121314151617181920212223func mapF(filename string, contents string) []mapreduce.KeyValue &#123; var res []mapreduce.KeyValue values := strings.FieldsFunc(contents, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, v := range values&#123; res = append(res, mapreduce.KeyValue&#123;v,"1"&#125;) &#125; return res&#125;func reduceF(key string, values []string) string &#123; var count int = 0 for _,v := range values&#123; intValue, err := strconv.Atoi(v) if err != nil&#123; fmt.Printf("%v make error: %v\n", v, err) &#125; count += intValue &#125; return strconv.Itoa(count)&#125; Part III: Distributing MapReduce tasks本部分将实现分布式MapReduce的调度模块。执行流程:启动一个Master RPC服务器, 服务器调用schedule来调用Map/Reduce任务;启动多个Worker RPC服务器, 并将Worker的端口信息注册到Master服务器的数据结构(channel)中。 Schedule: 负责整个MapReduce任务的调度, 查找当前可用Worker, 然后通过worker来执行Map/Reduce任务。 注意事项: 应该保证Schedule中所有的goroutine全部完成后才能返回. 所以应该使函数阻塞直到所有的goroutine完成。 Part IV: Handling worker failuresMaster来处理失败的workers, 当某worker上的Map/Reduce任务失败后, 需要将这个任务转移给其他worker来执行。 在设计调度任务函数schedule()的时候考虑容错性, 判断在Worker上调用RPC是否成功, 若失败则重新分配一个新的worker服务器来处理task。整个容错逻辑可以放到一个for循环中, 只有当任务成功调用才break跳出循环。 1234567891011121314151617181920212223for i := 0; i &lt; ntasks; i++&#123; waitGroup.Add(1) go func(TaskNumber int, n_other int, phase jobPhase)&#123; defer waitGroup.Done() for &#123; worker := &lt;- registerChan var args DoTaskArgs args.JobName = jobName args.File = mapFiles[TaskNumber] args.Phase = phase args.TaskNumber = TaskNumber args.NumOtherPhase = n_other ok := call(worker, "Worker.DoTask", &amp;args, new(struct&#123;&#125;)) if ok &#123; go func()&#123; registerChan &lt;- worker &#125;() break &#125; &#125; &#125;(i, n_other, phase) &#125; Part V: Inverted index generation本部分将构建一个倒排索引Map/Reduce任务。 Map任务描述:拿到一个网页URL和URL对应网页文本, 对网页文本进行分词, 将每个词作为key, 网页URL作为value发射出去。 123456789func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; values := strings.FieldsFunc(value, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, word := range values&#123; res = append(res, mapreduce.KeyValue&#123;word, document&#125;) &#125; return res&#125; Reduce任务描述：拿到一个关键词key, 和关键词对应的URL集合, 首先对URL进行去重(可能一个URL中出现多次关键词), 然后对URL进行排序(可以不排序), 根据需要的结构对整个URL集合作拼接(URL集合的长度即为URL中出现关键词的URL个数), 最后将关键词和拼接字符串发射出去。 123456789101112131415func reduceF(key string, values []string) string &#123; var buffer bytes.Buffer values = deleteDuplicates(values) sort.Strings(values) size := len(values) for index, value := range values&#123; buffer.WriteString(value) if index != (size - 1)&#123; buffer.WriteString(",") &#125; &#125; return strconv.Itoa(size) + " " + buffer.String()&#125; 去重的算法大家当然是轻车熟路了： 12345678910111213func deleteDuplicates(values []string) []string&#123; var res []string valuesMap := make(map[string]bool) for _, v := range values&#123; if _, ok := valuesMap[v]; !ok&#123; valuesMap[v] = true res = append(res, v) &#125;else&#123; continue &#125; &#125; return res&#125;]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
      </tags>
  </entry>
</search>
