<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[走向分布式（三）- Go&Concurrency]]></title>
    <url>%2FGo%26Concurrency%2FGo%26Concurrency%2F</url>
    <content type="text"><![CDATA[Goroutine和Channel擦出的火花，使Go得以轻松应对数以万计的并发逻辑。 并发Go 作为一门新兴的编程语言，最大特点就在于它是原生支持并发的。和传统基于 OS 线程和进程实现不同，Go 语言的并发是基于用户态的并发，这种并发方式就变得非常轻量，能够轻松运行几万甚至是几十万的并发逻辑。因此使用 Go 开发的服务端应用采用的就是“协程模型”，每一个请求由独立的协程处理完成。 比进程线程模型高出几个数量级的并发能力，而相对基于事件回调的服务端模型，Go 开发思路更加符合人的逻辑处理思维，因此即使使用 Go 开发大型的项目，也很容易维护。 并发模型Go 的并发属于 CSP 并发模型的一种实现，CSP 并发模型的核心概念是：“不要通过共享内存来通信，而应该通过通信来共享内存”。这在 Go 语言中的实现就是 Goroutine 和 Channel。在1978发表的 CSP 论文中有一段使用 CSP 思路解决问题的描述。 “Problem: To print in ascending order all primes less than 10000. Use an array of processes, SIEVE, in which each process inputs a prime from its predecessor and prints it. The process then inputs an ascending stream of numbers from its predecessor and passes them on to its successor, suppressing any that are multiples of the original prime.” 要找出10000以内所有的素数，这里使用的方法是筛法，即从2开始每找到一个素数就标记所有能被该素数整除的所有数。直到没有可标记的数，剩下的就都是素数。下面以找出10以内所有素数为例，借用 CSP 方式解决这个问题。 从上图中可以看出，每一行过滤使用独立的并发处理程序，上下相邻的并发处理程序传递数据实现通信。通过4个并发处理程序得出10以内的素数表，对应的 Go 实现代码如下： 12345678910111213141516171819202122232425262728func main()&#123; origin, wait := make(chan int), make(chan struct&#123;&#125;) Processor(origin, wait) for num := 2; num &lt; 10000; num++ &#123; origin &lt;- num &#125; close(origin) &lt;- wait&#125;func Processor(seq chan int, wait chan struct&#123;&#125;)&#123; go func()&#123; prime, ok := &lt;-seq if !ok &#123; close(wait) return &#125; fmt.Println(prime) out := make(chan int) Processor(out, wait) for num := range seq&#123; if num%prime != 0&#123; out &lt;- num &#125; &#125; close(out) &#125;()&#125; 这个例子体现使用 Go 语言开发的两个特点： Go 语言的并发很简单，并且通过提高并发可以提高处理效率。 协程之间可以通过通信的方式来共享变量。 并发控制当并发成为语言的原生特性之后，在实践过程中就会频繁地使用并发来处理逻辑问题，尤其是涉及到网络I/O的过程，例如 RPC 调用，数据库访问等。下述内容整理自今日头条技术团队的博文——《今日头条Go建千亿级微服务的实践》 下图是一个微服务处理请求的抽象描述： 当 Request 到达 GW 之后，GW 需要整合下游5个服务的结果来响应本次的请求，假定对下游5个服务的调用不存在互相的数据依赖问题。那么这里会同时发起5个 RPC 请求，然后等待5个请求的返回结果。为避免长时间的等待，这里会引入等待超时的概念。超时事件发生后，为了避免资源泄漏，会发送事件给正在并发处理的请求。在实践过程中，今日头条技术团队得出两种抽象的模型。 Wait Cancel Wait和Cancel两种并发控制方式，在使用 Go 开发服务的时候到处都有体现，只要使用了并发就会用到这两种模式。在上面的例子中，GW 启动5个协程发起5个并行的 RPC 调用之后，主协程就会进入等待状态，需要等待这5次 RPC 调用的返回结果，这就是 Wait 模式。另一中 Cancel 模式，在5次 RPC 调用返回之前，已经到达本次请求处理的总超时时间，这时候就需要 Cancel 所有未完成的 RPC 请求，提前结束协程。Wait 模式使用会比较广泛一些，而对于 Cancel 模式主要体现在超时控制和资源回收。 在 Go 语言中，分别有 sync.WaitGroup 和 context.Context 来实现这两种模式。 超时控制合理的超时控制在构建可靠的大规模微服务架构显得非常重要，不合理的超时设置或者超时设置失效将会引起整个调用链上的服务雪崩。 图中被依赖的服务G由于某种原因导致响应比较慢，因此上游服务的请求都会阻塞在服务G的调用上。如果此时上游服务没有合理的超时控制，导致请求阻塞在服务G上无法释放，那么上游服务自身也会受到影响，进一步影响到整个调用链上各个服务。 在 Go 语言中，Server 的模型是“协程模型”，即一个协程处理一个请求。如果当前请求处理过程因为依赖服务响应慢阻塞，那么很容易会在短时间内堆积起大量的协程。每个协程都会因为处理逻辑的不同而占用不同大小的内存，当协程数据激增，服务进程很快就会消耗大量的内存。 协程暴涨和内存使用激增会加剧 Go 调度器和运行时 GC 的负担，进而再次影响服务的处理能力，这种恶性循环会导致整个服务不可用。在使用 Go 开发微服务的过程中，曾多次出现过类似的问题，我们称之为协程暴涨。 有没有好的办法来解决这个问题呢？通常出现这种问题的原因是网络调用阻塞过长。即使在我们合理设置网络超时之后，偶尔还是会出现超时限制不住的情况，对 Go 语言中如何使用超时控制进行分析，首先我们来看下一次网络调用的过程。 第一步，建立 TCP 连接，通常会设置一个连接超时时间来保证建立连接的过程不会被无限阻塞。 第二步，把序列化后的 Request 数据写入到 Socket 中，为了确保写数据的过程不会一直阻塞，Go 语言提供了 SetWriteDeadline 的方法，控制数据写入 Socket 的超时时间。根据 Request 的数据量大小，可能需要多次写 Socket 的操作，并且为了提高效率会采用边序列化边写入的方式。因此在 Thrift 库的实现中每次写 Socket 之前都会重新 Reset 超时时间。 第三步，从 Socket 中读取返回的结果，和写入一样， Go 语言也提供了 SetReadDeadline 接口，由于读数据也存在读取多次的情况，因此同样会在每次读取数据之前 Reset 超时时间。 分析上面的过程可以发现影响一次 RPC 耗费的总时间的长短由三部分组成：连接超时，写超时，读超时。而且读和写超时可能存在多次，这就导致超时限制不住情况的发生。为了解决这个问题，在 kite 框架中引入了并发超时控制的概念，并将功能集成到 kite 框架的客户端调用库中。 并发超时控制模型如上图所示，在模型中引入了“Concurrent Ctrl”模块，这个模块属于微服务熔断功能的一部分，用于控制客户端能够发起的最大并发请求数。并发超时控制整体流程是这样的 首先，客户端发起 RPC 请求，经过“Concurrent Ctrl”模块判断是否允许当前请求发起。如果被允许发起 RPC 请求，此时启动一个协程并执行 RPC 调用，同时初始化一个超时定时器。然后在主协程中同时监听 RPC 完成事件信号以及定时器信号。如果 RPC 完成事件先到达，则表示本次 RPC 成功，否则，当定时器事件发生，表明本次 RPC 调用超时。这种模型确保了无论何种情况下，一次 RPC 都不会超过预定义的时间，实现精准控制超时。 1234567891011121314151617181920import( "context")func Handler(r *Request)&#123; timeout := r.Value("timeout") ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() done := make(chan struct&#123;&#125;, 1) go func()&#123; RPC(ctx,...) done &lt;- struct&#123;&#125; &#125;() select&#123; case &lt;- done: // nice case &lt;- ctx.Done() // timeout &#125;&#125; Go 语言在1.7版本的标准库引入了“context”，这个库几乎成为了并发控制和超时控制的标准做法，随后1.8版本中在多个旧的标准库中增加对“context”的支持，其中包括“database/sql”包。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走向分布式（二）- RPC&Threads]]></title>
    <url>%2FRPC%26Threads%2F</url>
    <content type="text"><![CDATA[远程进程之间的通信的两把利器：线程和RPC。 线程为什么线程？ 允许您利用并发性，这是分布式系统中很自然的要求。 I/O并发性：在等待来自另一个服务器的响应时，处理下一个请求。 多核：线程在几个核心上并行运行。 线程==“执行线程”(thread of execution)线程允许一个程序一次（逻辑上讲）执行许多事情。线程共享内存。 每个线程都包含一些线程状态：程序计数器，寄存器，堆栈。 程序中应该有多少个线程？ 尽可能多的“有用”的应用程序。 Go鼓励你创造许多线程。通常比核心更多的线程。Go运行时会在可用CPU上安排它们。 Go线程是不是免费的，但你应该认为他们是。创建线程比方法调用更昂贵。 线程中的挑战： 共享数据 一个线程读取另一个线程正在改变的数据？ 可能导致竞赛状况。 -&gt;不要共享或协调共享（例如，互斥体） 线程之间的协调等待所有的Map线程完成可能会导致死锁（通常比竞争更易于注意）-&gt;使用Go channel或WaitGroup。 并发性的粒度 粗粒度 - &gt;简单，但很少并发/并行性。 细粒度 - &gt;更多的并发性，更多的竞争和死锁。 实例练习 Crawler 安排I/O并发在获取URL的同时，处理另一个URL 一次获取每个URL要避免浪费网络带宽要对远程服务器友好=&gt;需要某种方式来跟踪哪些网址被访问了。[在不同核心上处理不同的URL]少认同是重要的。 解决方案消除深度—使用fetched代替 顺序方案：将fetched map传递给递归调用当fetcher需要很长时间时不会重叠I/O不利用多个核心 1234567891011121314151617func CrawlSerial(url string, fetcher Fetcher, fetched map[string]bool) &#123; if fetched[url] &#123; return &#125; fetched[url] = true body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) for _, u := range urls &#123; CrawlSerial(u, fetcher, fetched) &#125; return&#125; 使用Go routines和 shared fetched map 为每个URL创建一个线程我们不通过你呢？（竞争） 为什么锁？（删除它们，似乎每一个还会工作！）没有锁可能会出什么问题？ URL的检查和标记不是原子的 所以可能会发生，我们获取相同的网址两次。T1检查提取[url]，T2检查提取[url]都看到该网址还没有被提取两者都返回假和两个取，这是错误的这叫做”竞赛条件” 该错误只显示一些线程交错 很难找到，很难推理Go可以检测到你的竞争（go run -race crawler.go）请注意，访问的检查和标记必须是原子的。 我们怎样才能决定我们处理一个页面？waitGroup 1234567891011121314151617181920212223242526272829303132333435363738394041424344type fetchState struct &#123; mu sync.Mutex fetched map[string]bool&#125;func (f *fetchState) CheckAndMark(url string) bool &#123; //defer f.mu.Unlock() //f.mu.Lock() if f.fetched[url] &#123; return true &#125; f.fetched[url] = true return false&#125;func mkFetchState() *fetchState &#123; f := &amp;fetchState&#123;&#125; f.fetched = make(map[string]bool) return f&#125;func CrawlConcurrentMutex(url string, fetcher Fetcher, f *fetchState) &#123; if f.CheckAndMark(url) &#123; return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) var done sync.WaitGroup for _, u := range urls &#123; done.Add(1) go func(u string) &#123; defer done.Done() CrawlConcurrentMutex(u, fetcher, f) &#125;(u) // Without the u argument there is a race &#125; done.Wait() return&#125; 使用Channel Channels：general-purse机制来协调线程消息的有限缓冲区多个线程可以在channel上发送和接收 （Go运行时在内部使用锁） 发送或接收可能会被阻止当channel已满时当channel是空的 通过主线程分派每个URL获取没有竞争获取map，因为它不共享！ 123456789101112131415161718192021222324252627282930313233343536func dofetch(url1 string, ch chan []string, fetcher Fetcher) &#123; body, urls, err := fetcher.Fetch(url1) if err != nil &#123; fmt.Println(err) ch &lt;- []string&#123;&#125; &#125; else &#123; fmt.Printf("found: %s %q\n", url1, body) ch &lt;- urls &#125;&#125;func master(ch chan []string, fetcher Fetcher) &#123; n := 1 fetched := make(map[string]bool) for urls := range ch &#123; for _, u := range urls &#123; if _, ok := fetched[u]; ok == false &#123; fetched[u] = true n += 1 go dofetch(u, ch, fetcher) &#125; &#125; n -= 1 if n == 0 &#123; break &#125; &#125;&#125;func CrawlConcurrentChannel(url string, fetcher Fetcher) &#123; ch := make(chan []string) go func() &#123; ch &lt;- []string&#123;url&#125; &#125;() master(ch, fetcher)&#125; 什么是最好的解决方案？ 所有并发的比串行的更难一些Go设计师认为避免共享内存即只使用channel 我们的解决方案是使用许多并发功能 加锁: 当共享是自然的时候例如，几个共享map的服务器线程 channels: 当线程间需要协调的时候例如，生产者/消费者风格的并发 使用Go的竞争探测器：https://golang.org/doc/articles/race_detector.htmlgo test -race mypkg 远程过程调用（RPC） 分布式系统的关键部分! 目标：易于编程的网络通信 隐藏客户端/服务器通信的大部分细节 客户来电很像普通的程序调用 服务器处理程序很像普通程序RPC被广泛使用！ RPC理想地使网络通信看起来就像fn调用：1234567Client: z = fn(x, y)Server: fn(x, y) &#123; compute return z &#125; RPC旨在达到这种透明度。 实例研究：kv.go 客户端“拨号”服务器并调用Call（） 调用类似于常规函数调用 服务器在单独的线程中处理每个请求 并发！因此，锁住了keyvalue。 RPC消息图：123Client Server request---&gt; &lt;---response 软件结构1234client app handlers stubs dispatcher RPC lib RPC lib net ------------ net 一些细节： 哪个服务器函数（处理程序）要调用？在Go中特指Call() 编组(Marshalling)：将数据格式化成数据包棘手的数组，指针，对象，＆CGo的RPC库非常强大！有些事情你不能通过：例如，channel，函数 绑定(Binding)：客户如何知道与谁交谈？也许客户端提供服务器主机名也许名称服务将服务名称映射到最佳服务器主机 RPC问题：如何处理失败？ 如丢包，断网，慢服务器，服务器崩溃。 什么是客户端RPC库的失败？ 客户端从不会看到来自服务器的响应 客户端*不知道服务器是否看到了请求！ 也许服务器/网络在发送回复之前失败 最简单的方案：“至少一次”(at least once) RPC库等待响应一段时间，如果没有到达，重新发送请求。 这样做几次后，若仍然没有回应则将错误返回给应用程序。 问：应用程序能够轻松处理“至少一次”吗？ 简单的问题”写”至少一次： 客户发送“从银行账户扣除$ 10” 问：这个客户端程序有什么问题？ Put（“k”，10） - 一个RPC在DB服务器中设置键的值 Put（“k”，20） - 客户端然后做第二个同一个键 问：至少有一次可以吗？ 是的：如果可以重复操作，例如只读操作 是的：如果应用程序有自己的应对重复计划(MIT 6.824 Lab1 中就用到该方案) 更好的RPC行为：“最多一次”(at most once) 想法：服务器RPC代码检测到重复的请求 返回以前的答复，而不是重新运行处理程序 问：如何检测重复的请求？ 客户端包含每个请求的唯一ID（XID） 使用相同的XID重新发送1234567server: if seen[xid]: r = old[xid] else r = handler() old[xid] = r seen[xid] = true 一些”最多一次”的复杂性 如何确保XID是唯一的？大随机数？结合唯一的客户端ID（IP地址？）和序列号？ 服务器必须最终放弃有关旧RPC的信息什么时候放弃安全？理念： 唯一的客户端ID 每客户端RPC序列号 客户端包括“每个RPC都看到所有回复&lt;= X” 很像TCP序列号和ack 或者一次只允许客户端一个未完成的RPC seq + 1的到达允许服务器放弃所有&lt;= seq 或者客户同意继续重试&lt;5分钟 服务器丢弃5分钟以上 如何处理dup req而原始仍在执行？服务器还不知道答复; 不想跑两次 想法：每个执行RPC的“挂起”标志; 等待或忽略 如果最多一次服务器崩溃并重新启动，该怎么办？ 如果在内存中至多有一次重复的信息，服务器将会忘记并在重新启动后接受重复的请求 也许它应该将重复信息写入磁盘？ 也许副本服务器也应该复制重复信息？ 那“仅一次”呢？ 最多一次，再加上无限重试加上容错服务。 Go RPC是“最多一次” 打开TCP连接; 写请求到TCP连接。 TCP可能会重新传输，但服务器的TCP将会过滤掉重复的内容。 Go代码没有重试（即不会创建第二个TCP连接）。 Go RPC代码返回一个错误，如果它没有得到答复： 也许在超时之后（来自TCP） 也许服务器没有看到请求 也许服务器处理请求，但服务器/网络在回复之前失败 对于实验1来说，Go RPC的”最多一次”是不够的 它仅只适用于一个RPC调用。如果worker没有回应，master重新发送给另一名worker，但原worker可能没有失败，并且也在努力。 Go RPC不能检测到这种重复：实验1中没有问题，因为在应用程序级别处理了。实验3将明确检测重复。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[走向分布式（一）- MapReduce]]></title>
    <url>%2Fmapreduce%2F</url>
    <content type="text"><![CDATA[没有完美的系统，任何系统的出现都是为了解决当时最主要的问题。 走向分布式 分布式： 一个业务分拆为多个子业务，部署在不同的服务器上。各个模块间通过远程服务调用(RPC)进行通信，整个计算机集合共同对外提供服务，但对于用户来说，就像是一台计算机在提供服务一样。 一个系统走向分布式，追求的无非是： 扩展性(Scalability) 高可用(High Availability) 高性能(High Performence) 获取相应特性的同时，便注定要接受一些牺牲: 牺牲效率 牺牲AP弹性 牺牲运维的便捷性 为了对应用掩盖“分布”，产生了三大抽象: 存储(Storage) 通信(Communication) 计算(Computation) 云计算不过是分布式这个旧瓶子装的新酒：分布式技术 + 服务化技术 + 虚拟化技术(资源隔离和管理) 服务模式(Service models)美国国家标准和技术研究院的云计算定义中明确了三种服务模式： 软件即服务（SaaS - Software as a service）平台即服务（PaaS - Platform as a service）基础设施即服务（IaaS - Infrastructure as a service） 而另外两位新成员分别是：移动后端即服务(MBaaS - Mobile “backend” as a service)和Serverless computing。 虚拟化包括资源虚拟化、统一分配检测资源、向资源池中添加资源。 实现远程服务调用(RPC), 线程(threads), 并发控制(concurrency control) 性能 愿景: 彻底的可扩展性N台服务器就有N倍的吞吐量。应对更多的负载只需要堆更多的机器就好了。 事实上:随着N的增长，扩展的难度越大。 负载不均衡(Load im-balance, stragglers) 不可并行化的代码(Non-parallelizable code) 共享资源(比如网络)带来的性能瓶颈容错数以千计的机器、复杂的网络意味着总会有错误产生，而我们希望这些错误对应用是透明的。 我们想要的是: 可用性(Availability)尽管机器出了毛病，应用不至于崩溃。 耐久性(Durability)当错误修复的时候，应用又能活蹦乱跳。 一个点子诞生了: 复制服务器如果一个服务器崩溃了，用户可以转而使用其他的。 一致性 通常，基础架构需要“良好定义的行为”(well-defined behavior)。比如”Get(k)获取到的值，产生自最近的Put(k,v).” 但实现良好行为相当困难！ 备份服务器(Replica servers)之间很难保证一致。 客户端可能在多步骤上传的中途崩溃。 服务器可能在执行后、响应前崩溃。 竞争网络资源可能造成服务器的死锁。 多主服务器的风险(split brain)。 一致性和高性能不可兼得。保证一致性需要通信，强一致性会影响系统性能。高可用常常意味着应用的“弱一致性”。 实例学习: MapReduceMapReduce 概述现实需求 处理大量的原始数据比如文档抓取、Web 请求日志； 处理各种类型的衍生数据比如倒排索引、Web 文档的图结构的各种表示形式、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合。潜在问题 如何处理并行计算、如何分发数据、如何处理错误？ 有问题，试着用抽象的方法解决如何？一种新的计算模型——MapReduce应运而生。MapReduce抽象出来大多数的运算都包含的操作：在输入数据的记录上应用 Map 操作得出一个中间 key/value pair 集合。然后在所有具有相同 key 值的 value 值上应用 Reduce 操作，从而达到合并中间的数据，得到结果。值得一提的是，它的灵感来自 Lisp 和许多其他函数式语言的 Map 和 Reduce 的原语。 MapReduce 原理简图1234567input is divided into M files Input1 -&gt; Map -&gt; a,1 b,1 c,1 Input2 -&gt; Map -&gt; b,1 Input3 -&gt; Map -&gt; a,1 c,1 | | | | -&gt; Reduce -&gt; c,2 -----&gt; Reduce -&gt; b,2 用户自定义的 Map 函数接受一个输入的key/value pair值，然后产生一个中间key/value pair 值的集合。MapReduce 库把所有具有相同中间数据的值集合在一起后传递给 reduce 函数。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般地，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通过一个迭代器把中间 value 值提供给 Reduce 函数，以此处理无法全部放入内存中的大量的 value 值的集合。 例子: Word Count1234567input is thousands of text filesMap(k, v) split v into words for each word w emit(w, "1")Reduce(k, v) emit(len(v)) MapReduce 特性MapReduce 隐藏了许多令人痛苦的细节 启动服务器上的软件 追踪任务进展 数据移动 错误恢复 MapReduce 可扩展性良好 N台服务器可以有N倍的吞吐量。因为Map()操作之间并不需要交互，所以Map()可以并行处理。同理Reduce()操作也可以。 新模型解放了程序员，他们不再需要为每个应用进行专门的并行优化了。毕竟程序员的时间比机器更宝贵! 可能限制性能的地方? 我们想知道这里还有什么可优化的地方。CPU? 内存? 硬盘? 网络?论文的作者认为网络带宽是瓶颈所在，所以他们尽量避免数据在网络间的移动。 MapReduce 详述运行流程 用户程序首先调用 MapReduce 库将输入文件分成 M 个数据片，每个数据片的大小一般从16MB 到 64MB(通过可选的参数可配置每个数据片的大小)。然后在集群中启动大量用户程序的副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 或 Reduce 任务分配给一个空闲的 worker。 被分配了 Map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给Reduce worker。 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对MapReduce 调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这 R 个输出文件合并成一个文件–他们经常把这些文件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。 我们此前提到的Word_Count例子也可用该流程图表达。 如何降低缓慢网络的影响? Map 输入从 GFS replica的本地磁盘中读取，而不是网络中。 中间数据经过仅经过网络一次。 Map worker写入本地磁盘，而不是GFS。 中间数据分成多个键的文件。 大规模的网络运输更有效。 如何获得负载均衡?有些应用执行所需的时间本身就比其他应用要长。解决方案：任务数多于worker数。Master分配新任务给完成先前任务的worker。所以，理想中没有因任务太大而占据太多完成时间的情况出现。更快的服务器做更多的工作，最终在相同时间完成。 如何进行容错?例如, 如果一台服务器在MR任务中崩溃了怎么办？隐藏失败是简化编程的一大组成部分!MR只需要重新运行失败的Map和Reduce任务。 为什么不需要从头开始整个工作？究其根源，是MR要求各个任务是纯函数: 他们不在调用里保持状态 他们不读或写文件, 除了预期的 MR 输入/输出 任务之间没有隐藏的通信。 如此便保证了重新执行(re-execution)会产生相同的结果。注意事项：对纯函数的要求是MR与其他并行编程方案相比主要的限制，但这对 MR 的简洁性至关重要。 故障恢复的细节 Map worker崩溃:master 发现 worker 不再回应 pings，即意识到worker崩溃。若崩溃的worker的中间Map输出损坏，但这部分数据可能被所有Reduce任务需要。此时master重新运行，将任务分配到其他GFS副本中。有些 Reduce workers 可能已经读取失败的worker的中间数据，这里我们需要有函数式(functional)和可确定(deterministic)的Map()!如果 Reduces已经获取了所有中间数据，master 不需要重新执行 Map。尽管若之后Reduce崩溃仍会导致失败的Map的重新执行。 Reduce worker 崩溃：已完成的任务不受影响，因它们已经被存储在了GFS上，并且有副本。master 重启 worker的未完成的任务在其他worker上。 Reduce worker 在构建输出文件的中途崩溃：GFS具有原子重命名功能，可防止输出在完成之前可见。所以master在其他地方重新运行Reduce任务是安全的。 其他问题 如果master分配两个workers同样的Map()任务怎么办?大概master错以为其中一个worker宕机了，它只会告诉 Reduce workers 其中的一个. 如果master分配两个workers同样的Reduce()任务怎么办?他们都将尝试写入同样的输出文件到GFS上。GFS的原子重命名可防止混淆;先完成的文件将是可见的。 如果一个 worker 十分缓慢怎么办(straggler)?也许是由于硬件的问题，master启动前几个任务的第二个副本。 如果一个 worker 由于 h/w 或 s/w 计算出错误的输出怎么办?MR 采取失效停止(fail-stop)策略评估CPUs和软件。 如果 master 崩溃怎么办?master可以周期性地建立备份，当master宕机后，可以从这些checkpoint中恢复过来，然而必须终止当期的mapreduce活动，用户需要重新开始任务。 对于哪些应用MapReduce效果不好?不是一切应用都适合map/shuffle/reduce模式。 数据量很小的时候。 小数据更新到大数据之中。 不可预料的读操作(Map和Reduce都不能选择输入) 多重洗牌(Multiple shuffles)。比如 page-rank，虽然可以用多重MR但不是最有效的。有更灵活的系统适用于该场景，当然模型也更复杂。 观感结论MapReduce凭一己之力让集群计算广受欢迎。 或许不是最有效、最灵活的。 可扩展性良好。 易于编程。错误和数据移动对程序员透明。 启发 限定编程模型使得并行和分布式计算非常容易，也易于构造容错的计算环境； 网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽； 多次执行相同的任务可以减少性能缓慢的机器带来的负面影响（即硬件配置的不平衡），同时解决了由于机器失效导致的数据丢失问题。 继承者MapReduce 这套分布式计算框架实现的主要局限在于： 用 MapReduce 写复杂的分析 pipeline 太麻烦。 它怎么改进都还是一个基于 batch mode 的框架。 MapReduce 的计算模型特别简单，只要分析任务稍微复杂一点，你就会发现一趟 MapReduce 是没法把事情做完了，你就得设计多个互相依赖的 MapReduce 任务，这就是所谓 pipeline。在数据流复杂的分析任务中，设计好的 pipeline 达到最高运行效率很困难，至于给 pipeline 调错更是复杂。 这时就需要用到 Flume 了。Flume 提供了一个抽象层次更高的 API，然后一个 planner 把 Flume 程序转换成若干个 MapReduce 任务去跑。其实Flume的思路没有多独特，它的编程模型很像微软的LINQ，本质上是一个编译器，把一个复杂的分析程序编译成一堆基本的 MapReduce 执行单元。多说一句，DryadLINQ 的计划算法也跟 Flume 异曲同工。另外其实自从有了 Dremel, 很多分析任务都可以直接用交互式查询来完成，写分析 pipeline 的时候也少了很多。 Google 还有很多这种基于 MapReduce 的封装： Tenzing : 把复杂的 SQL 查询转换（编译）成 MapReduce. Sawzall : 直接基于 MapReduce 模型的专用语言。 MillWheel:解决流计算的问题了。 MillWheel 的所谓流计算则跟函数式编程里的懒惰求值大有渊源。比如计算：1(map (fn [x] (* x 2)) (map (fn [x] (+ x 1)) data-list)) 最笨的做法就是先把 data-list 每项加 1，输出一个列表作为每项乘 2 的 map 任务的输入，然后再输出另一个列表，这就是传统的MapReduce实现。Clojure 利用 LazySeq 实现了对 map 的懒惰求值，可以做到「要一个算一个」：当要取上述结果的第一项时，它才去取 data-list 中的第一项，作加 1 和乘 2 操作然后输出，如此类推，就不是做完一个 map 再做另一个 map 了。 MillWheel 做的则是方向正好反过来的「来一个算一个」，data-list 里来一个输入就输出一个结果，每一步都不需要等上一步全部完成（数据流往往是无限的，没有「全部完成」的概念）。 比如计算：1(reduce + 0 (map (fn [x] (* x 2)) data-stream)) 在 MillWheel 里，就可以随着 data-stream 数据的涌入，实时显示当前的数据总和，而不是到 data-stream 结束时才输出一个结果，而且这样 x * 2 的中间结果也压根用不着存储下来。可以看到，具体怎么实现上述运算，是个具体实现的底层优化的问题，在概念上计算模型还是基本的 map 和 reduce，就好比同一条 SQL 查询语句可用于不同的执行引擎。 作为常用计算模型的 MapReduce 并没有什么被淘汰的可能。当然，MapReduce 不是唯一可用的计算模型，MillWheel 可以很方便地实现其他计算模型：基于图的计算框架 Pregel。 每个系统都有自己的历史地位，一篇论文，一个系统带给我们更多的是一种思路，以及更深层次的哲学层面的东西。而不是一个具体的系统实现。具体的系统实现可能会迭代、优化和被遗弃，但其背后的思想将作为人类文明中的宝贵财富传承下去。 MIT 6.824 Lab 1Preamble: Getting familiar with the sourceMapreduce包提供了一个简单的Map / Reduce库。应用程序通常应该调用Distributed()[位于master.go中]来启动一个作业，但是也可以调用Sequential()[也在master.go中]来获得调试的顺序执行。 执行流程与论文中的运行流程无异，但有了具体的函数名看起来更亲切一些。 该应用程序提供大量输入文件, 一个Map函数, 一个Reduce函数, nReduce个reduce tasks。 RPC服务器启动, 等该worker注册(Register)至RPC中. 任务处于可执行状态时, 调度器(Schedule)决定如何把task分配给worker和如何容错。 Master把每个输入文件作为一个Map任务, 每个任务至少调用一次doMap. 这些任务或者直接执行或者由DoTask RPC发射, 每个对doMap()的调用读取合适的文件, 对每个文件调用map, 对每个map文件最后产生nReduce个文件。 Master对每个reduce人物至少调用一次doReduce(). doReduce()收集由map产生的nReduce文件, 然后对这些文件运行reduce函数. 最终产生nReduce个中间文件。 Master调用mr.merge(), 合并所有的中间文件为一个输出文件。 Master发送Shutdown RPC关闭workers, 然后关闭RPC服务器。 Part I: Map/Reduce input and output本部分要实现DoMap()和DoReduce()两个函数。 DoMap功能描述:通过inFile文件名读取文件中的内容, 将内容传入Map函数中, 返回得到Key-value对数组。将Key-value pair数组通过Split函数(实验中的Split函数为ihash()), 平均分配到nReduce个中间文件中, nReduce名字可以通过reduceName构造出来。 注意事项：Key-Value写入文件需要用Json进行序列化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func doMap( jobName string, mapTaskNumber int, inFile string, nReduce int, mapF func(file string, contents string) []KeyValue,) &#123; file, err := os.Open(inFile) if err != nil&#123; log.Fatal("Open file error: ", err) &#125; fileInfo, err := file.Stat() if err != nil&#123; log.Fatal("Get file info error: ", err) &#125; fileSize := fileInfo.Size() buf := make([]byte, fileSize) _, err = file.Read(buf) if err != nil&#123; log.Fatal("Read error: ", err) &#125; res := mapF(inFile, string(buf)) rSize := len(res) file.Close() // Generate Intermediate files for i := 0; i &lt; nReduce; i++ &#123; fileName := reduceName(jobName, mapTaskNumber, i) file, err := os.Create(fileName) if err != nil&#123; log.Fatal("Create intermediate file error:", err) &#125; enc := json.NewEncoder(file) for r := 0; r &lt; rSize; r++ &#123; kv := res[r] if ihash(kv.Key) % nReduce == i&#123; err := enc.Encode(&amp;kv) if err != nil&#123; log.Fatal("Encode error: ", kv) &#125; &#125; &#125; file.Close() &#125;&#125; DoReduce功能描述: Map生成nReduce个中间文件后, DoReduce遍历读取这些中间文件, 通过序列化器拿出所有的Key-Value pair, 然后将Key-value放入新的数据结构。为将所有key相同的value值合并, 自然想到使用Map数据结构。对所有的key进行排序, 生成有序的key数组. 然后对key-values(注意此处key对应的值为一个数组)进行Reduce操作, 并将结果写入新的文件(文件名由mergeName获得). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func doReduce( jobName string, reduceTaskNumber int, outFile string, nMap int, reduceF func(key string, values []string) string,) &#123; keyValues := make(map[string][]string) for i := 0; i &lt; nMap; i++ &#123; fileName := reduceName(jobName, i, reduceTaskNumber) file, err := os.Open(fileName) if err != nil&#123; log.Fatal("Open Error: ",fileName) &#125; decoder := json.NewDecoder(file) for&#123; var kv KeyValue err := decoder.Decode(&amp;kv) if err != nil&#123; break &#125; _, ok := keyValues[kv.Key] if !ok&#123; keyValues[kv.Key] = make([]string, 0) &#125; keyValues[kv.Key] = append(keyValues[kv.Key], kv.Value) &#125; file.Close() &#125; var keys []string for k, _ := range keyValues &#123; keys = append(keys, k) &#125; sort.Strings(keys) mergeFileName := mergeName(jobName, reduceTaskNumber) file, err := os.Create(mergeFileName) if err != nil&#123; log.Fatal("Create file error: ", err) &#125; enc := json.NewEncoder(file) for _, k := range keys&#123; res := reduceF(k, keyValues[k]) enc.Encode(&amp;KeyValue&#123;k, res&#125;) &#125; file.Close() &#125; Part II: Single-worker word count本部分将完成一个简单的Map-Reduce词频统计的任务。任务描述：对输入文件内容进行分词, 然后将词发射出去(词频默认为1), Reduce将values进行求和即可。 1234567891011121314151617181920212223func mapF(filename string, contents string) []mapreduce.KeyValue &#123; var res []mapreduce.KeyValue values := strings.FieldsFunc(contents, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, v := range values&#123; res = append(res, mapreduce.KeyValue&#123;v,"1"&#125;) &#125; return res&#125;func reduceF(key string, values []string) string &#123; var count int = 0 for _,v := range values&#123; intValue, err := strconv.Atoi(v) if err != nil&#123; fmt.Printf("%v make error: %v\n", v, err) &#125; count += intValue &#125; return strconv.Itoa(count)&#125; Part III: Distributing MapReduce tasks本部分将实现分布式MapReduce的调度模块。执行流程:启动一个Master RPC服务器, 服务器调用schedule来调用Map/Reduce任务;启动多个Worker RPC服务器, 并将Worker的端口信息注册到Master服务器的数据结构(channel)中。 Schedule: 负责整个MapReduce任务的调度, 查找当前可用Worker, 然后通过worker来执行Map/Reduce任务。 注意事项: 应该保证Schedule中所有的goroutine全部完成后才能返回. 所以应该使函数阻塞直到所有的goroutine完成。 Part IV: Handling worker failuresMaster来处理失败的workers, 当某worker上的Map/Reduce任务失败后, 需要将这个任务转移给其他worker来执行。 在设计调度任务函数schedule()的时候考虑容错性, 判断在Worker上调用RPC是否成功, 若失败则重新分配一个新的worker服务器来处理task。整个容错逻辑可以放到一个for循环中, 只有当任务成功调用才break跳出循环。 1234567891011121314151617181920212223for i := 0; i &lt; ntasks; i++&#123; waitGroup.Add(1) go func(TaskNumber int, n_other int, phase jobPhase)&#123; defer waitGroup.Done() for &#123; worker := &lt;- registerChan var args DoTaskArgs args.JobName = jobName args.File = mapFiles[TaskNumber] args.Phase = phase args.TaskNumber = TaskNumber args.NumOtherPhase = n_other ok := call(worker, "Worker.DoTask", &amp;args, new(struct&#123;&#125;)) if ok &#123; go func()&#123; registerChan &lt;- worker &#125;() break &#125; &#125; &#125;(i, n_other, phase) &#125; Part V: Inverted index generation本部分将构建一个倒排索引Map/Reduce任务。 Map任务描述:拿到一个网页URL和URL对应网页文本, 对网页文本进行分词, 将每个词作为key, 网页URL作为value发射出去。 123456789func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; values := strings.FieldsFunc(value, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, word := range values&#123; res = append(res, mapreduce.KeyValue&#123;word, document&#125;) &#125; return res&#125; Reduce任务描述：拿到一个关键词key, 和关键词对应的URL集合, 首先对URL进行去重(可能一个URL中出现多次关键词), 然后对URL进行排序(可以不排序), 根据需要的结构对整个URL集合作拼接(URL集合的长度即为URL中出现关键词的URL个数), 最后将关键词和拼接字符串发射出去。 123456789101112131415func reduceF(key string, values []string) string &#123; var buffer bytes.Buffer values = deleteDuplicates(values) sort.Strings(values) size := len(values) for index, value := range values&#123; buffer.WriteString(value) if index != (size - 1)&#123; buffer.WriteString(",") &#125; &#125; return strconv.Itoa(size) + " " + buffer.String()&#125; 去重的算法大家当然是轻车熟路了： 12345678910111213func deleteDuplicates(values []string) []string&#123; var res []string valuesMap := make(map[string]bool) for _, v := range values&#123; if _, ok := valuesMap[v]; !ok&#123; valuesMap[v] = true res = append(res, v) &#125;else&#123; continue &#125; &#125; return res&#125;]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
</search>
