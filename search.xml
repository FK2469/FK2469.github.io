<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[走向分布式（一）- MapReduce]]></title>
    <url>%2Fmapreduce%2F</url>
    <content type="text"><![CDATA[没有完美的系统，任何系统的出现都是为了解决当时最主要的问题。 走向分布式 分布式： 一个业务分拆为多个子业务，部署在不同的服务器上。各个模块间通过远程服务调用(RPC)进行通信，整个计算机集合共同对外提供服务，但对于用户来说，就像是一台计算机在提供服务一样。 一个系统走向分布式，追求的无非是： 扩展性(Scalability) 高可用(High Availability) 高性能(High Performence) 获取相应特性的同时，便注定要接受一些牺牲: 牺牲效率 牺牲AP弹性 牺牲运维的便捷性 为了对应用掩盖“分布”，产生了三大抽象: 存储(Storage) 通信(Communication) 计算(Computation) 云计算不过是分布式这个旧瓶子装的新酒：分布式技术 + 服务化技术 + 虚拟化技术(资源隔离和管理) 服务模式(Service models)美国国家标准和技术研究院的云计算定义中明确了三种服务模式： 软件即服务（SaaS - Software as a service）平台即服务（PaaS - Platform as a service）基础设施即服务（IaaS - Infrastructure as a service） 而另外两位新成员分别是：移动后端即服务(MBaaS - Mobile “backend” as a service)和Serverless computing。 虚拟化包括资源虚拟化、统一分配检测资源、向资源池中添加资源。 实现 远程服务调用(RPC), 线程(threads), 并发控制(concurrency control) 性能 愿景: 彻底的可扩展性N台服务器就有N倍的吞吐量。应对更多的负载只需要堆更多的机器就好了。 事实上:随着N的增长，扩展的难度越大。 负载不均衡(Load im-balance, stragglers) 不可并行化的代码(Non-parallelizable code) 共享资源(比如网络)带来的性能瓶颈 容错 数以千计的机器、复杂的网络意味着总会有错误产生，而我们希望这些错误对应用是透明的。 我们想要的是: 可用性(Availability) 尽管机器出了毛病，应用不至于崩溃。 耐久性(Durability) 当错误修复的时候，应用又能活蹦乱跳。 一个想法: 复制服务器 如果一个服务器崩溃了，用户可以转而使用其他的。 一致性 通常，基础架构需要“良好定义的行为”(well-defined behavior)。比如”Get(k)获取到的值，产生自最近的Put(k,v).” 但实现良好行为相当困难！ 备份服务器(Replica servers)之间很难保证一致。 客户端可能在多步骤上传的中途崩溃。 服务器可能在执行后、响应前崩溃。 竞争网络资源可能造成服务器的死锁 多主服务器的风险(split brain)一致性和高性能不可兼得。保证一致性需要通信，强一致性会影响系统性能。高可用常常意味着应用的“弱一致性”。 实例学习: MapReduceMapReduce 概述现实需求 处理大量的原始数据比如文档抓取、Web 请求日志； 处理各种类型的衍生数据比如倒排索引、Web 文档的图结构的各种表示形式、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合。潜在问题 如何处理并行计算、如何分发数据、如何处理错误？ 有问题，试着用抽象的方法解决如何？一种新的计算模型——MapReduce应运而生。MapReduce抽象出来大多数的运算都包含的操作：在输入数据的记录上应用 Map 操作得出一个中间 key/value pair 集合。然后在所有具有相同 key 值的 value 值上应用 Reduce 操作，从而达到合并中间的数据，得到结果。值得一提的是，它的灵感来自 Lisp 和许多其他函数式语言的 Map 和 Reduce 的原语。 MapReduce 原理简图1234567input is divided into M files Input1 -&gt; Map -&gt; a,1 b,1 c,1 Input2 -&gt; Map -&gt; b,1 Input3 -&gt; Map -&gt; a,1 c,1 | | | | -&gt; Reduce -&gt; c,2 -----&gt; Reduce -&gt; b,2 用户自定义的 Map 函数接受一个输入的 key/value pair 值，然后产生一个中间 key/value pair 值的集合。MapReduce 库把所有具有相同中间数据的值集合在一起后传递给 reduce 函数。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般地，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通过一个迭代器把中间 value 值提供给 Reduce 函数，以此处理无法全部放入内存中的大量的 value 值的集合。 例子: word count1234567input is thousands of text filesMap(k, v) split v into words for each word w emit(w, "1")Reduce(k, v) emit(len(v)) MapReduce 特性MapReduce 隐藏了许多令人痛苦的细节 启动服务器上的软件 追踪任务进展 数据移动 错误恢复 MapReduce 可扩展性良好 N台服务器可以有N倍的吞吐量。因为Map()操作之间并不需要交互，所以Map()可以并行处理。同理Reduce()操作也可以。 新模型解放了程序员，他们不再需要为每个应用进行专门的并行优化了。毕竟程序员的时间比机器更宝贵! 可能限制性能的地方? 我们想知道这里还有什么可优化的地方。CPU? 内存? 硬盘? 网络?论文的作者认为网络带宽是瓶颈所在，所以他们尽量避免数据在网络间的移动。 MapReduce 详述运行流程 用户程序首先调用 MapReduce 库将输入文件分成 M 个数据片，每个数据片的大小一般从16MB 到 64MB(通过可选的参数可配置每个数据片的大小)。然后在集群中启动大量用户程序的副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 或 Reduce 任务分配给一个空闲的 worker。 被分配了 Map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给Reduce worker。 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对MapReduce 调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这 R 个输出文件合并成一个文件–他们经常把这些文件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。 重温一下之前Word Count的例子！ 如何降低缓慢网络的影响? Map 输入从 GFS replica的本地磁盘中读取，而不是网络中。 中间数据经过仅经过网络一次。 Map worker写入本地磁盘，而不是GFS。 中间数据分成多个键的文件。 大规模的网络运输更有效。 如何获得负载均衡?有些应用执行所需的时间本身就比其他应用要长。 解决方案：任务数多于worker数。Master分配新任务给完成先前任务的worker。所以，理想中没有因任务太大而占据太多完成时间的情况出现。更快的服务器做更多的工作，最终在相同时间完成。 如何进行容错?例如, 如果一台服务器在MR任务中崩溃了怎么办？隐藏失败是简化编程的一大组成部分!MR只需要重新运行失败的Map和Reduce任务。 为什么不从头开始整个工作？ 究其根源，是MR要求各个任务是纯函数: 他们不在调用里保持状态 他们不读或写文件, 除了预期的 MR 输入/输出 任务之间没有隐藏的通信。因此, 重新执行(re-execution)产生相同的结果。 对纯函数的要求是MR与其他并行编程方案相比主要的限制，但这对 MR 的简洁性至关重要。 故障恢复的细节 Map worker崩溃:master 发现 worker 不再回应 pings，即意识到worker崩溃。若崩溃的worker的中间Map输出损坏，但这部分数据可能被所有Reduce任务需要。此时master重新运行，将任务分配到其他GFS副本中。有些 Reduce workers 可能已经读取失败的worker的中间数据，这里我们需要有函数式(functional)和可确定(deterministic)的Map()!如果 Reduces已经获取了所有中间数据，master 不需要重新执行 Map。尽管若之后Reduce崩溃仍会导致失败的Map的重新执行。 Reduce worker 崩溃：已完成的任务不受影响，因它们已经被存储在了GFS上，并且有副本。master 重启 worker的未完成的任务在其他worker上 Reduce worker 在构建输出文件的中途崩溃：GFS具有原子重命名功能，可防止输出在完成之前可见。所以master在其他地方重新运行Reduce任务是安全的。 其他问题 如果master分配两个workers同样的Map()任务怎么办?大概master错以为其中一个worker宕机了，它只会告诉 Reduce workers 其中的一个. 如果master分配两个workers同样的Reduce()任务怎么办?他们都将尝试写入同样的输出文件到GFS上。GFS的原子重命名可防止混淆;先完成的文件将是可见的。 如果一个 worker 十分缓慢怎么办(straggler)?也许是由于硬件的问题，master启动前几个任务的第二个副本。 如果一个 worker 由于 h/w 或 s/w 计算出错误的输出怎么办?MR 采取失效停止(fail-stop)策略评估CPUs和软件。 如果 master 崩溃怎么办? 对于哪些应用MapReduce效果不好?不是一切应用都适合map/shuffle/reduce模式。 数据量很小的时候。 小数据更新到大数据之中。 不可预料的读操作(Map和Reduce都不能选择输入) 多重洗牌(Multiple shuffles)。比如 page-rank，虽然可以用多重MR但不是最有效的。有更灵活的系统适用于该场景，当然模型也更复杂。 结论MapReduce凭一己之力让集群计算广受欢迎。 或许不是最有效、最灵活的 可扩展性良好 易于编程。错误和数据移动对程序员透明。 启发 限定编程模型使得并行和分布式计算非常容易，也易于构造容错的计算环境； 网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽； 多次执行相同的任务可以减少性能缓慢的机器带来的负面影响（即硬件配置的不平衡），同时解决了由于机器失效导致的数据丢失问题。 MIT 6.824 Lab 1Preamble: Getting familiar with the sourceMapreduce包提供了一个简单的Map / Reduce库。应用程序通常应该调用Distributed()[位于master.go中]来启动一个作业，但是也可以调用Sequential()[也在master.go中]来获得调试的顺序执行。 执行流程与论文中的运行流程无异，但有了具体的函数名看起来更亲切一些。 该应用程序提供大量输入文件, 一个Map函数, 一个Reduce函数, nReduce个reduce tasks。 RPC服务器启动, 等该worker注册(Register)至RPC中. 任务处于可执行状态时, 调度器(Schedule)决定如何把task分配给worker和如何容错。 Master把每个输入文件作为一个Map任务, 每个任务至少调用一次doMap. 这些任务或者直接执行或者由DoTask RPC发射, 每个对doMap()的调用读取合适的文件, 对每个文件调用map, 对每个map文件最后产生nReduce个文件。 Master对每个reduce人物至少调用一次doReduce(). doReduce()收集由map产生的nReduce文件, 然后对这些文件运行reduce函数. 最终产生nReduce个中间文件。 Master调用mr.merge(), 合并所有的中间文件为一个输出文件。 Master发送Shutdown RPC关闭workers, 然后关闭RPC服务器。 Part I: Map/Reduce input and outputPart II: Single-worker word countPart III: Distributing MapReduce tasksPart IV: Handling worker failuresPart V: Inverted index generation]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
      </tags>
  </entry>
</search>
