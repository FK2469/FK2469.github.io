<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分布式容错系统]]></title>
    <url>%2FFault-Tolerant%20Virtual%20Machines%2F</url>
    <content type="text"><![CDATA[虚拟化与容错系统的结合与碰撞。在分布式系统中有很多种容错方法。最常见的就是——主/副服务器方法（当主服务器宕机之后，由副服务器来接管它的工作）。这种方法通常需要机器之间的高带宽。 另外还有确定（deterministic）状态机方法：将另一台服务器初始化为和主服务器一样的状态，然后让它们都接受到同样的输入，这样它们的状态始终保持一致。虽然这种方法对于非确定的（non-deterministic）操作并不适用（因此会引入很多额外的操作来保持同步），但与状态机的改变相比仍然是很小的一部分。 本文中讨论的方法是使用虚拟机作为状态机，它具有以下优点： 操作全部被虚拟化 虚拟机本身就支持 non-deterministic 操作 虚拟机管理程序（Hypervision）能够记录所有在虚拟机上的操作，所以能够记录主服务器（Primary）所有操作，然后在副服务器（Backup）上进行演绎 基本设计方案 如图就是本文提到的容错系统的架构，一个 Primary，一个 Backup，Primary 和 Backup 之间通过 Logging Channel 进行通信，Primary 和 Backup 基本保持同步，Backup 稍稍落后，它们两个之间会通过 heartbeat 进行 fail 检测，并且它们使用共享磁盘（Shared Disk）。 可确定（deterministic）操作的演绎让两台机器初始状态相同，它们接受相同的输入，顺序相同，两台机器执行的任务的结果就会相同。 但是如果存在非确定的（non-deterministic）操作（比如中断事件、读取CPU时钟计数器的值操作就是非确定的），它会影响状态机的执行。 难点在于： 需要捕捉全部的输入和 non-deterministic 操作在保证 Backup 是deterministic 的 需要准确将全部输入和 non-deterministic 操作应用到 Backup 中 需要保证系统高效 设计方案为：将所有的 input 和 non-deterministic 操作写入到 log 中（file），对于 non-deterministic 操作还要记录和它相关的状态信息等，确保 non-deterministic 操作后Backup状态还是和 Primary 一致 FT（Fault-Tolerance）协议FT 协议是应用于 logging channel 的协议，协议的基本要求为： 如果 Primary 宕机了，Backup 接替它的工作，Backup 之后向外界发出所有的 Output 要和 Primary 原本应当发送的一致。 为了保证以上的要求，设计如下系统： Primary会在所有关于本次Output 的所有信息都发送给 Backup 之后（并且要确保 Backup 收到）才会把 output 发送给外界 Primary 只是推迟将 output 发送给外界，而不会暂停执行后边的任务 流程如图所示： 但是这种方法不能保证 output 只发出一次，如果 primary 宕机了，backup 不能判断它是在发送了 output 之前还是之后宕机的，因此 backup 会再发送一次 output。但是这个问题很容易解决，因为： output 是通过网络进行发送的，例如 TCP 之类的网络协议能够检测重复的数据包 即使 output 被发送了2次其实也没关系。如果 output 是一个写操作，它会在同一个位置写入两次，结果不会发生变化；如果 output 是读取操作，读的内容会被放入 bounce buffer（为了消除 DMA 竞争），数据会在 IO 中断之后被送到 宕机检测如何知道有机器宕机，在该系统中是十分重要的。该设计使用的是UDP heartbeat 机制来检测 Primary 与 Backup 之间的通信是否正常。 但是使用这种方法会存在裂脑问题（split-brain，Primary 和 Backup 同时宕机），该怎么解决呢？ 该设计中使用了共享存储（Shared Storage），对它的操作是原子的，Primary 和 Backup不能同时进行一个操作（提供原子的 test-and-set 操作） 如果检测出 Primary 宕机，Backup 会成为 Primary，接替之前的工作，然后再寻找一个 Backup。 具体实现启动/重启 Virtual Machine如何启动一个和 Primary 状态一样的 Backup？ VMware Vmotion 操作能够将一台 VM 从一个 Server 完整的迁移到另一个 Server（只需要很短的中断），在该设计中的方法对 Vmotion 做了一点修改，不是进行迁移，而是直接克隆。 管理 Logging Channel 如图，该设计使用了一个大的 buffer，来保存 logging entries，Primary 把自己的 entry 存到 buffer 中，由 logging channel 发送给Backup 的 buffer，然后 Backup 从 buffer 读取命令执行。 如果 Backup 的 buffer 空了，没有命令执行了，Backup 会等待新的 entry 如果 Primary 的 buffer 满了，Primary 会等待，等 buffer 中有空余空间再继续执行 Disk I/O问题 disk 操作是并行的，同时对 disk 的同一位置进行操作会导致 non-deterministic解决方案：检测 IO 竞争，使这些操作串行执行 Disk IO 使用 DMA（Direct Memory Access），同时访问内存同一位置的操作会导致 non-deterministic解决方案：对 disk 操作的内存设置内存的页保护，但是这种方法代价太高；该设计中使用了 bounce buffer，它的大小和 disk 所操作的内存部分大小是一致的，read 操作直接将内容读入 buffer，当其他操作完成，写入内存，write 操作将写内容写入 buffer，之后再写入磁盘。 总结Vmware 提出的这种 Primary/Backup 方法是分布式容错方法中非常重要的一部分，可以用在许多系统中，不仅仅是分布式存储（GFS 的容错方法），也可以用在分布式计算中，因为它是将所有的操作都记录下来，将它们重新在 Backup 上进行演绎，从而起到了备份的作用，能够做到容错（Fault-Tolerance）。 观感虚拟化技术的应用,使企业得以把原来独占物理主机的业务系统集中部署到虚拟化环境下.这种部署方式在提高资源利用率和降低管理成本的同时,也严重降低了系统的可靠性。虽然本论文试图通过备份的方式来解决这一问题,但却引入了较大的系统开销。另外,由于系统的运行状态具有一定的规律性和周期性,因此可以通过对业务系统历史数据的分析来对系统的未来运行状态进行预测。 中科院软件研究所提出了一种基于隐马尔可夫模型的系统失效恢复性能优化方法。通过对系统运行时状态的预测分析,计算系统未来运行状态的概率趋势,并在运行过程中动态调整系统失效恢复功能与正常业务功能之间的资源分配,从而降低了系统的运行时性能开销,提高了业务系统服务能力。实验分析显示,该方法可以在保障系统可靠性的同时有效地降低系统的性能开销,在系统运行状态稳定的情况下,最高可以降低 2/3 的系统响应时间。 Q&amp;A问:为什么说在物理服务器上保证“确定性执行”(deterministic execution)比在VM上要困难得多? 答:因为虚拟机监控程序本质上模拟了硬件的所有方面，包括所有潜在的非确定性源，比如中断时间。 问:GFS和VMware FT都提供容错功能。我们应该如何权衡两者? 答:VMware FT复制计算;您可以使用它透明地向任何现有网络服务器添加容错功能。FT提供相当严格的一致性，对服务器和客户端是透明的。例如，您可以使用FT来获取现有的邮件服务器，并使其容错。相比之下，GFS提供了仅用于存储的容错功能。因为GFS是专门针对特定的简单服务(存储)的，所以它的复制比FT更有效。例如，GFS不需要在所有副本上完全相同的指令下导致中断。GFS通常是实现完全容错服务的更大系统的一部分。例如，VMware FT本身依赖于主和备份共享的容错存储服务(图1中的共享磁盘)，您可以使用类似GFS之类的东西来实现(尽管严格说来，GFS与FT并不是很合适)。 问:第3.4节的反弹缓冲(bounce buffers)如何帮助避免竞争(race)? 答:当I/O正在进行时，缓冲缓冲区是私有的I/O系统。当DMA正在进行时，来宾操作系统不会看到内存的任何变化。当I/O完成时，monitor将反弹缓冲区切换到来宾操作系统的地址空间。因此，客户操作系统不可能观察到部分完成的DMA。此外，由于监视器记录输入和输出事件，所以备份监视器将在缓冲区中以与主服务器相同的位置(at the exact same point)进行交换。 问:什么是“共享存储器上的原子测试集操作”? 答:这意味着一台机器可以在单个原子操作中读取与写入一个磁盘块。如果两台机器调用这个操作，那么它们的两个“读、写周期”不会被混在一起，而是一台机器执行它的读和写，然后是另一台机器的读和写。 问:根据输出规则(Output Rule)，损失了多少性能? 答:其实从表2中可以得到一些启示。通过遵循输出规则，传输速率会降低，但影响不是很大。然而，接收带宽明显减少，但这是因为传入的数据包导致了日志通道上的大量通信。 问:如果应用程序调用一个随机数生成器怎么办?不会在主和备份中不会产生不同的结果，并导致执行的差异吗? 答:主备份和备份将从它们的随机数生成器得到相同的数字。所有随机性的来源都由管理程序控制;VM运行完全相同的代码。例如，如果VM从时钟芯片读取物理时间，Hypervisor将看到读取并确保备份中的读取与主服务器上的读取值相同。主和备份的虚拟CPU负载应该是相同的，因为它们执行相同的指令，并且在相同的虚拟时间内处于空闲状态。 问:创建者如何确定他们捕捉到了所有可能的非确定性的形式？ 答:我的猜测是这样的。作者在一家公司工作，在那里许多人了解VM监视器、微处理器和客户OSes的内部结构，并将意识到许多缺陷。对于VM-FT来说，作者利用之前一个项目的日志和重放机制进行了确定性重放(deterministic replay)，它必须已经处理了非确定性的来源。我认为确定性重放的设计者做了大量的测试，并获得了大量的非决定论的经验。 问:如果主服务器在发送输出到外部世界后突然发生故障，会发生什么情况? 答:输出可能发生两次:一次在主服务器上，一次在备份中。但这种重复网络和磁盘I/O都能妥善处理。如果输出是一个网络数据包，那么接收客户端的TCP软件将自动丢弃该副本。如果输出事件是磁盘I/O，磁盘I/Os是幂等的(两者都将相同的数据写入相同的位置，并且没有插入I/Os)。 问：第3.4节讲述发生故障时主要的未完成的磁盘I/O;它说：“相反，我们在备份虚拟机的上线过程中重新发出挂起的I/O。”待定I/O(pending I/O)在哪里存储，重新发布需要多久？ 答：本文讨论的磁盘I/O中有一个日志条目，指示I/O已启动，但没有条目指示完成。这些是必须在备份上重新启动的I/O操作。当I/O完成时，I/O设备产生一个I/O完成中断。因此，如果日志中缺少I/O完成中断，则备份将重新启动I/O。如果日志中有I/O完成中断，则不需要重新启动I/O。 问：这个系统有多安全？ 答：作者假定主要和备份遵循协议，并且不是恶意的（例如，攻击者没有妥协管理程序）。系统无法处理受影响的虚拟机管理程序。另一方面，虚拟机管理程序可以防御恶意或错误的客户操作系统和应用程序。 问：仅解决故障停止故障是否合理？什么是其他类型的失败？ 答：这是合理的，因为许多现实世界的故障本质上是故障停止，例如许多网络和电源故障。做得比这好，需要应付似乎运行正常但计算结果不正确的计算机;在最坏的情况下，也许失败是恶意攻击者的结果。这种较大规模的非故障停车故障通常被称为“拜占庭式失败”。有些方法可以解决拜占庭式的失败问题，我们将在课程结束时介绍这些失败的情况，但大部分情况是关于失败 - 停止失败。]]></content>
      <tags>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[由同步异步I/O引发的...]]></title>
    <url>%2FSynchronize%26Asynchronize%2F</url>
    <content type="text"><![CDATA[混淆常常发生在熟读《Linux》的同时也学习了一门脚本语言。 在程序员的世界里，“概念”既廉价又珍贵。说它廉价，一来是因为祖师爷们大都是从数学家转职成计算机科学家，所以很多概念都是从数学里面借用过来；二来是徒子徒孙们在资本以及媒体的怂恿下疯狂地造新概念。且不提“人工智能”相关的概念，就算是普通的软件开发领域，也有很多像“组件化”和“模块化”这样难区分的概念。因此，一个清晰准确的概念，其实十分的珍贵。 引子在知乎上经常看到有人用烧水、做面包、打车、买书、等餐等例子来解释这同步异步、阻塞非阻塞，以及他们的组合“同步阻塞”、“同步非阻塞”吧啦吧啦。这种答案的评论经常是清一色的“生动、形象、清晰明了”，常常令我联想到“好！威武！有希望了！”。的确，在你痛苦纠结的时候看完这样一个答案，的确会产生“醍醐灌顶”、“我懂了”的错觉。然而就像你偶然间看到心灵鸡汤里面一句话，乍一听觉得深受启发，仔细一想其实一派胡言。尤其是故事性强的、有韵律感的那种。 但我们毕竟是实践学科，还是要措辞严谨、有一定的信息量并且确定讨论范围。这些例子让人觉得有问题就是因为太缺乏信息量了，而且把不同层次的概念混着用。 函数中的同步异步、阻塞非阻塞这是我们开发中最常遇见的场景，一个ajax调用啊，一个函数的异步版本等等。网络上试图辨析这几个词语的时候，也大都是基于函数这个层面。我本来写了好多，复盘的时候怎么看怎么不爽。为了保证本文的鲁棒性哈，我选择直接引述Tornado文档里的原话: A function blocks when it waits for something to happen before returning. A function may block for many reasons: network I/O, disk I/O, mutexes, etc. In fact, every function blocks, at least a little bit, while it is running and using the CPU. An asynchronous function returns before it is finished, and generally causes some work to happen in the background before triggering some future action in the application (as opposed to normal synchronous functions, which do everything they are going to do before returning). 这个时候可以看到，和那些解析一样，此处同步异步、阻塞非阻塞的定义是正交的！ I/O中的同步异步、阻塞非阻塞然而涉及到I/O以后，事件就变得并不简单。我们来看一个“傻傻分不清楚”的说法: Python中有个网络框架——twisted，它的reactor模式基于epoll或者poll实现(熟读《Linux》的朋友们一定知道它们被归类于同步I/O)。与此同时，网上几乎所有的文章都说twisted是异步的。 这就是不确定讨论层次乱用词的结果。解释起来很简单，“twisted是异步框架”中的“异步”和上一部分中的“异步”极为相似，就是业务代码中的异步函数调用以后自己把事情交代给框架代码，然后返回，啥都不管，等框架搞定。而“基于epoll实现”那是更底层的方式，也是我们这一部分要强调的。 我们可以想象这样的场景，对于一个在Linux平台开发的程序员来说，直接使用POSIX标准的函数(standard POSIX functions)仍然比较复杂。所以各种库和框架百花齐放对其进行封装。比如Node.js里的File System模块，就在封装POSIX函数的基础上，对每种函数都提供了同步和异步形式。 12const fs = require('fs');const data = fs.readFileSync('/file.md'); // blocks here until file is read 1234const fs = require('fs');fs.readFile('/file.md', (err, data) =&gt; &#123; if (err) throw err;&#125;); 此处的“异步”的定义和后面我们将复习的OS层面的定义一样，开始与“非阻塞”相关。此处我们引用Node.js文档里的原话: Blocking methods execute synchronously and non-blocking methods execute asynchronously. 更具体一点，这里指代的是：运行在Node.js进程里的JavaScript主线程不会被其他非JavaScript操作(比如网络I/O、磁盘I/O)所阻塞。 而且，不同库、框架的作者提供这种异步语义的语法也不尽相同。 如Python的twisted框架，以提供reactor module这样的方式，封装了epoll这样的同步阻塞I/O(或者说基于epoll实现)。而Node.js中，就以回调函数的语法，封装了异步非阻塞I/O(当然它在Linux下的实现并非直接封装原生API)。至于Python的asyncio库，则以生成器、协程的方式呈现(当然语法从3.3时的yield from变成了3.5中的async/await)，其实现主要也是基于多路复用。 1234567def hello():print 'Hello from the reactor loop!'print 'Lately I feel like I\'m stuck in a rut.'from twisted.internet import reactorreactor.callWhenRunning(hello)print 'Starting the reactor.'reactor.run() 123456const fs = require('fs');fs.unlink('/tmp/hello', (err) =&gt; &#123; if (err) throw err; console.log('successfully deleted /tmp/hello');&#125;); 1234567891011121314import asyncio@asyncio.coroutinedef hello(): print("Hello world!") # 异步调用asyncio.sleep(1): r = yield from asyncio.sleep(1) print("Hello again!")# 获取EventLoop:loop = asyncio.get_event_loop()# 执行coroutineloop.run_until_complete(hello())loop.close() 操作系统层次Linux中的同步、异步I/O而Linux中对同步I/O、异步I/O的定义又和“阻塞、非阻塞”捆绑了在一起。 对于“阻塞、非阻塞”这个概念，熟悉操作系统的朋友们应该很好理解了——正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。 POSIX中同步、异步I/O的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 可以看到，两者的区别就在于Synchronous I/O做“I/O operation”的时候会将进程阻塞。按照这个定义，Blocking I/O，Non-Blocking I/O，I/O Multiplexing都属于Synchronous I/O。 有人会说，Non-Blocking I/O并没有被阻塞啊。这里有个非常“鸡贼”的地方，定义中所指的“I/O operation”是指真实的I/O操作，就是recvfrom这个系统调用。Non-Blocking I/O在执行recvfrom的时候，如果内核的数据没有准备好，这时候不会阻塞进程。但是，当内核中数据准备好的时候，recvfrom会将数据从内核拷贝到用户内存中，这个时候进程是被阻塞了，在这段时间内，进程是被阻塞的。 而Asynchronous I/O则不一样，当进程发起I/O操作之后，就直接返回再也不理睬了，直到内核发送一个信号，告诉进程说I/O完成。在这整个过程中，进程完全没有被阻塞。 这张图就蛮清晰地说明了Linux下5种I/O模型之间的区别。我们可以发现Non-Blocking I/O和Asynchronous I/O的区别还是很明显的。在Non-Blocking I/O中，虽然进程大部分时间都不会被阻塞，但是它仍然要求进程去主动的检查，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而Asynchronous I/O则完全不同。它就像是用户进程将整个I/O操作交给了内核完成，内核做完后发信号通知。在此期间，用户进程不需要去检查I/O操作的状态，也不需要主动地去拷贝数据。 Windows里的同步、异步I/O In synchronous file I/O, a thread starts an I/O operation and immediately enters a wait state until the I/O request has completed. A thread performing asynchronous file I/O sends an I/O request to the 内核 by calling an appropriate function. If the request is accepted by the 内核, the calling thread continues processing another job until the 内核 signals to the thread that the I/O operation is complete. It then interrupts its current job and processes the data from the I/O operation as necessary. 可以看到哈，Windows里的定义也是引入了”wait state”这样的概念，和Linux的说法大同小异。 小结小小总结一下，日常中的同步和异步仅仅是一组语义或者通信机制(就像程序设计语言的特性)，具体到操作系统层面的实现则有不同的定义(就像不同编译器之间的实现)。 我们讨论同步异步的时候，需要说明针对哪一个通信层次。比如异步编程框架是说框架内的业务代码相对于框架是异步的，而框架相对于操作系统的系统调用还需要具体情况具体分析。 以Node.js的异步I/O实现为例。Windows下主要通过IOCP来向系统内核发送I/O调用和从内核获取已完成的I/O操作，配以事件循环，以此完成异步I/O的过程。在Linux下通过epoll实现这个过程，FreeBSD下通过kqueue实现，Solaris下通过Event ports实现。不同的是线程池在Windows下由内核（IOCP）直接提供(算是真正的异步方案)，*nix系列下由libuv自行实现(因为Linux原生提供的支持有缺陷，无法利用系统缓存)。 详解在上一Part，我们已经有了基本的辨析，但这还不够。虽然我们用看似“一针见血”的方式直观地感受了“异步”的魅力，但我们必须引入更多的信息才能让自己的理解更准确。就是一个把书读厚、把书读薄的过程啦~ 当然，整理完这篇文章的时候，我发现貌似把书读得太厚了一点点（逃 我们什么时候忘掉了异步？“异步”这个名词的大规模流行是在Web 2.0浪潮中，它伴随着AJAX的第一个A（Asynchronous）席卷了世界。在Node出现之前，最习惯异步编程的程序员莫过于前端/GUI工程师了。其中充斥的各种Ajax和事件，都是典型的异步应用场景。 但事实上，异步早就存在于操作系统的底层。在底层系统中，异步通过信号量、消息等方式有了广泛的应用。意外的是，在绝大多数高级编程语言中，异步并不多见，疑似被屏蔽了一般。对应到我们的本科生活中就好比，我们学完操作系统之后，开心地做Web开发，结果就再也没记起来还有异步这回事。 使用PHP开发的同学最能体会这个观点。PHP对调用层不仅屏蔽了异步，甚至连多线程都不提供。PHP语言从头到脚都是以同步阻塞的方式来执行的。它的优点十分明显，利于程序员顺序编写业务逻辑；它的缺点在小规模站点中基本不存在，但是在复杂的网络应用中，阻塞导致它无法更好地并发。 而在其他语言中，尽管可能存在异步的API，但是程序员还是习惯采用同步的方式来编写应用。在众多高级编程语言或运行平台中，将异步作为主要编程方式和设计理念的，Node是首个！ 高性能的服务器软件 = 事件驱动 + 异步I/O上面这个公式是Node.js的作者Ryan Dahl在与高性能服务器软件斗智斗勇很多年以后得到的经验。 而JavaScript比C的开发门槛要低，相比Lua其历史包袱几乎为零。另外，JavaScript在浏览器中有广泛的事件驱动方面的应用，暗合Ryan Dahl基于事件驱动的需求。当时，Chrome浏览器的JavaScript引擎V8横空出世，保证了JavaScript的高性能，因此JavaScript就到了北京成了Node.js的实现语言。 作为后端JavaScript的运行平台，Node保留了前端浏览器JavaScript中那些熟悉的接口，没有改写语言本身的任何特性，依旧基于作用域和原型链。成功将前端中广泛运用的思想迁移到了服务器端。事实上，这是一个很重要的因果关系——Node.js只是保留了熟悉的语义，而不是受限于JavaScript的特性所以只能这么实现。这个我们在后面请求调用的时候还会提到。 与Node的事件驱动、异步I/O设计理念比较相近的一个知名产品为Nginx。Nginx采用纯C编写，性能表现非常优异。它们的区别在于，Nginx具备面向客户端管理连接的强大能力，但是它的背后依然受限于各种同步方式的编程语言。但Node却是全能选手，它既可以作为服务器端去处理客户端的大量并发请求，也能作为客户端向网络中的各个应用进行并发请求。 单线程对于事件驱动和异步，大家已经很熟悉了，但JavaScript的特性里还有一个小兄弟还没有介绍——单线程。 当然，这里的单线程仅仅只是JavaScript执行在单线程中罢了。在Node中，无论是*nix还是Windows平台，内部完成I/O任务的另有线程池。 Node保持了JavaScript在浏览器中单线程的特点。而且在Node中，JavaScript与其余线程是无法共享任何状态的。单线程的最大好处是不用像多线程编程那样处处在意状态的同步问题，这里没有死锁的存在，也没有线程上下文交换所带来的性能上的开销。 同样，单线程也有它自身的弱点，这些弱点是学习Node的过程中必须要面对的。 具体有以下3方面: 无法利用多核CPU。 错误会引起整个应用退出。 大量计算占用CPU导致无法继续调用异步I/O。 Node采用与HTML5中Web Workers规范相同的思路来解决单线程中大计算量的问题：child_process。 子进程的出现，意味着Node可以从容地应对单线程在健壮性和无法利用多核CPU方面的问题。通过将计算分发到各个子进程，可以将大量计算分解掉，然后再通过进程之间的事件消息来传递结果，这可以很好地保持应用模型的简单和低依赖。通过Master-Worker的管理方式，也可以很好地管理各个工作进程，以达到更高的健壮性。关于如何通过子进程来充分利用硬件资源和提升应用的健壮性，这是一个值得探究的话题。此处我们暂且不表~ 我们为啥需要异步I/O?用户体验异步的概念之所以首先在Web 2.0中火起来，是因为在浏览器中JavaScript在单线程上执行，而且它还与UI渲染共用一个线程。这意味着JavaScript在执行的时候UI渲染和响应是处于停滞状态的，如果没有异步的话，用户体验会相当糟糕。同理，异步也可以减少前端获取后端资源的时间。这些都为提高用户体验打下了基础。 资源分配单线程同步编程模型会因阻塞I/O导致硬件资源得不到更优的使用。多线程编程模型也因为编程中的死锁、状态同步等问题让开发人员头疼。Node在两者之间给出了它的方案：利用单线程，远离多线程死锁、状态同步等问题；利用异步I/O，让单线程远离阻塞，以更好地使用CPU。 OS对异步I/O实现的支持情况限于文章篇幅，此处主要以UNIX系统中网络I/O为例。 背景知识文件描述符fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。 缓存 I/O缓存I/O又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存(page cache)中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。但数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 I/O 模型阻塞 I/O（Blocking I/O）在Linux中，默认情况下所有的Socket都是Blocking I/O，一个典型的读操作流程大概是这样：当用户进程调用了recvfrom这个系统调用，内核就开始了I/O的第一个阶段：准备数据（对于网络I/O来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候内核就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（因为进程默认将Socket设置成阻塞）。当内核一直等到数据准备好了，它就会将数据从内核中拷贝到用户内存，然后内核返回结果，用户进程才解除阻塞的状态，重新运行起来。 Blocking I/O的特点就是在I/O执行的两个阶段都被阻塞了。 非阻塞 I/O（Non-Blocking I/O）Linux下，可以通过设置Socket使其变为Non-blocking。当对一个Non-blocking Socket执行读操作时，流程是这个样子： 当用户进程发出read操作时，如果内核中的数据还没有准备好，那么它并不会阻塞用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦内核中的数据准备好了，并且又再次收到了用户进程的系统调用，那么它马上就将数据拷贝到了用户内存，然后返回。 当一个应用进程像这样对一个非阻塞描述符循环调用read时，我们称之为轮询(polling)。这样做往往耗费大量CPU时间，不过这种模型偶尔也会遇到，通常在某种专一系统中才能遇到。 Non-Blocking I/O的特点是用户进程需要不断的主动询问内核数据好了没有。 I/O 多路复用（ I/O multiplexing）I/O multiplexing就是我们说的select，poll，epoll，有些地方也称这种I/O方式为event driven I/O。select/epoll的好处就在于单个process就可以同时处理多个网络连接的I/O。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有Socket，当某个Socket有数据到达了，就通知用户进程。 换句话说，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 当用户进程调用了select，那么整个进程会被阻塞，而同时，内核会“监视”所有select负责的Socket，当任何一个Socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从内核拷贝到用户进程。 I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 这个图和Blocking I/O的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个系统调用 (select 和 recvfrom)，而Blocking I/O只调用了一个系统调用 (recvfrom)。但是，用select的优势在于它可以同时处理多个连接。此时的socket应该采用非阻塞模式。 所以，如果处理的连接数不是很高的话，使用select/epoll的Web server不一定比使用多线程 + Blocking I/O的Web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。 在I/O multiplexing Model中，实际中，对于每一个Socket，一般都设置成为Non-blocking。但是，如上图所示，整个用户的进程其实是一直被阻塞的。只不过进程阻塞在select或poll上，而不是被Socket I/O给阻塞。因为它等待事件的发生然后对其作相应的反应。正因为如此，它也被称作事件循环。 一个真正reactor模式的实现是需要实现循环独立抽象出来并具有如下的功能： 监视一系列与你I/O操作相关的文件描述符（description) 不停地向你汇报那些准备好的I/O操作的文件描述符 一个设计优秀的reactor模式实现需要做到： 处理所有不同系统会出现的I/O事件 提供优雅的抽象来帮助你在使用reactor时少花些心思去考虑它的存在 提供你可以在抽象层外使用的公共协议实现。 为什么I/O多路复用要搭配非阻塞I/O？ 先设想如下场景：假如 socket 的读缓冲区中已有足够多的数据，需要调用三次 read 才能读取完。或 ACCEPT 队列已经有三个「握手已完成的连接」。非阻塞 I/O 的处理方式：循环的 read 或 accept，直到读完所有的数据（抛出 EWOULDBLOCK 异常）。阻塞 I/O 的处理方式：每次只能调用一次 read 或 accept，因为多路复用只会告诉你 fd 对应的 socket 可读了，但不会告诉你有多少的数据可读，所以在 handle_read/handle_accept 中只能read/accept 一次，你无法知道下一次 read/accept 会不会发生阻塞。所以只能等 ioloop 的第二次循环，ioloop 告诉你 fd 可用后再继续调用 handle_read/handle_accept 处理，然后再循环第三次。 我们会发现，后者的处理方式要复杂很多，稍不注意就会阻塞整个进程。 如果上述的解释中，采用非阻塞I/O仅仅算是一种最佳实践的话，那么下面几种情况，必须采用非阻塞I/O: 在边缘触发的环境: ET模式指的是当数据从无到有时，才通知该fd。数据读不完，也不会再次通知，所以read时一定要采用循环的方式一直读到read函数返回-1为止。此时采用阻塞的read，那么就阻塞了整个线程。 在多线程环境: 惊群现象，就是一个典型场景，多个进程或者线程通过 select 或者 epoll 监听一个 listen socket，当有一个新连接完成三次握手之后，所有进程都会通过 select 或者 epoll 被唤醒，但是最终只有一个进程或者线程 accept 到这个新连接，若是采用了阻塞 I/O，没有accept 到连接的进程或者线程就 block 住了。 在触发select bug的时候: man 2 select「BUGS」节： Under Linux, select() may report a socket file descriptor as “ready for reading”, while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block. 当某个socket接收缓冲区有新数据分节到达，然后select报告这个socket描述符可读，但随后，协议栈检查到这个新分节检验和错误，然后丢弃这个分节，这时候调用read则无数据可读，如果socket没有被设置nonblocking，此read将阻塞当前线程。可以看出，select返回某个描述符读写就绪，并不意味着接下来的读写操作全过程就一定不会阻塞。所以I/O多路复用绝大部分时候是和非阻塞的socket联合使用。就算数据不会被别人读走，也可能被内核丢弃。还有文档没有明说的其它情况。 与I/O复用密切相关的另一种I/O模型是在多线程中使用阻塞式I/O。这种模型与上述模型极为相似，但它没有使用select阻塞在多个文件描述符上，而是使用多个线程(每个文件描述符对应一个线程)，这样每个线程都可以自由地调用read之类的阻塞式I/O系统调用了。 select &amp; poll &amp; epollselect, poll, epoll 都是I/O多路复用的具体的实现，而且他们出现是有先后顺序的。 I/O多路复用这个概念被提出来以后， select是第一个实现 (1983 左右在BSD里面实现的)。 select 被实现以后，很快就暴露出了很多问题。 select 会修改传入的参数数组，这个对于一个需要调用很多次的函数，是非常不友好的。 select 如果任何一个sock(I/O stream)出现了数据，select 仅仅会返回，但是并不会告诉你是那个sock上有数据，于是你只能自己一个一个的找。 select 只能监视1024个链接。 select 不是线程安全的，如果你把一个sock加入到select, 然后突然另外一个线程发现这个sock不用，要收回。对不起，这个select 不支持的，如果你丧心病狂的竟然关掉这个sock, select的标准行为是不可预测的， 这个可是写在文档中的哦.“If a file descriptor being monitored by select() is closed in another thread, the result is unspecified” 于是14年以后(1997年）一帮人又实现了poll, poll 修复了select的很多问题，比如: poll 去掉了1024个链接的限制。 poll 从设计上来说，不再修改传入数组，不过这个要看你的平台了，所以行走江湖，还是小心为妙。 其实拖14年那么久也不是效率问题， 而是那个时代的硬件实在太弱，一台服务器处理1千多个链接简直就是神一样的存在了，select很长段时间已经满足需求。但是poll仍然不是线程安全的， 这就意味着，不管服务器有多强悍，你也只能在一个线程里面处理一组I/O流。你当然可以那多进程来配合了，不过然后你就有了多进程的各种问题。 于是5年以后, Davide Libenzi 实现了epoll。 epoll 可以说是I/O 多路复用最新的一个实现，epoll 修复了poll 和select绝大部分问题, 比如： epoll 现在是线程安全的。 epoll 现在不仅告诉你sock组里面数据，还会告诉你具体哪个sock有数据，你不用自己去找了。 信号驱动式I/O该模型实际使用较少，在此仅简短介绍。其流程如下：开启套接字信号驱动IO功能;系统调用sigaction执行信号处理函数（非阻塞，立刻返回）;数据就绪，生成sigio信号，通过信号回调通知应用来读取数据。此种I/O方式存在的一个很大的问题：Linux中信号队列是有限制的，如果超过这个数字问题就无法读取数据。 异步 I/O（asynchronous I/O）Linux下的asynchronous I/O其实用得很少。先看一下它的流程：用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何阻塞。然后，内核会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，内核会给用户进程发送一个signal，告诉它read操作完成了。不幸的是，它有一个致命缺陷——AIO仅支持内核I/O中的O_DIRECT方式读取，导致无法利用系统缓存。 当然，系统原生层次虽然不给力，但程序员们可以模拟呀~ glibc的AIO便是典型的线程池模拟异步I/O。然而遗憾的是，它存在一些难以忍受的缺陷和bug，不推荐采用。libev的作者Marc Alexander Lehmann重新实现了一个异步I/O的库：libeio。libeio实质上依然是采用线程池与阻塞I/O模拟异步I/O。最初，Node在*nix平台下采用了libeio配合libev实现I/O部分，实现了异步I/O。在Node v0.9.3中，自行实现了线程池来完成异步I/O。 那么在Windows平台下的状况如何呢？Windows有一种独有的内核异步IO方案：IOCP。IOCP的思路是真正的异步I/O方案，调用异步方法，然后等待I/O完成通知。IOCP内部依旧是通过线程实现，不同在于这些线程由系统内核接手管理。(又是一个“语义与实现”的区别)IOCP的异步模型与Node.js的异步调用模型已经十分近似。 Node的异步I/O事件循环、观察者、请求对象、IO线程池这四者共同构成了Node异步I/O操作的基本要素。 Node.js架构理解Node的异步I/O之前先了解一下Node.js的架构！ Nodejs结构大体分为三个部分: Node.js标准库:这部分由JavaScript编写。也就是平时我们经常require的各个模块，如：http、fs、express、request。这部分在源码的lib目录下可以看到; Node bingdings:nodejs程序的main函数入口，还有提供给lib模块的C++类接口，这一层是javascript与底层C/C++沟通的桥梁，由C++编写，这部分在源码的src目录下可以看到; 最底层，支持Nodejs运行的关键。V8 引擎:用来解析、执行javascript代码的运行环境。libuv:提供最底层的IO操作接口，包括文件异步IO的线程池管理和网络的IO操作，是整个异步IO实现的核心！这部分由C/C++编写，在源码的deps目录下可以看到。 事件循环事件循环是Node.js的执行模型，回调函数被应用得这么普遍完全归功于它。 在进程启动时，Node便会创建一个类似于while(true)的循环，每执行一次循环体的过程称为Tick。每个Tick的过程就是查看是否有事件待处理，如果有，就取出事件及其相关的回调函数。如果存在关联的回调函数，就执行它们。然后进入下个循环，如果不再有事件处理，就退出进程。 观察者为了判断Tick过程中是否有事件需要处理，引入了“观察者”。每个事件循环中有一个或者多个观察者，而判断是否有事件要处理的过程就是向这些观察者询问是否有要处理的事件。 浏览器采用了类似的机制。事件可能来自用户的点击或者加载某些文件时产生，而这些产生的事件都有对应的观察者。在Node中，事件主要来源于网络请求、文件I/O等，这些事件对应的观察者有文件I/O观察者、网络I/O观察者等。观察者将事件进行了分类。 事件循环是一个典型的生产者/消费者模型。异步I/O、网络请求等则是事件的生产者，源源不断为Node提供不同类型的事件，这些事件被传递到对应的观察者那里，事件循环则从观察者那里取出事件并处理。 I/O线程池I/O线程池便是实现上述循环的基础。 NodeJavaScript在windows平台和*nix平台实现线程池的方式不同，windwos下通过IOCP方式，*nix采用自定义线程池。但是node对不同平台进行了抽象，提供了libuv抽象层。 请求对象请求对象是JavaScript发起调用到内核执行完I/O操作的过渡过程中的一种中间产物。 对于一般的非异步回调函数，函数由我们自行调用。对于Node中的异步I/O调用而言，回调函数却不由开发者来调用。那么从我们发出调用后，到回调函数被执行，中间发生了什么呢？ 123456fs.open = function(path, flags, mode, callback) &#123; binding.open(pathModule._makeLong(path), stringToFlags(flags), mode, callback);&#125;; 这段代码的调用过程大致可描述为：lib/fs.js → src/node_file.cc → uv_fs 在图中，可以看到平台判断的流程，需要说明的是，这一步是在编译的时候已经决定好的，并不是在运行时中。 具体来说，当我们调用 fs.open 时，Node.js 通过 process.binding 调用 C/C++ 层面的 Open 函数，然后通过它调用 Libuv 中的具体方法 uv_fs_open，最后执行的结果通过回调的方式传回，完成流程。 而在uv_fs_open()的调用过程中，我们创建了一个FSReqWrap请求对象。从JavaScript层传入的参数和当前方法都被封装在该对象中。而我们最关注的回调函数则被设置在该对象的oncomplete_sym属性上: 1req_wrap -&gt; object -&gt; Set(oncomplete_sym, callback); 对象包装完毕后，在Windows下，则调用QueueUserWorkItem()方法将这个FSReqWrap对象推入线程池中等待执行，该方法的代码如下所示：123QueueUserWorkItem(&amp;uv_fs_thread_proc, \ req, \ WT_EXECUTEDEFAULT) QueueUserWorkItem()方法接受3个参数：第一个参数是将要执行的方法的引用，这里引用的是uv_fs_thread_proc；第二个参数是uv_fs_thread_proc方法运行时所需要的参数；第三个参数是执行的标志。 当线程池中有可用线程时，我们会调用uv_fs_thread_proc()方法。uv_fs_thread_proc()方法会根据传入参数的类型调用相应的底层函数。以uv_fs_open()为例，实际上调用fs__open()方法。 至此，JavaScript调用立即返回，由JavaScript层面发起的异步调用的第一阶段就此结束。JavaScript线程可以继续执行当前任务的后续操作。当前的I/O操作在线程池中等待执行，不管它是否阻塞I/O，都不会影响到JavaScript线程的后续执行，如此就达到了异步的目的。 内容有点多，先小小地总结一下。我们在 Javascript 中调用的方法，最终都会通过 process.binding 传递到 C/C++ 层面，最终由他们来执行真正的操作。Node.js 即这样与操作系统进行互动。 通过这个过程，我们可以发现，实际上，Node.js 虽然说是用的 Javascript，但只是在开发时使用 Javascript 的语法来编写程序。真正的执行过程还是由 V8 将 Javascript 解释，然后由 C/C++ 来执行真正的系统调用，所以并不需要过分担心 Javascript 执行效率的问题。可以看出，Node.js 并不是一门语言，而是一个平台，这点一定要分清楚。 执行回调组装好请求对象、送入I/O线程池等待执行，实际上完成了异步I/O的第一部分，回调通知是第二部分。 线程池中的I/O操作调用完毕之后，会将获取的结果储存在req-&gt;result属性上，然后调用PostQueuedCompletionStatus()通知IOCP，告知当前对象操作已经完成：PostQueuedCompletionStatus((loop)-&gt;iocp, 0, 0, &amp;((req)-&gt;overlapped))PostQueuedCompletionStatus()方法的作用是向IOCP提交执行状态，并将线程归还线程池。通过PostQueuedCompletionStatus()方法提交的状态，可以通过GetQueuedCompletionStatus()提取。 在这个过程中，我们其实还动用了事件循环的I/O观察者。在每次Tick的执行中，它会调用IOCP相关的GetQueuedCompletionStatus()方法检查线程池中是否有执行完的请求，如果存在，会将请求对象加入到I/O观察者的队列中，然后将其当做事件处理。 I/O观察者回调函数的行为就是取出请求对象的result属性作为参数，取出oncomplete_sym属性作为方法，然后调用执行，以此达到调用JavaScript中传入的回调函数的目的。 至此，整个异步I/O的流程完全结束。 当有执行完的操作时，观察者会将它们加入到自己的队列，形成事件队列。 然后就回到了上文的事件循环，每次tick询问观察者有无事件待处理，有了上面的讲述，整个关系就打通了。 然后我们再复习一下。所谓单线程指的是Javascript运行环境的单线程，Node.js 并没有给 Javascript 执行时创建新线程的能力，最终的实际操作，还是通过 Libuv 以及它的事件循环来执行的。这也就是为什么 Javascript 一个所谓单线程的语言，能在 Node.js 里面实现异步操作的原因，两者并不冲突。在Node中，除了JavaScript的执行是单线程外，Node自身其实是多线程的，只是I/O线程使用的CPU较少。另一个需要重视的观点则是，除了用户代码无法并行执行外，所有的I/O（磁盘I/O和网络I/O等）则是可以并行起来的。 浏览器端的异步上一Part最后一段有一句“这就是Javascript一个单线程的语言，能在 Node.js 里面实现异步操作的原因。两者并不冲突”。那么问题就来了，浏览器端怎么搞的？ 有了Node.js的先验知识我们可以知道，所谓异步，本质上还是借助于多线程的宿主实现的。毕竟JavaScript只是一门脚本语言嘛~ JavaScript的单线程是指一个浏览器进程中只有一个JavaScript的执行线程，同一时刻内只会有一段代码在执行（可以使用IE的标签式浏览试试看效果，这时打开的多个页面使用的都是同一个JavaScript执行线程，如果其中一个页面在执行一个运算量较大的function时，其他窗口的JavaScript就会停止工作）。 而异步机制是浏览器的两个或以上常驻线程共同完成的，例如异步请求是由两个常驻线程：JavaScript执行线程和事件触发线程共同完成的，JavaScript的执行线程发起异步请求（这时浏览器会开一条新的HTTP请求线程来执行请求，这时JavaScript的任务已完成，继续执行线程队列中剩下的其他任务），然后在未来的某一时刻事件触发线程监视到之前的发起的HTTP请求已完成，它就会把完成事件插入到JavaScript执行队列的尾部等待JavaScript处理。 又例如定时触发（settimeout和setinterval）是由浏览器的定时器线程执行的定时计数，然后在定时时间把定时处理函数的执行请求插入到JavaScript执行队列的尾端（所以用这两个函数的时候，实际的执行时间是大于或等于指定时间的，不保证能准确定时的）。所以，所谓的JavaScript的单线程和异步更多的应该是属于浏览器的行为，他们之间没有冲突，更不是同一种事物，没有什么区别不区别的。 总之，但凡这种「既是单线程又是异步」的语言有一个共同特点：它们是 event-driven 的。驱动它们的 event 来自一个异构的平台。这些语言的 top-level 不象 C 那样是 main，而是一组 event-handler。虽然所有 event-handler 都在同一个线程内执行，但是它们被调用的时机是由那个驱动平台决定的。而且设计要求每个 event-handler 要尽快结束。未做完的工作可以通知那个异构的驱动平台来完成。所以那个驱动平台可以有许多线程。 浏览器与Node Chrome浏览器和Node的组件构成如图所示。我们知道浏览器中除了V8作为JavaScript引擎/虚拟机外，还有一个WebKit布局引擎。 HTML5在发展过程中定义了更多更丰富的API。在实现上，浏览器提供了越来越多的功能暴露给JavaScript和HTML标签。这个愿景虽然美好，但对于前端浏览器的发展现状而言，HTML5标准统一的过程是相对缓慢的。 JavaScript作为一门图灵完备的语言，长久以来却限制在浏览器的沙箱中运行，它的能力取决于浏览器中间层提供的支持有多少，不得不令人唏嘘。 除了HTML、WebKit和显卡这些UI相关技术没有支持外，Node的结构与Chrome十分相似。它们都是基于事件驱动的异步架构，浏览器通过事件驱动来服务界面上的交互，Node通过事件驱动来服务I/O， 另外还有一张图能比较清晰地描绘各组件之间的关系: 事件驱动与高性能服务器经典服务器模型下面为几种经典的服务器模型，这里对比下它们的优缺点。 同步式。对于同步式的服务，一次只能处理一个请求，并且其余请求都处于等待状态。 每进程/每请求。为每个请求启动一个进程，这样可以处理多个请求，但是它不具备扩展性，因为系统资源只有那么多。 每线程/每请求。为每个请求启动一个线程来处理。尽管线程比进程要轻量，但是由于每个线程都占用一定内存，当大并发请求到来时，内存将会很快用光，导致服务器缓慢。每线程/每请求的扩展性比每进程/每请求的方式要好，但对于大型站点而言依然不够。每线程/每请求的方式目前还被Apache所采用。但当并发增长到上万时，内存耗用的问题将会暴露出来，这即是著名的C10k问题。 为了解决高并发问题，基于事件驱动的服务模型出现了，像Node与Nginx均是基于事件驱动的方式实现的，采用单线程避免了不必要的内存开销和上下文切换开销。 Node通过事件驱动的方式处理请求，无须为每一个请求创建额外的对应线程，可以省掉创建线程和销毁线程的开销，同时操作系统在调度任务时因为线程较少，上下文切换的代价很低。这使得服务器能够有条不紊地处理请求，即使在大量连接的情况下，也不受线程上下文切换开销的影响，这是Node高性能的一个原因。 Node具有与Nginx相同的特性，不同之处在于Nginx采用纯C写成，性能较高，但是它仅适合于做Web服务器，用于反向代理或负载均衡等服务，在处理具体业务方面较为欠缺。Node则是一套高性能的平台，可以利用它构建与Nginx相同的功能，也可以处理各种具体业务，而且与背后的网络保持异步畅通。两者相比，Node没有Nginx在Web服务器方面那么专业，但场景更大，自身性能也不错。在实际项目中，我们可以结合它们各自优点，以达到应用的最优性能。 Go这里还要提一句我们的老朋友Go，因为它作为后端开发的新秀也常常被用来写并发框架、服务器程序等，可能会存在一些误解。 Web等服务端程序要处理的请求从本质上来讲是并行处理的问题，每个请求基本独立，互不依赖，几乎没有数据交互，这不是一个并发编程的模型！并发编程框架只是解决了其语义表述的复杂性，并不是从根本上提高处理的效率，也许是并发连接和并发编程的英文都是concurrent吧，很容易产生“并发编程框架和coroutine可以高效处理大量并发连接”的误解。 Go语言运行库封装了异步IO，所以可以写出貌似并发数很多的服务端，可即使我们通过调整$GOMAXPROCS来充分利用多核CPU并行处理，其效率也不如我们利用IO事件驱动设计的、按照事务类型划分好合适比例的线程池。在响应时间上，协作式调度是硬伤。 goroutine最大的价值是其实现了并发协程和实际并行执行的线程的映射以及动态扩展，随着其运行库的不断发展和完善，其性能一定会越来越好，尤其是在CPU核数越来越多的未来，终有一天我们会为了代码的简洁和可维护性而放弃那一点点性能的差别。 基于事件的服务模型存在的问题 CPU的利用率。如今CPU基本均是多核的，真正的服务器（非VPS）往往还有多个CPU。一个Node进程只能利用一个核，这将抛出Node实际应用的第一个问题：如何充分利用多核CPU服务器？ 由于所有处理都在单线程上进行，影响事件驱动服务模型性能的点在于CPU的计算能力，它的上限决定这类服务模型的性能上限，但它不受多进程或多线程模式中资源上限的影响，可伸缩性远比前两者高。如果解决掉多核CPU的利用问题，带来的性能上提升是可观的。 面对单进程单线程对多核使用不足的问题，前人的经验是启动多进程即可。理想状态下每个进程各自利用一个CPU，以此实现多核CPU的利用。所幸，Node提供了child_process模块，并且也提供了child_process.fork()函数供我们实现进程的复制。通过这些基础技术，用child_process模块在单机上搭建Node集群是件相对容易的事情。因此在多核CPU的环境下，让Node进程能够充分利用资源不再是难题。 进程的健壮性。另外，由于Node执行在单线程上，一旦单线程上抛出的异常没有被捕获，将会引起整个进程的崩溃。这给Node的实际应用抛出了第二个问题：如何保证进程的健壮性和稳定性？ 单线程的架构并不少见，其中尤以PHP最为知名——在PHP中没有线程的支持。它的健壮性是由它给每个请求都建立独立的上下文来实现的。但是对于Node来说，所有请求的上下文都是统一的，它的稳定性是亟需解决的问题。当然，严格意义上讲，Node并非真正的单线程架构。Node自身还有一定的I/O线程存在，这些I/O线程由底层libuv处理，这部分线程对于JavaScript开发者而言是透明的，只在C++扩展开发时才会关注到。JavaScript代码永远运行在V8上，是单线程的。 搭建好了集群，为了保障Node应用的健壮性，我们需要建立一个完善的机制来应对，比如自动重启、负载均衡、状态共享等。 当然，上述两条解决方案由普通工程师徒手实现的话实在太痛苦了，所以Node.js在v0.8的时候就提供了cluster模块来方便解决上述两条问题。 其他平台事实上，Node的异步I/O并非首创，但却是第一个成功的平台。在那之前，也有一些知名的基于事件驱动的实现，具体如下所示: Ruby的Event Machine。 Perl的AnyEvent。 Python的Twisted。 在这些平台上采用事件驱动的方式时，需要花一定精力了解这些库。这些库没能成功的原因则是同步I/O库的存在。本章描述的异步I/O实现，其主旨是使I/O操作与CPU操作分离。奈何这些语言平台上的标准I/O库都是阻塞式的，一旦事件循环中存在阻塞I/O，将导致其余I/O无法立即进行，性能会急剧下降，其效果类似于同步式服务，其他请求将不能立即处理。因为在这些成熟的语言平台上，异步不是主流，尽管有这些事件驱动的实现库，但开发者总会习惯性地采用同步I/O库，这导致预想的高性能直接落空。Ryan Dahl在评估他最早的选型时，便以此为依据，在小本本上划掉了Lua等脚本语言。]]></content>
      <tags>
        <tag>I/O模型</tag>
        <tag>Node.js</tag>
        <tag>Unix网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝尔实验室和 CSP 线程]]></title>
    <url>%2FBell_Labs_and_CSP_Threads%2F</url>
    <content type="text"><![CDATA[CSP的来龙去脉 原作者: Russ Cox原文链接:Bell Labs and CSP Threads 译者按: 学习Go语言的时候发现了这篇小文，很是喜欢，翻译出来以飨读者。 介绍本文是并发编程历史的一部分, 侧重于Hoare的通信顺序进程 (CSP-Communication Sequential Process) [1] [1a]的沿袭与传承。这种并发编程的特色不在于执行上的效率, 而是逻辑上的清晰。作者认为只将并发编程作为提高性能的一种手段，是一种普遍的错误。例如, 以重叠磁盘I/O请求, 通过预取结果到预期查询来减少延迟, 或利用多个处理器。这些优势固然重要, 但与本讨论无关。毕竟, 它们可以在其他样式 (如异步事件驱动编程) 中实现。相反, 我们对并发编程感兴趣, 因为它提供了一种自然的抽象, 可以使某些程序更简单。 看文章之前你要忘掉的大多数计算机科学专业的本科生“被迫”阅读Andrew Birrell的“使用线程编程入门”。SRC线程模型是当前大多数线程包所使用的并发模型。该模型产生的所有问题都源于它们太底层了。与Hoare提供的通信原语不同, SRC 样式的线程模块中的基元必须与其他技术 (通常是共享内存) 相结合, 以便有效地使用。一般情况下, 程序员往往不构建自己的高级结构,并且由于需要过度注意这样底层的细节而感到沮丧。 现在, 把Birrell的教程忘得一干二净。这是一个不同的线程模型。如果您将其作为不同的线程模型来处理, 您可能会发现它更容易理解。 通信顺序进程 CSP到1978年, 有许多用于通信和同步的方法。共享内存是最常见的通信机制, 而信号量、关键区域和监视器是同步机制之一。Hoare用单一的语言原语: 同步通信，解决了这两个问题。在Hoare的 CSP 语言中, 进程通过从命名的缓冲通道发送或接收值来进行通信。由于信道是缓冲的, 因此发送操作会一直阻塞, 直到该值已转移到接收器, 从而提供了同步机制。 Hoare的示例之一是重新格式化80列卡, 以便其在125列的打印机上进行打印。在他的解决方案中，一个进程一次读取卡, 将分解后的内容字符按字符发送到第二个进程。第二个过程将组合125个字符, 将组发送到行打印机。这听起来很琐碎, 但是在没有缓冲的I/O库的情况下, 单进程解决方案所涉及的必要簿记(bookkeeping)十分繁重。事实上, 缓冲的I/O库实际上只是封装了这两种导出单字符通信接口的进程。 作为另一个例子, Hoare将其归功于Doug McIlroy。考虑产生所有的小于1000的素数。Eratosthenes的筛子可以由执行以下伪代码的管道模拟:123456p = get a number from left neighborprint ploop: n = get a number from left neighbor if (p does not divide n) send n to right neighbor 生成过程可以将数字2、3、4、…, 1000 插入到管道的左端: 第一个进程在行中消除了倍数为 2, 第二个消除了倍数 3, 第三个消除了倍数 5, 等等: 到目前为止，这些例子的线性流水线性质是对CSP的一般性质的错误描述，但是即使仅限于线性流水线，该模型也是相当强大的。事实上，Unix操作系统的过滤(filter)和流水线方法的成功已经有力地证明了这一点。[2]事实上，管道早于Hoare的论文。在1964年10月11日的内部贝尔实验室的备忘录中，Doug McIlroy 随手记录了将成为Unix流水线的想法：“我们应该有一些方法来连接程序，就像花园软管那样。当数据被另一部分需要的时候，可以直接将一部分管道与另一部分连接来传输数据。这也是一种IO的方式。”[3] Hoare的通信过程比典型的 Unix shell 管道更通用, 因为它们可以以任意模式连接。事实上,Hoare给出的一个例子是一个 3x3 的进程矩阵, 有点像素数筛子, 可以用一个 3x3 的正方形矩阵乘以一个向量。 当然, Unix 的管道机制不需要线性布局;只有 shell 语法才需要。McIlroy提出了支持通用管道shell的早期语法, 但没有喜欢到想要去实现它的地步 (源于个人对话, 2011)。后来的shell确实支持一些限制形式的非线性管道。比如，Rochkind的 2dsh 支持dags; Tom Duff的 rc 支持 trees。 Hoare的语言新颖而有影响力, 但缺乏几个关键方面。主要的缺点是用于通信的缓冲通道不是一级对象: 它们不能存储在变量中, 不能作为参数传递给函数, 也不能通过通道发送。因此, 通信结构必须在编写程序时固定。因此, 我们必须编写一个程序来打印前1000素数, 而不是第一个n素数, 并将一个向量乘以一个 3x3 矩阵而不是一个 nxn 矩阵。 Pan 和 Promela1980年，在霍尔的文章发表两年之后，Gerard Holzmann和Rob Pike创建了一个名为Pan的协议分析器，以CSP方言为输入。Pan的CSP方言有连接，选择和循环，但没有变量。尽管如此，Holzmann报告说：“Pan在1980年11月21日在贝尔实验室数据交换控制协议中发现了它的第一个错误。” [14]。这种方言很可能是贝尔实验室第一个CSP语言，而且它的确为Pike提供了使用和实现类似CSP的语言的经验。 Holzmann的协议分析仪被开发成Spin模型检查器和它的Promela语言，它与Newsqueak一样，channel成为了一等对象。 Newsqueak话分两头说，Luca Cardelli和Rob Pike也将CSP中的想法转化为用于生成用户界面代码的Squeak迷你语言 [4]。(该Squeak和Squeak Smalltalk的实现是不同的。) Pike之后扩展Squeak成成熟的编程语言Newsqueak [5] [6]。它启发了Plan 9的Alef [7] [8] ，Inferno的Limbo[9] ，和Google的Go[13]。Newsqueak超过Squeak的主要语义优势是,Newsqueak将channel视为一等对象：与CSP和Squeak不同，channel 可以存储在变量中，作为参数传递给函数，并通过通道发送。这反过来使通信结构的程序化构建成为可能，从而允许创建比手动设计更合理的结构。特别是，Doug McIlroy展示了Newsqueak的通信设施如何被用来编写优雅的程序来操纵符号幂级数 [10]。传统语言中的类似尝试往往会在簿记中陷入泥潭。Rob Pike也以类似的方式展示了如何利用通信工具来突破常见的基于事件的编程模型，编写并发窗口系统 [11]。 AlefAlef [7] [8] 是Phil Winterbottom设计的一种语言，将Newsqueak的思想应用于一个完整的系统编程语言。Alef有两种类型的我们一直在调用进程：procs和线程。该程序被组织成一个或多个特效，它们是可预先调度的共享内存操作系统进程。每个proc包含一个或多个任务，这些任务是合作安排的协同程序。请注意，每个任务都分配给一个特定的proc：他们不会在proc之间迁移。 proc的主要用途是提供可以独立于主任务而阻塞I/O的上下文。（Plan 9有没有选择调用，甚至在Unix上，如果你想与非网络重叠计算I/O。你需要多个特效）Acme的论文 [12] 有特效和线程的一个很好的简短的讨论，因为这样做的关于Plan 9窗口系统的讲义，也在下面提到。 LimboInferno操作系统是用于机顶盒的Plan 9衍生产品。其编程语言Limbo [9]深受Alef的影响。它消除了过程和任务之间的区别，虽然它们比大多数人认为的过程更轻量。所有的并行性都是抢占式的。有趣的是，尽管如此，语言并没有提供真正的锁支持。取而代之的是，channel沟通通常提供足够的同步，并鼓励程序员以“任何一块数据总是有一个明确的所有者”的原则而设计代码。显式锁定是不必要的。 Libthread回到Plan 9的世界，当Plan 9被移植到更多架构时，Alef编译器变得很难维护。Libtread最初是为了将Alef程序移植到C中而创建的，因此Alef编译器可以退役了。Alef的过程和任务在libthread中被称为过程和线程。该手册是最权威的参考。 GoRob Pike和 Ken Thompson来到谷歌, 并将CSP放在在Go语言的并发支持的中心。 入门要直观地感受该模型, 特别是进程和线程如何交互, 值得阅读 Alef 用户指南[8]。此演示文稿的前三十张幻灯片很好地介绍了 Alef 构造在 C 中的表示方式。 CSP 模型的威力的最佳例子是McIlroy和Pike的论文, 上面提到[10] [11]. Rob Pike的主页包含了关于并发编程的课程的讲义:简介, 以及有关上述两篇文章的幻灯片:[斜视]和[窗口系统]。倒数三张提供了一个很好的例子, 说明Plan 9程序通常如何使用procs和任务。 Rob Pike在Google的一个技术讲座提供了一个很好的介绍 (57 分钟的视频)。 Rob Pike在2010年与Ru​​ss Cox的Google I/O谈话 中的一半，显示了如何使用channel和Go的并发来实现负载均衡工作管理系统。 相关资源John Reppy已经将同样的想法应用于ML，并开发了Concurrent ML。他使用CML构建了eXene多线程（非事件驱动）X Windows系统工具包。 引用[1] C. A. R. Hoare, “Communicating Sequential Processes,” Communications of the ACM 21(8) (August 1978), 666-677. [1a]C. A. R. Hoare, Communicating Sequential Processes. Prentice Hall, Englewood Cliffs, New Jersey, 1985. [2] Michael S. Mahoney, ed., The Unix Oral History Project, Release 0: The Beginning [3] M. Douglas McIlroy, [internal Bell Labs memorandum]https://swtch.com/~rsc/thread/mdmpipe.pdf), October 1964. [4] Luca Cardelli and Rob Pike, “Squeak: a Language for Communicating with Mice,” Computer Graphics, 19(3) (July 1985: SIGGRAPH ‘85 Proceedings), 199-204. [5] Rob Pike, “The Implementation of Newsqueak,” Software–Practice and Experience, 20(7) (July 1990), 649-659. [6] Rob Pike, “Newsqueak: a Language for Communicating with Mice,” Computing Science Technical Report 143, AT&amp;T Bell Laboratories, Murray Hill, 1989. [7] Phil Winterbottom, “Alef Language Reference Manual,” in Plan 9 Programmer’s Manual: Volume Two, AT&amp;T, Murray Hill, 1995. [8] Bob Flandrena, “Alef Users’ Guide,” in Plan 9 Programmer’s Manual: Volume Two, AT&amp;T, Murray Hill, 1995. [9] Dennis M. Ritchie, “The Limbo Programming Language,” in Inferno Programmer’s Manual, Volume 2, Vita Nuova Holdings Ltd., York, 2000. 10] M. Douglas McIlroy, “Squinting at Power Series,” Software–Practice and Experience, 20(7) (July 1990), 661-683. [11] Rob Pike, “A Concurrent Window System,” Computing Systems, 2(2) 133-153. [12] Rob Pike, “Acme: A User Interface for Programmers,” Proceedings of the Winter 1994 USENIX Conference, 223-234. [13] golang.org, “The Go Programming Language”. [14] Gerard Holzmann, “Spin’s Roots”. [15] Gerard Holzmann, [“Promela Language Reference”]http://spinroot.com/spin/Man/promela.html().]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统学习笔记（四）- GFS]]></title>
    <url>%2FGFS%2F</url>
    <content type="text"><![CDATA[GFS——Google分布式存储的基石。 GFS是构建在廉价服务器上的大型分布式系统。它将服务器故障视为正常现象，通过集成持续的监控、错误侦测、灾难冗余以及自动恢复的机制以实现自动容错。由于文件非常巨大，重新考虑了I/O 操作和 Block 的尺寸。针对海量文件的访问模式，客户端对数据块缓存是没有意义的，为了性能优化和原子性保证，GFS中绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。应用程序和文件系统 API 的协同设计提高了整个系统的灵活性。在保证系统可靠性和稳定性的同时，大幅降低了系统成本。 设计假设 系统是构建在很多廉价的、普通的组件上，组件会经常发生故障。它必须不间断监控自己、侦测错误，能够容错和快速恢复。系统存储了适当数量的大型文件，GFS预期几百万个，每个通常是100MB或者更大，即使是GB级别的文件也需要高效管理。也支持小文件，但是不需要着重优化。 系统主要面对两种读操作：大型流式读和小型随机读。在大型流式读中，单个操作会读取几百KB，也可以达到1MB或更多。相同客户端发起的连续操作通常是在一个文件读取一个连续的范围。小型随机读通常在特定的偏移位置上读取几KB。重视性能的应用程序通常会将它们的小型读批量打包、组织排序，能显著的提升性能。 也会面对大型的、连续的写，将数据append到文件。append数据的大小与一次读操作差不多。一旦写入，几乎不会被修改。不过在文件特定位置的小型写也是支持的，但没有着重优化。 系统必须保证多客户端对相同文件并发append的高效和原子性。GFS的文件通常用于制造者消费者队列或者多路合并。几百个机器运行的制造者，将并发的append到一个文件。用最小的同步代价实现原子性是关键所在。文件被append时也可能出现并发的读。 持久稳定的带宽比低延迟更重要。GFS更注重能够持续的、大批量的、高速度的处理海量数据，对某一次读写操作的回复时间要求没那么严格。接口GFS提供了一个非常亲切的文件系统接口，尽管它没有全量实现标准的POSIX API。像在本地磁盘中一样，GFS按层级目录来组织文件，一个文件路径（path）能作为一个文件的唯一ID。GFS支持常规文件操作，比如create、delete、open、close、read和write。 除了常规操作，GFS还提供快照和record append操作。快照可以用很低的花费为一个文件或者整个目录树创建一个副本。record append允许多个客户端并发的append数据到同一个文件，而且保证它们的原子性。这对于实现多路合并、制造消费者队列非常有用，大量的客户端能同时的append，也不用要考虑锁等同步问题。 系统架构GFS节点的三种角色: Master ChunkServer——CS ClientGFS文件被划分为固定大小的chunk，由Master在创建时分配一个64位、全局唯一的chunk句柄。CS以普通的Linux文件的形式将chunk存储在磁盘中。为保证可靠性，chunk在不同的机器中复制多份，默认为三份。 主控服务器中维护了系统的元数据，包括文件及chunk命名空间、文件到chunk之间的映射、chunk位置信息。它也负责整个系统的全局控制，如chunk租约管理、垃圾回收无用chunk、chunk复制等。Master会定期与CS通过心跳的方式交换信息。 GFS 客户端代码以库的形式被链接到客户程序里。客户端代码实现了 GFS 文件系统的 API 接口函数、应用程序与 Master 节点和 Chunk 服务器通讯、以及对数据进行读写操作。客户端和 Master 节点的通信只获取元数据，所有的数据操作都是由客户端直接和 Chunk 服务器进行交互的。GFS不提供 POSIX 标准的 API 的功能，因此，GFS API调用不需要深入到Linux vnode 级别。客户端访问GFS时，首先访问Master，获取与之交互的CS信息，然后直接访问这些CS，完成数据存取工作。 GFS中的Client不缓存文件数据，只缓存Master中获取的元数据，这是由GFS的应用特点决定的。GFS的主要应用就是MapReduce与Bigtable。对于MapReduce,GFS client使用方式为顺序读写，没有缓存文件数据的必要;而Bigtable作为分布式表格系统，内部实现了一套缓存机制。何况，如何维护客户端缓存与实际数据之间的一致性是一个极其复杂的问题。chunkserver不需要缓存文件数据因为chunk被存储为本地文件，Linux提供的OS层面的buffer缓存已经保存了频繁访问的文件。 关键问题租约机制GFS数据追加以记录为单位，每个记录的大小为几十KB到几MB不等，如果每次记录追加都需要请求Master，那么Master显然会成为系统的性能瓶颈。因此GFS通过租约(lease)机制将chunk写操作授权给chunk。拥有租约授权的CS称为主CS，其他副本所在的CS称为备CS。租约授权针对单个chunk，在租约有效期内，对该chunk的写操作都由主ChunkServer负责，从而减轻Master的负载。一般来说，租约的有效期比较长，比如60秒，只要没有出现异常，主CS可以不断向Master请求延长租约的有效期直到整个chunk写满。 假设chunk A在GFS中保存了三个副本——A1(主)、A2、A3。若A2所在CS下线后由重新上线，并且在A2下线的过程中，副本A1和A3有更新，那么A2需要被Master当成垃圾回收掉。GFS通过对每个chunk维护一个版本号来解决，每次给CS进行租约授权或者主CS重新延长有效期时，Master会将chunk的版本号加1。对应该场景，A2的版本号太低，从而被标记成可删除的chunk。 一致性模型GFS 支持一个宽松的一致性模型，这个模型能够很好的支撑GFS的高度分布的应用，同时还保持了相对简单且容易实现的优点。本节GFS讨论 GFS 的一致性的保障机制，以及对应用程序的意义。 GFS一致性保障机制文件命名空间的修改（例如，文件创建）是原子性的。它们仅由 Master 节点的控制：命名空间锁提供了原子性和正确性的保障；Master 节点的操作日志定义了这些操作在全局的顺序。数据修改后文件 region的状态取决于操作的类型、成功与否、以及是否同步修改。表 1 总结了各种操作的结果。 先来看看两种操作结果: 如果所有客户端，无论从哪个副本读取，读到的数据都一样，那么我们认为文件 region 是“一致的”； 如果对文件的数据修改之后，region 是一致的，并且客户端能够看到写入操作全部的内容，那么这个 region是“已定义的”。 当一个数据修改操作成功执行，并且没有受到同时执行的其它写入操作的干扰，那么影响的 region 就是已定义的（隐含了一致性）：所有的客户端都可以看到写入的内容。 并行修改操作成功完成之后，region 处于一致的、未定义的状态：所有的客户端看到同样的数据，但是无法读到任何一次写入操作写入的数据。通常情况下，文件 region 内包含了来自多个修改操作的、混杂的数据片段。失败的修改操作导致一个 region 处于不一致状态（同时也是未定义的）：不同的客户在不同的时间会看到不同的数据。后面我们将描述应用如何区分已定义和未定义的 region。应用程序没有必要再去细分未定义 region 的不同类型。 再看看两种操作:GFS主要是为了追加(append)而不是改写(overwrite)而设计的。一方面是因为改写的需求比较少，或者可以通过追加来实现。比如可以只使用GFS的追加功能构建分布式表格系统Bigtable;另一方面是因为追加的一致性模型相比改写更加简单有效。若改写失败，之后读操作可能读到不正确的数据；若追加失败，读操作只是读到过期而不是错误的数据。因为改写操作指定一个变动发生的偏移量，而追加记录操作不会。在前一种情况下，变动的位置是预先确定的，而后一种情况下，是由系统决定的。并发写入到相同位置是不可序列化的，还可能导致文件区域的损坏。 对于追加记录操作，GFS保证追加操作会至少发生一次且是原子的(即作为一个连续的字节序列)，但系统并不保证块的所有副本相同。若出现异常，失败的副本可能会出现一些可识别的填充(padding)记录或者重复数据。这些数据占据的文件 region 被认定是不一致的，这些数据通常比用户数据小的多。这里的复制策略是考虑到GFS的应用程序和服务可以容忍一致性放松后的语义，是针对特定领域的，且削弱了一致性的保证。 追加流程 客户机向 Master 节点询问哪一个 Chunk 服务器持有当前的租约，以及其它副本的位置。如果没有一个Chunk 持有租约，Master 节点就选择其中一个副本建立一个租约（这个步骤在图上没有显示）。 Master 节点将主 Chunk 的标识符以及其它副本（又称为 secondary 副本、二级副本）的位置返回给客户机。客户机缓存这些数据以便后续的操作。只有在主 Chunk 不可用，或者主 Chunk 回复信息表明它已不再持有租约的时候，客户机才需要重新跟 Master 节点联系。 客户机把数据推送到所有的副本上。客户机可以以任意的顺序推送数据。Chunk 服务器接收到数据并保存在它的内部 LRU 缓存中，一直到数据被使用或者过期交换出去。由于数据流的网络传输负载非常高，通过分离数据流和控制流，GFS可以基于网络拓扑情况对数据流进行规划，提高系统性能，而不用去理会哪个Chunk 服务器保存了主 Chunk。 当所有的副本都确认接收到了数据，客户机发送写请求到主 Chunk 服务器。这个请求标识了早前推送到所有副本的数据。主 Chunk 为接收到的所有操作分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。它以序列号的顺序把操作应用到它自己的本地状态中（alex 注：也就是在本地执行这些操作，这句话按字面翻译有点费解，也许应该翻译为“它顺序执行这些操作，并更新自己的状态”）。 主 Chunk 把写请求传递到所有的二级副本。每个二级副本依照主 Chunk 分配的序列号以相同的顺序执行这些操作。 所有的二级副本回复主 Chunk，它们已经完成了操作。 主 Chunk 服务器20回复客户机。任何副本产生的任何错误都会返回给客户机。在出现错误的情况下，写入操作可能在主 Chunk 和一些二级副本执行成功。（如果操作在主 Chunk 上失败了，操作就不会被分配序列号，也不会被传递。）客户端的请求被确认为失败，被修改的 region 处于不一致的状态。GFS的客户机代码通过重复执行失败的操作来处理这样的错误。在从头开始重复执行之前，客户机会先从步骤3到步骤7做几次尝试。 如果应用程序一次写入的数据量很大，或者数据跨越了多个 Chunk，GFS 客户机代码会把它们分成多个写操作。这些操作都遵循前面描述的控制流程，但是可能会被其它客户机上同时进行的操作打断或者覆盖。因此，共享的文件 region 的尾部可能包含来自不同客户机的数据片段，尽管如此，由于这些分解后的写入操作在所有的副本上都以相同的顺序执行完成，Chunk 的所有副本都是一致的。这使文件 region 处于一致的、但是未定义的状态。 提高网络效率分离控制流和数据流！ 目标:充分利用每台机器的带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时。 为了充分利用每台机器的带宽:数据沿着一个Chunk服务器链顺序的推送，而不是以其他拓扑形式分散推送(如，树形拓扑结构)。线性推送模式下，每台机器所有的出口带宽都用于以最快的速度传输数据，而不是在多个接收者之间分配带宽。 为了尽可能地避免出现网络瓶颈和高延迟的链接:每台机器都尽量地在网络拓扑中选择一台还没有接收到数据的、离自己最近的机器作为目标推送数据。由于网络拓扑简单，通过IP地址就可以计算出节点的”距离”。 最小化延迟:利用基于TCP连接的、管道式数据推送方式来减少延时。当一个ChunkServer接收到一些数据，它就立即开始转发。由于采取全双工网络，立即发送数据并不会降低接收数据的速率。抛开网络阻塞，传输B个字节到R个副本的理想时间是B/T+RL，T是网络吞吐量，L是在两台机器数据传输的延迟。比如网速100Mbps,L远小于1ms的情况下，1MB数据80ms左右就能分发出去。 容错机制和检验高可用Google使用两条简单但有效的策略保证整个系统的高可用性:快速恢复和复制。 快速恢复master和chunkserver都被设计成都能在秒级别重启。 Master复制通过操作日志和checkpoint的方式进行，并且有一台称为”Shadow Master”的实时热备。 master的operation log和checkpoint都会复制到多台机器上，要保证这些机器的写都成功了，才认为是成功。只有一台master在来做garbage collection等后台操作。当master挂掉后，它能在很多时间内重启；当master所在的机器挂掉后，监控会在其他具有operation log的机器上重启启动master。 新启动的master只提供读服务，因为可能在挂掉的一瞬间，有些日志记录到primary master上，而没有记录到secondary master上（这里GFS没有具体说同步的流程）。 Chunk复制每个chunk在多个机架上有副本，副本数量由用户来指定。当chunkserver不可用时，GFS master会自动的复制副本，保证副本数量和用户指定的一致。 数据完整性每个chunkserver都会通过checksum来验证数据是否损坏的。 每个chunk被分成多个64KB的block，每个block有32位的checksum，checksum在内存中和磁盘的log中都有记录。 对于读请求，chunkserver会检查读操作所涉及block的所有checksum值是否正确，如果有一个block的checksum不对，那么会报错给client和master。client这时会从其他副本读数据，而master会clone一个新副本，当新副本clone好后，master会删除掉这个checksum出错的副本。 诊断工具主要是通过log，包括重要事件的log(chunkserver上下线)，RPC请求，RPC响应等。 Master设计Master内存占用Master在内存中维护了系统中的全部元数据，包括文件及chunk命名空间、文件到chunk之间的映射、chunk副本的位置信息。其中前两者需要以记录变更的方式持久化到磁盘，chunk副本位置信息不需要持久化，可通过ChunkServer汇报获取。 貌似将元数据全部保存在内存中的方法有问题，因为Chunk数量以及整个系统的承载能力都受限于Master服务器所拥有的内存大小。但实际应用中，Master对命名空间进行了压缩存储。压缩后每个文件在文件命名空间的元数据也不超过64字节，由于GFS中的文件一般都申大文件，因此文件命名空间占用内存不多。故，Master内存容量不会称为GFS的系统瓶颈。 即便需要支持更大的系统，为Master服务器增加额外内存的费用是很少的，而通过增加有限的费用，我们就能把元数据全部保存在内存里，增强了系统的简洁性、可靠性、高性能和灵活性。 创建、重新复制、负载均衡GFS中副本分布策略需要考虑多种因素，如网络拓扑、机架分布、磁盘利用率等。为了提高系统的可用性，GFS会避免将同一个chunk的所有副本都存放在同一个机架的情况。 需要创建chunk副本的情况有三种:chunk创建、chunk复制、以及负载均衡。 chunk创建GFS在创建chunk的时候，选择chunkserver时考虑的因素包括： 磁盘空间使用率低于平均值的chunkserver。 限制每台chunkserver的最近的创建chunk的次数，因为创建chunk往往意味着后续需要写大量数据，所以，应该把写流量尽量均摊到每台chunkserver上。 chunk的副本放在处于不同机架的chunkserver上。chunk复制当一个chunk的副本数量少于预设定的数量时，需要做复制的操作，例如，chunkserver宕机，副本数据出错，磁盘损坏，或者设定的副本数量增加。 chunk的复制的优先级是按照下面的因素来确定的： 丢失两个副本的chunk比丢失一个副本的chunk的复制认为优先级高 文件正在使用比文件已被删除的chunk的优先级高 阻塞了client进程的chunk的优先级高 chunk复制的时候，选择新chunkserver要考虑的点： 磁盘使用率 单个chunkserver的复制个数限 多个副本需要在多个机架 集群的复制个数限制 限制每个chunkserver的复制网络带宽，通过限制读流量的速率来限制负载均衡周期性地检查副本分布情况，然后调整到更好的磁盘使用情况和负载均衡。GFS master对于新加入的chunkserver，逐渐地迁移副本到上面，防止新chunkserver带宽打满。垃圾回收在GFS删除一个文件后，并不会马上就对文件物理删除，而是在后面的定期清理的过程中才真正的删除。 具体地，对于一个删除操作，GFS仅仅是写一条日志记录，然后把文件命名成一个对外部不可见的名称，这个名称会包含删除的时间戳。GFS master会定期的扫描，当这些文件存在超过3天后，这些文件会从namespace中删掉，并且内存的中metadata会被删除。 在对chunk namespace的定期扫描时，会扫描到这些文件已经被删除的chunk，然后会把metadata从磁盘中删除。 在与chunkserver的heartbeat的交互过程中，GFS master会把不在metadata中的chunk告诉chunkserver，然后chunkserver就可以删除这些chunk了。 采用这种方式删除的好处： 利用心跳方式交互，在一次删除失败后，还可以通过下次心跳继续重试操作 删除操作和其他的全局扫描metadata的操作可以放到一起做 坏处： 有可能有的应用需要频繁的创建和删除文件，这种延期删除方式会导致磁盘使用率偏高，GFS提供的解决方案是，对一个文件调用删除操作两次，GFS会马上做物理删除操作，释放空间。快照Snapshot的整个流程如下： client向GFS master发送Snapshot请求。 GFS master收到请求后，会回收所有这次Snapshot涉及到的chunk的lease。 当所有回收的lease到期后，GFS master写入一条日志，记录这个信息。然后，GFS会在内存中复制一份snapshot涉及到的metadata。 当snapshot操作完成后，client写snapshot中涉及到的chunk C的流程如下: client向GFS master请求primary chunkserver和其他chunkserver。 GFS master发现chunk C的引用计数超过1，即snapshot和本身。它会向所有有chunk C副本的chunkserver发送创建一个chunk C的拷贝请求，记作是chunk C’，这样，把最新数据写入到chunk C’即可。本质上是copy on write。 ChunkServer设计ChunkServer管理大小约为64MB的chunk，存储的时候需要保证chunk尽可能均匀地分布在不同的磁盘之中，需要考虑的因素包括:磁盘空间、最近新建chunk数等。另外，Linux文件系统删除64MB大文件消耗的时间太长且没有必要，因此，删除chunk的时候可只将其对应的chunk文件移动到磁盘回收站。以后新建chunk时可重用。 ChunkServer是一个磁盘和网络I/O密集型应用，为了最大限度地发挥机器性能，需要能够做到将磁盘和网络操作异步化。 观感评价GFS是一个具有良好可扩展性并能够在软件层面自动处理各种异常情况的系统。由于Google的系统一开始能很好地解决可扩展性的问题，所以后续的系统能够构建在前一个系统之上并且一步一步引入新的功能。Bigtable在GFS之上将海量数据组织成表格形式，Megastore、Spanner又进一步在Bigtable之上融合一些关系型数据库的功能。 自动化容错是GFS的亮点所在，在设计GFS时认为节点失效是常态，通过在软件层面进行故障检测，并且通过chunk复制操作将原有故障节点的服务迁移到新的节点。系统还会根据一定的策略，如磁盘使用情况、机器负载等执行负载均衡操作。由于软件层面的自动化容错，底层可采用廉价的硬件，从而大大降低云服务的成本。 单Master设计是可行的。事实上，绝大多数分布式存储系统都和GFS一样依赖单总控节点。其中用于满足Master元数据管理的写时复制B树实现相当复杂。 问答根据《大规模分布式存储系统》中的一组问题，来检验一下学习成果！ 1）为什么存储三个副本？而不是两个或者四个？两个副本不足以保证高可用性。四个副本成本较高。 2）Chunk的大小为何选择64MB？这个选择主要基于哪些考虑？优点 可以减少GFS client和GFS master的交互次数，chunk size比较大的时候，多次读可能是一块chunk的数据，这样，可以减少GFS client向GFS master请求chunk位置信息的请求次数。 对于同一个chunk，GFS client可以和GFS chunkserver之间保持持久连接，提升读的性能。 chunk size越大，chunk的metadata的总大小就越小，使得chunk相关的metadata可以存储在GFS master的内存中。 缺点 chunk size越大时，可能对部分文件来讲只有1个chunk，那么这个时候对该文件的读写就会落到一个GFS chunkserver上，成为热点。 64MB应该是google得出的一个比较好的权衡优缺点的经验值。 3）GFS主要支持追加（append）、改写（overwrite）操作比较少。为什么这样设计？如何基于一个仅支持追加操作的文件系统构建分布式表格系统Bigtable？该特性是Google根据现有应用需求而确定的。 4）为什么要将数据流和控制流分开？如果不分开，如何实现追加流程？更有效地利用网络带宽。 5）GFS有时会出现重复记录或者补零记录（padding），为什么？ padding出现场景： last chunk的剩余空间不满足当前写入量大小，需要把last chunk做padding，然后告诉客户端写入下一个chunk append操作失败的时候，需要把之前写入失败的副本padding对齐到master 重复记录出现场景： append操作部分副本成功，部分失败，然后告诉客户端重试，客户端会在成功的副本上再次append，这样就会有重复记录出现 6）租约（Lease）是什么？在GFS起什么作用？它与心跳（heartbeat）有何区别？lease是gfs master把控制写入顺序的权限下放给chunkserver的机制，以减少gfs master在读写流程中的参与度，防止其成为系统瓶颈。心跳是gfs master检测chunkserver是否可用的标志。 7）GFS追加操作过程中如果备副本（Secondary）出现故障，如何处理？如果主副本（Primary）出现故障，如何处理？对于备副本故障，写入的时候会失败，然后primary会返回错误给client。按照一般的系统设计，client会重试一定次数，发现还是失败，这时候client会把情况告诉给gfs master，gfs master可以检测chunkserver的情况，然后把最新的chunkserver信息同步给client，client端再继续重试。 对于主副本故障，写入的时候会失败，client端应该是超时了。client端会继续重试一定次数，发现还是一直超时，那么把情况告诉给gfs master，gfs master发现primary挂掉，会重新grant lease到其他chunkserver，并把情况返回给client。 8）GFS Master需要存储哪些信息？Master数据结构如何设计？namespace、文件到chunk的映射以及chunk的位置信息 namespace采用的是B-Tree，对于名称采用前缀压缩的方法，节省空间；（文件名，chunk index）到chunk的映射，可以通过hashmap；chunk到chunk的位置信息，可以用multi_hashmap，因为是一对多的映射。 9）假设服务一千万个文件，每个文件1GB,Master中存储的元数据大概占用多少内存？1GB/64MB = 1024 / 64 = 16。总共需要16 10000000 64 B = 10GB 10）Master如何实现高可用性？ metadata中namespace，以及文件到chunk信息持久化，并存储到多台机器 对metadata的做checkpoint，保证重启后replay消耗时间比较短，checkpoint可以直接映射到内存使用，不用解析 在primary master发生故障的时候，并且无法重启时，会有外部监控将secondary master，并提供读服务。secondary master也会监控chunkserver的状态，然后把primary master的日志replay到内存中 11）负载的影响因素有哪些？如何计算一台机器的负载值？主要是考虑CPU、内存、网络和I/O，但如何综合这些参数并计算还是得看具体的场景，每部分的权重随场景的不同而不同。 12）Master新建chunk时如何选择ChunkServer？如果新机器上线，负载值特别低，如何避免其他ChunkServer同时往这台机器迁移chunk？ 13）如果某台ChunkServer报废，GFS如何处理？RE-BALANCE。当有 Chunk服务器离线了，或者通过 Chksum 校验（参考5.2节）发现了已经损坏的数据，Master节点通过克隆已有的副本保证每个 Chunk 都被完整复制 。 14）如果ChunkServer下线后过一会重新上线，GFS如何处理？因为是过一会，所以假设chunk re-replication还没有执行，那么在这期间，可能这台chunkserver上有些chunk的数据已经处于落后状态了，client读数据的时候或者chunkserver定期扫描的时候会把这些状态告诉给master，master告诉上线后的chunkserver从其他机器复制该chunk，然后master会把这个chunk当作是垃圾清理掉。 对于没有落后的chunk副本，可以直接用于使用。 15）如何实现分布式文件系统的快照操作？ 16）ChunkServer数据结构如何设计？chunkserver主要是存储64KB block的checksum信息，需要由chunk+offset，能够快速定位到checksum，可以用hashmap。 17）磁盘可能出现“位翻转”错误，ChunkServer如何应对？利用checksum机制，分读和写两种情况来讨论： 对于读，要检查所读的所有block的checksum值。对于写，分为append和write。对于append，不检查checksum，延迟到读的时候检查，因为append的时候，对于最后一个不完整的block计算checksum时候采用的是增量的计算，即使前面存在错误，也能在后来的读发现。对于overwrite，因为不能采用增量计算，要覆盖checksum，所以，必须要先检查只写入部分数据的checksum是否不一致，否则，数据错误会被隐藏。 18）ChunkServer重启后可能有一些过期的chunk,Master如何能够发现？chunkserver重启后，会汇报chunk及其version number，master根据version number来判断是否过期。如果过期了，那么会做以下操作： 过期的chunk不参与数据读写流程 master会告诉chunkserver从其他的最新副本里拷贝一份数据 master将过期的chunk假如garbage collection中 问题：如果chunkserver拷贝数据的过程过程中，之前拷贝的数据备份又发生了变化，然后分为两种情况讨论： 如果期间lease没变，那么chunkserver不知道自己拷贝的数据是老的，应该会存在不一致的问题？ 如果期间lease改变，那么chunkserver因为还不能提供读服务，那么version number应该不会递增，继续保持stable状态，然后再发起拷贝。 继任者——Colossus Master将单一主控服务器改造为多主控服务器构成的集群，将所有管理数据进行数据分片后分配到不同的主控服务器中。这样水平扩展性得到极大增强。 Chunk Server通过应用纠删码算法，在减少备份数目的情况下达到类似的高可用性要求。 Client使得客户端可以管理备份数据的存储地点，对于提高I/O效率很有帮助。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统学习笔记（三）- Go&Concurrency]]></title>
    <url>%2FGo%26Concurrency%2F</url>
    <content type="text"><![CDATA[Goroutine和Channel擦出的火花，使Go得以轻松应对数以万计的并发逻辑。 并发简略介绍Golang 最大卖点就是原生支持并发。和传统基于 OS 线程和进程实现不同，Go 语言的并发是基于用户态的并发，这种并发方式就变得非常轻量，能够轻松运行几万甚至是几十万的并发逻辑。因此使用 Go 开发的服务端应用采用的就是“协程模型”，每一个请求由独立的协程处理完成。 比进程线程模型高出几个数量级的并发能力，而相对基于事件回调的服务端模型，Go 开发思路更加符合人的逻辑处理思维，因此即使使用 Go 开发大型的项目，也很容易维护。 详细特性 goroutine是Go语言运行库的功能，不是操作系统提供的功能，goroutine不是用线程实现的。具体可参见Go语言源码里的pkg/runtime/proc.c goroutine就是一段代码，一个函数入口，以及在堆上为其分配的一个堆栈。所以它非常廉价，我们可以很轻松的创建上万个goroutine，但它们并不是被操作系统所调度执行。 除了被系统调用阻塞的线程外，Go运行库最多会启动$GOMAXPROCS个线程来运行goroutine。 goroutine是协作式调度的，如果goroutine会执行很长时间，而且不是通过等待读取或写入channel的数据来同步的话，就需要主动调用Gosched()来让出CPU。 和所有其他并发框架里的协程一样，goroutine里所谓“无锁”的优点只在单线程下有效，如果$GOMAXPROCS &gt; 1并且协程间需要通信，Go运行库会负责加锁保护数据，这也是为什么sieve.go这样的例子在多CPU多线程时反而更慢的原因。 Web等服务端程序要处理的请求从本质上来讲是并行处理的问题，每个请求基本独立，互不依赖，几乎没有数据交互，这不是一个并发编程的模型，而并发编程框架只是解决了其语义表述的复杂性，并不是从根本上提高处理的效率，也许是并发连接和并发编程的英文都是concurrent吧，很容易产生“并发编程框架和coroutine可以高效处理大量并发连接”的误解。 Go语言运行库封装了异步IO，所以可以写出貌似并发数很多的服务端，可即使我们通过调整$GOMAXPROCS来充分利用多核CPU并行处理，其效率也不如我们利用IO事件驱动设计的、按照事务类型划分好合适比例的线程池。在响应时间上，协作式调度是硬伤。即便它更符合我们的我们的处理思维。 goroutine最大的价值是其实现了并发协程和实际并行执行的线程的映射以及动态扩展，随着其运行库的不断发展和完善，其性能一定会越来越好，尤其是在CPU核数越来越多的未来，终有一天我们会为了代码的简洁和可维护性而放弃那一点点性能的差别。 并发模型Go的Goroutine 和 Channel属于 CSP 并发模型的一种实现，CSP 并发模型的核心概念是：“不要通过共享内存来通信，而应该通过通信来共享内存”。在1978发表的 CSP 论文中有一段使用 CSP 思路解决问题的描述: “Problem: To print in ascending order all primes less than 10000. Use an array of processes, SIEVE, in which each process inputs a prime from its predecessor and prints it. The process then inputs an ascending stream of numbers from its predecessor and passes them on to its successor, suppressing any that are multiples of the original prime.” 要找出10000以内所有的素数，这里使用的方法是筛法，即从2开始每找到一个素数就标记所有能被该素数整除的所有数。直到没有可标记的数，剩下的就都是素数。下面以找出10以内所有素数为例，借用 CSP 方式解决这个问题。 从上图中可以看出，每一行过滤使用独立的并发处理程序，上下相邻的并发处理程序传递数据实现通信。通过4个并发处理程序得出10以内的素数表，对应的 Go 实现代码如下： 12345678910111213141516171819202122232425262728func main()&#123; origin, wait := make(chan int), make(chan struct&#123;&#125;) Processor(origin, wait) for num := 2; num &lt; 10000; num++ &#123; origin &lt;- num &#125; close(origin) &lt;- wait&#125;func Processor(seq chan int, wait chan struct&#123;&#125;)&#123; go func()&#123; prime, ok := &lt;-seq if !ok &#123; close(wait) return &#125; fmt.Println(prime) out := make(chan int) Processor(out, wait) for num := range seq&#123; if num%prime != 0&#123; out &lt;- num &#125; &#125; close(out) &#125;()&#125; 当然，如同之前我们讨论过的MapReduce一样，这些编程模型只代表了一种理念和思维方式，具体实现以后是语言的特性或项目的解决方案，使用时仍然应该按照具体场景来分析。 事实上Go从来也不排斥共享内存，当然也不鼓励滥用Channel。Go的wiki就有一页专门说这件事。 其中有句话让我很受启发，与君共勉：Go is pragmatic in letting you use the tools that solve your problem best and not forcing you into one style of code. Channel的应用场景：传递数据的所有权,分配工作单位,通信异步结果。 Mutex的应用场景：缓存,状态。 另一个重要的同步原语是sync.WaitGroup。它允许合作的goroutines共同等待一个阈值事件, 然后再独立地继续。这在两种情况下很有用：第一个，“清理”的场景。一个sync.WaitGroup 可以用来确保所有的goroutines(包括主协程)等待，直到所有协程终止干净。第二个更一般的情况是一个循环算法, 它涉及一组 goroutines, 它们都独立工作一段时间, 然后在一个屏障上等待, 然后再独立进行。此模式可能重复多次。数据在障碍事件可能被交换。该策略是批量同步并行 (BSP-Bulk Synchronous Parallelism) 的基础。 以上内容是我整理的Go语言机制及其背后并发模型的影子。 对于并发模型这个概念，我目前还是停留在很简单的语用层面。从代数的层面讲，其实Communicating Sequential Processes 是一门语言，有它的形式语法等。而Go只不过是借鉴了一些概念，和 Plan9 libthread, DirectShow … 一样。但用 Golang 写的程序是不能用 CSP 工具分析的, Golang 编译器也没这样的分析, 所以依然无法杜绝死锁/活锁 (Go 编译器有一个 -race 检测, 不过和 CSP 的分析是两回事)。 既然都提到这么多并发模型了，那不如再聊聊Actors模型。它主要使用消息机制来实现并发，目标是让开发者不再考虑进程/线程，每个Actor最多只能同时进行一样工作，Actor内部可以有自己的变量和数据。 Actors模型避免了由操作系统进行任务调度的问题，在操作系统进程之上，多个Actor可能运行在同一个进程(或线程)中。这就节省了大量的Context切换。 在Actors模型中，每个Actor都有一个专属的命名”邮箱”, 其他Actor可以随时选择一个Actor通过邮箱收发数据,对于“邮箱”的维护，通常是使用发布订阅的机制实现的，比如我们可以定义发布者是自己，订阅者可以是某个Socket接口，另外的消息总线或者直接是目标Actor。 目前akka库是比较流行的Actors编程模型实现，支持Scala和Java语言。 CSP和Actor的区别: CSP进程通常是同步的(即任务被推送进Channel就立即执行，如果任务执行的线程正忙，则发送者就暂时无法推送新任务)，Actor进程通常是异步的(消息传递给Actor后并不一定马上执行)。 CSP中的Channel通常是匿名的, 即任务放进Channel之后你并不需要知道是哪个Channel在执行任务，而Actor是有“身份”的，你可以明确的知道哪个Actor在执行任务。 在CSP中，我们只能通过Channel在任务间传递消息, 在Actor中我们可以直接从一个Actor往另一个Actor传输数据.CSP中消息的交互是同步的，Actor中支持异步的消息交互。 而Actor有而CSP没有的一个很棒的特性就是其容错机制。根据在 Erlang 中设计的 OTP 规范将 Actors 组织成一个监督层次结构, 我们可以在应用程序中构建一个失败域。从而将错误模型化，不同层次的错误该如何应对都可以建模。 并发控制当并发成为语言的原生特性之后，在实践过程中就会频繁地使用并发来处理逻辑问题，尤其是涉及到网络I/O的过程，例如 RPC 调用，数据库访问等。下述内容整理自今日头条技术团队的博文——《今日头条Go建千亿级微服务的实践》 下图是一个微服务处理请求的抽象描述： 如图所示，当 Request 到达 GW 之后，GW 需要整合下游5个服务的结果来响应本次的请求，假定对下游5个服务的调用不存在互相的数据依赖问题。那么这里会同时发起5个 RPC 请求，然后等待5个请求的返回结果。为避免长时间的等待，这里会引入等待超时的概念。超时事件发生后，为了避免资源泄漏，会发送事件给正在并发处理的请求。在实践过程中，今日头条技术团队得出两种抽象的模型。 Wait Cancel Wait和Cancel两种并发控制方式，在使用 Go 开发服务的时候到处都有体现，只要使用了并发就会用到这两种模式。在上面的例子中，GW 启动5个协程发起5个并行的 RPC 调用之后，主协程就会进入等待状态，需要等待这5次 RPC 调用的返回结果，这就是 Wait 模式。另一中 Cancel 模式，在5次 RPC 调用返回之前，已经到达本次请求处理的总超时时间，这时候就需要 Cancel 所有未完成的 RPC 请求，提前结束协程。Wait 模式使用会比较广泛一些，而对于 Cancel 模式主要体现在超时控制和资源回收。 在 Go 语言中，分别有 sync.WaitGroup 和 context.Context 来实现这两种模式。 超时控制合理的超时控制在构建可靠的大规模微服务架构显得非常重要，不合理的超时设置或者超时设置失效将会引起整个调用链上的服务雪崩。 图中被依赖的服务G由于某种原因导致响应比较慢，因此上游服务的请求都会阻塞在服务G的调用上。如果此时上游服务没有合理的超时控制，导致请求阻塞在服务G上无法释放，那么上游服务自身也会受到影响，进一步影响到整个调用链上各个服务。 在 Go 语言中，Server 的模型是“协程模型”，即一个协程处理一个请求。如果当前请求处理过程因为依赖服务响应慢阻塞，那么很容易会在短时间内堆积起大量的协程。每个协程都会因为处理逻辑的不同而占用不同大小的内存，当协程数据激增，服务进程很快就会消耗大量的内存。 协程暴涨和内存使用激增会加剧 Go 调度器和运行时 GC 的负担，进而再次影响服务的处理能力，这种恶性循环会导致整个服务不可用。今日头条团队在使用 Go 开发微服务的过程中，曾多次出现过类似的问题，并称之为协程暴涨。 有没有好的办法来解决这个问题呢？通常出现这种问题的原因是网络调用阻塞过长。即使在我们合理设置网络超时之后，偶尔还是会出现超时限制不住的情况，对 Go 语言中如何使用超时控制进行分析，首先我们来看下一次网络调用的过程。 第一步，建立 TCP 连接，通常会设置一个连接超时时间来保证建立连接的过程不会被无限阻塞。 第二步，把序列化后的 Request 数据写入到 Socket 中，为了确保写数据的过程不会一直阻塞，Go 语言提供了 SetWriteDeadline 的方法，控制数据写入 Socket 的超时时间。根据 Request 的数据量大小，可能需要多次写 Socket 的操作，并且为了提高效率会采用边序列化边写入的方式。因此在 Thrift 库的实现中每次写 Socket 之前都会重新 Reset 超时时间。 第三步，从 Socket 中读取返回的结果，和写入一样， Go 语言也提供了 SetReadDeadline 接口，由于读数据也存在读取多次的情况，因此同样会在每次读取数据之前 Reset 超时时间。 分析上面的过程可以发现影响一次 RPC 耗费的总时间的长短由三部分组成：连接超时，写超时，读超时。而且读和写超时可能存在多次，这就导致超时限制不住情况的发生。为了解决这个问题，今日头条团队在其内部框架——kite中引入了并发超时控制的概念，并将功能集成到 kite 框架的客户端调用库中。 并发超时控制模型如上图所示，在模型中引入了“Concurrent Ctrl”模块，这个模块属于微服务熔断功能的一部分，用于控制客户端能够发起的最大并发请求数。并发超时控制整体流程是这样的: 首先，客户端发起 RPC 请求，经过“Concurrent Ctrl”模块判断是否允许当前请求发起。如果被允许发起 RPC 请求，此时启动一个协程并执行 RPC 调用，同时初始化一个超时定时器。然后在主协程中同时监听 RPC 完成事件信号以及定时器信号。如果 RPC 完成事件先到达，则表示本次 RPC 成功，否则，当定时器事件发生，表明本次 RPC 调用超时。这种模型确保了无论何种情况下，一次 RPC 都不会超过预定义的时间，实现精准控制超时。 1234567891011121314151617181920import( "context")func Handler(r *Request)&#123; timeout := r.Value("timeout") ctx, cancel := context.WithTimeout(context.Background(), timeout) defer cancel() done := make(chan struct&#123;&#125;, 1) go func()&#123; RPC(ctx,...) done &lt;- struct&#123;&#125; &#125;() select&#123; case &lt;- done: // nice case &lt;- ctx.Done() // timeout &#125;&#125; Go 语言在1.7版本的标准库引入了“context”，这个库几乎成为了并发控制和超时控制的标准做法，随后1.8版本中在多个旧的标准库中增加对“context”的支持，其中包括“database/sql”包。]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统学习笔记（二）- RPC&Threads]]></title>
    <url>%2FRPC%26Threads%2F</url>
    <content type="text"><![CDATA[远程进程之间的通信的两把利器：线程和RPC。 线程为什么线程？ 允许您利用并发性，这是分布式系统中很自然的要求。 I/O并发性：在等待来自另一个服务器的响应时，处理下一个请求。 多核：线程在几个核心上并行运行。 线程==“执行线程”(thread of execution)线程允许一个程序一次（逻辑上讲）执行许多事情。线程共享内存。 每个线程都包含一些线程状态：程序计数器，寄存器，堆栈。 程序中应该有多少个线程？ 尽可能多的“有用”的应用程序。 Go鼓励你创造许多线程。通常比核心更多的线程。Go运行时会在可用CPU上安排它们。 Go线程是不是免费的，但你应该认为他们是。创建线程比方法调用更昂贵。 线程中的挑战： 共享数据 一个线程读取另一个线程正在改变的数据？ 可能导致竞赛状况。 -&gt;不要共享或协调共享（例如，互斥体） 线程之间的协调等待所有的Map线程完成可能会导致死锁（通常比竞争更易于注意）-&gt;使用Go channel或WaitGroup。 并发性的粒度 粗粒度 - &gt;简单，但很少并发/并行性。 细粒度 - &gt;更多的并发性，更多的竞争和死锁。 实例练习 Crawler 安排I/O并发在获取URL的同时，处理另一个URL 一次获取每个URL要避免浪费网络带宽要对远程服务器友好=&gt;需要某种方式来跟踪哪些网址被访问了。[在不同核心上处理不同的URL]少认同是重要的。 解决方案消除深度—使用fetched代替 顺序方案：将fetched map传递给递归调用当fetcher需要很长时间时不会重叠I/O不利用多个核心 1234567891011121314151617func CrawlSerial(url string, fetcher Fetcher, fetched map[string]bool) &#123; if fetched[url] &#123; return &#125; fetched[url] = true body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) for _, u := range urls &#123; CrawlSerial(u, fetcher, fetched) &#125; return&#125; 使用Go routines和 shared fetched map 为每个URL创建一个线程我们不通过你呢？（竞争） 为什么锁？（删除它们，似乎每一个还会工作！）没有锁可能会出什么问题？ URL的检查和标记不是原子的 所以可能会发生，我们获取相同的网址两次。T1检查提取[url]，T2检查提取[url]都看到该网址还没有被提取两者都返回假和两个取，这是错误的这叫做”竞赛条件” 该错误只显示一些线程交错 很难找到，很难推理Go可以检测到你的竞争（go run -race crawler.go）请注意，访问的检查和标记必须是原子的。 我们怎样才能决定我们处理一个页面？waitGroup 1234567891011121314151617181920212223242526272829303132333435363738394041424344type fetchState struct &#123; mu sync.Mutex fetched map[string]bool&#125;func (f *fetchState) CheckAndMark(url string) bool &#123; //defer f.mu.Unlock() //f.mu.Lock() if f.fetched[url] &#123; return true &#125; f.fetched[url] = true return false&#125;func mkFetchState() *fetchState &#123; f := &amp;fetchState&#123;&#125; f.fetched = make(map[string]bool) return f&#125;func CrawlConcurrentMutex(url string, fetcher Fetcher, f *fetchState) &#123; if f.CheckAndMark(url) &#123; return &#125; body, urls, err := fetcher.Fetch(url) if err != nil &#123; fmt.Println(err) return &#125; fmt.Printf("found: %s %q\n", url, body) var done sync.WaitGroup for _, u := range urls &#123; done.Add(1) go func(u string) &#123; defer done.Done() CrawlConcurrentMutex(u, fetcher, f) &#125;(u) // Without the u argument there is a race &#125; done.Wait() return&#125; 使用Channel Channels：general-purse机制来协调线程消息的有限缓冲区多个线程可以在channel上发送和接收 （Go运行时在内部使用锁） 发送或接收可能会被阻止当channel已满时当channel是空的 通过主线程分派每个URL获取没有竞争获取map，因为它不共享！ 123456789101112131415161718192021222324252627282930313233343536func dofetch(url1 string, ch chan []string, fetcher Fetcher) &#123; body, urls, err := fetcher.Fetch(url1) if err != nil &#123; fmt.Println(err) ch &lt;- []string&#123;&#125; &#125; else &#123; fmt.Printf("found: %s %q\n", url1, body) ch &lt;- urls &#125;&#125;func master(ch chan []string, fetcher Fetcher) &#123; n := 1 fetched := make(map[string]bool) for urls := range ch &#123; for _, u := range urls &#123; if _, ok := fetched[u]; ok == false &#123; fetched[u] = true n += 1 go dofetch(u, ch, fetcher) &#125; &#125; n -= 1 if n == 0 &#123; break &#125; &#125;&#125;func CrawlConcurrentChannel(url string, fetcher Fetcher) &#123; ch := make(chan []string) go func() &#123; ch &lt;- []string&#123;url&#125; &#125;() master(ch, fetcher)&#125; 什么是最好的解决方案？ 所有并发的比串行的更难一些Go设计师认为避免共享内存即只使用channel 我们的解决方案是使用许多并发功能 加锁: 当共享是自然的时候例如，几个共享map的服务器线程 channels: 当线程间需要协调的时候例如，生产者/消费者风格的并发 使用Go的竞争探测器：https://golang.org/doc/articles/race_detector.htmlgo test -race mypkg 远程过程调用（RPC） 分布式系统的关键部分! RPC的概念代表着分布式计算的重大突破，同时也使分布式编程和传统编程相似，实现了分布透明性。该概念由Birrell和Nelson在1984年首次提出，为许多分布式系统的编程铺平了道路，一直到现在。 目标：易于编程的网络通信 隐藏客户端/服务器通信的大部分细节(对参数和结果的解码和编码、消息传递以及保留调用要求的语义) 客户调用很像普通的程序调用 服务器处理程序很像普通程序RPC被广泛使用！ RPC理想地使网络通信看起来就像fn调用：1234567Client: z = fn(x, y)Server: fn(x, y) &#123; compute return z &#125; RPC旨在达到这种透明度。 实例研究：kv.gokv.go 客户端“拨号”服务器并调用Call（） 调用类似于常规函数调用 服务器在单独的线程中处理每个请求 并发！因此，锁住了keyvalue字段。 RPC消息图：123Client Server request---&gt; &lt;---response 软件结构1234client app handlers stubs dispatcher RPC lib RPC lib net ------------ net 一些细节： 哪个服务器函数（处理程序）要调用？在Go中特指Call() 编组(Marshalling)：将数据格式化成数据包棘手的数组，指针，对象，＆CGo的RPC库非常强大！有些事情你不能通过：例如，channel，函数 绑定(Binding)：客户如何知道与谁交谈？也许客户端提供服务器主机名也许名称服务将服务名称映射到最佳服务器主机 RPC问题：如何处理失败？ 比如：丢包，网络断线，服务器运行缓慢，服务器崩溃。 错误对RPC客户端意味着什么？ 客户端从不会看到来自服务器的响应。 客户端不知道服务器是否看到了请求！也许服务器/网络在发送回复之前失败。 最简单的方案：“至少一次”(at least once)语义 RPC库等待响应一段时间，如果没有到达，重新发送请求。 这样做几次后，若仍然没有回应则将错误返回给应用程序。 采用“至少一次”调用语义，调用者可能收到返回的结果，也可能收到一个异常。在收到返回结果的情况下，调用者知道该方法至少执行过一次，而异常信息则通知它没有接收到执行结果。“至少一次”调用语义可以通过重发请求消息来达到，它屏蔽了调用或结果消息的遗漏故障。但可能会遇到下列类型的故障： 由于包含远程对象服务器故障而引起的系统崩溃 随机故障。重发调用消息时，远程对象可能会接收到这一消息并多次执行某一方法，结果导致存储或返回了错误的值。 问：应用程序能够轻松处理“至少一次”吗？ 简单的问题”写”至少一次： 客户发送“从银行账户扣除 $10” 问：这个客户端程序有什么问题？ Put（“k”，10） - 一个RPC调用在数据库服务器中设置键值对。 Put（“k”，20） - 客户端对同一个键设置其他值。 问：至少有一次可以吗？ 是的：如果可以重复操作，例如只读操作。 是的：如果应用程序有自己的应对重复计划(MIT 6.824 Lab1 中就用到该方案)。换句话说，如果能设计服务器中的操作使其服务接口中的所有方法都是幂等操作的话，那么“至少一次”调用语义是可以接受的。 更好的RPC行为：“最多一次”(at most once)想法：服务器RPC代码检测到重复的请求，返回以前的答复，而不是重新运行处理程序。 问：如何检测重复的请求？客户端包含每个请求的唯一ID（XID）;使用相同的XID重新发送。1234567server: if seen[xid]: r = old[xid] else r = handler() old[xid] = r seen[xid] = true 一些”最多一次”的复杂性 如何确保XID是唯一的？很大的随机数？将唯一的客户端ID（IP地址？）和序列号组合起来？ 服务器必须最终放弃有关旧RPC的信息什么时候放弃安全？理念： 唯一的客户端ID 每客户端RPC序列号 客户端包括“seen all replies &lt;= X” 很像TCP序列号和ack 或者一次只允许客户端一个未完成的RPC，到达的是seq+1，那么忽略其他小于seq的请求。 或者客户端最多可以尝试5次，服务器会忽略大于5次的请求。 当原来的请求还在执行，怎么样处理相同seq的请求？服务器还不知道答复; 不想跑两次 想法：给每个执行的RPC“挂起”标志; 等待或忽略 如果“最多一次”服务器崩溃并重新启动，该怎么办？ 如果在内存中至多有一次重复的信息，服务器将会忘记，并在重新启动后接受重复的请求。 也许它应该将重复信息写入磁盘？ 也许副本服务器也应该复制重复信息？ 那“仅一次”呢？ 最多一次 + 无限重试 + 容错服务。 Go RPC是“最多一次” 打开TCP连接; 写请求到TCP连接。 TCP可能会重新传输，但服务器的TCP将会过滤掉重复的内容。 Go代码没有重试（即不会创建第二个TCP连接）。 Go RPC代码返回一个错误，如果它没有得到答复： 也许是TCP连接的超时 也许服务器没有看到请求 也许服务器处理请求，但服务器/网络在回复之前失败 对于实验1来说，Go RPC的”最多一次”是不够的 它仅只适用于一个RPC调用。如果worker没有回应，master重新发送给另一名worker，但原worker可能没有失败，并且也在努力。 Go RPC不能检测到这种重复： 实验1中没有问题，因为在应用程序级别处理了。实验3将明确检测重复。 带有故障模拟的RPC实现分析labrpc——MIT 6.824中一个简单的RPC库，它很像Go的RPC包，但是带有模拟网络。它用channel模拟了network、endpoint、server、dispatch等内容，同时提供注入故障功能（网络重传、延时、丢包等）。这个模拟的网络会延迟请求和回复、会丢失请求和回复、会重新排序请求和回复。在揭示RPC原理的同时，对于学习分布式系统如何测试也有一定的借鉴意义。 注意：实验一 MapReduce实现使用的是Go的RPC包。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778type JunkArgs struct &#123; X int&#125;type JunkReply struct &#123; X string&#125;type JunkServer struct &#123; mu sync.Mutex log1 []string log2 []int&#125;func (js *JunkServer) Handler1(args string, reply *int) &#123; js.mu.Lock() defer js.mu.Unlock() js.log1 = append(js.log1, args) *reply, _ = strconv.Atoi(args)&#125;func (js *JunkServer) Handler2(args int, reply *string) &#123; js.mu.Lock() defer js.mu.Unlock() js.log2 = append(js.log2, args) *reply = "handler2-" + strconv.Itoa(args)&#125;func (js *JunkServer) Handler3(args int, reply *int) &#123; js.mu.Lock() defer js.mu.Unlock() time.Sleep(20 * time.Second) *reply = -args&#125;// args is a pointerfunc (js *JunkServer) Handler4(args *JunkArgs, reply *JunkReply) &#123; reply.X = "pointer"&#125;// args is a not pointerfunc (js *JunkServer) Handler5(args JunkArgs, reply *JunkReply) &#123; reply.X = "no pointer"&#125;func TestBasic(t *testing.T) &#123; runtime.GOMAXPROCS(4) rn := MakeNetwork() e := rn.MakeEnd("end1-99") js := &amp;JunkServer&#123;&#125; svc := MakeService(js) rs := MakeServer() rs.AddService(svc) rn.AddServer("server99", rs) rn.Connect("end1-99", "server99") rn.Enable("end1-99", true) &#123; reply := "" e.Call("JunkServer.Handler2", 111, &amp;reply) if reply != "handler2-111" &#123; t.Fatalf("wrong reply from Handler2") &#125; &#125; &#123; reply := 0 e.Call("JunkServer.Handler1", "9099", &amp;reply) if reply != 9099 &#123; t.Fatalf("wrong reply from Handler1") &#125; &#125;&#125; 由该测试框架可以看出，一次RPC需要用到如下的API: MakeNetwork: 创建网络，由Client和Server组成。 MakeServer: 创建Server，Server里面有不同的Service。 MakeService: 创建Service，Service里面定义了不同的handle。 MakeEnd: 创建Client。 Connect: Client调用Server前先要执行Connect。 Call: Client调用Server端的过程，通过参数执行要调用的handle，handle需要的参数。 同时，整个RPC过程中涉及到以下基本结构: 服务器（Server）， 支持含有多个服务srv := MakeServer() srv.AddService(svc) 客户端（ClientEnd），调用Call()函数，发生一个RPC请求并等待结果 reply := end.Call(“Raft.AppendEntries”, args, &amp;reply) 网络（Network），每个结构都持有一个sync.Mutex 再剧透一下其中的关键函数: Server.AddService() 由于它可能在多个goroutine中调用，所以上锁。而deter的作用是在函数退出前，调用之后的代码，就是添加完新服务后，做解锁操作。 Server.dispatch() 分发请求。为什么持有锁？因为跟AddService并发访问可能冲突。 ClientEnd.Call() 使用反射查找参数类型，使用“gob”序列化参数（Go的自带编解码包gob.NewEncoder）; ClientEnd.ch（chan reqMsg）是用于发送请求的通道e.ch &lt;- req; 需要一个通道从接收回复(&lt;- req.replyCh)。 Network.MakeEnd，创建客户端，使用一个线程或者goroutine模拟网络，每个请求分别在不同的goroutine处理。一个端点是否可以拥有多个未处理的请求？为什么使用rn.mu.Lock()？锁保护了什么？保护Network中的活跃数组信息。 Network.ProcessReq，如果网络不可靠，可能会延迟或者丢失请求，在一个新的线程中分发请求。通过读取e.ch等待回复直到时间过去100毫秒。100毫秒只是来看看服务器是否崩溃。最后返回回复（req.replyCh &lt;- reply）。ProcessReq没有持有rn锁，是否安全？ Service.dispatch，为请求找到合适的处理方法。 RPC基本结构Network1234567891011type Network struct &#123; mu sync.Mutex reliable bool longDelays bool // pause a long time on send on disabled connection longReordering bool // sometimes delay replies a long time ends map[interface&#123;&#125;]*ClientEnd // ends, by name enabled map[interface&#123;&#125;]bool // by end name servers map[interface&#123;&#125;]*Server // servers, by name connections map[interface&#123;&#125;]interface&#123;&#125; // endname -&gt; servername endCh chan reqMsg&#125; Network中各字段含义如下: servers：该网络中的所有Server ends：该网络中的所有Client connections：Client到Server的所有链接 endCh：golang的channel，用来模拟传送数据的网络 enabled：模拟Server是否宕机 reliable：用来模拟网络是否可靠 longDelays：用来模拟慢Server longReording：用来模拟网络的乱序 Server12345type Server struct &#123; mu sync.Mutex services map[string]*Service count int // incoming RPCs&#125; Server各字段含义如下:service: Server所包含的Servicecount: 到达Server的RPC数 Client1234type ClientEnd struct &#123; endname interface&#123;&#125; // this end-point's name ch chan reqMsg // copy of Network.endCh &#125; Client各字段含义如下: endname：客户端名称 reqMsg：发送消息的模拟网络，和Network的endCh是同一个channel RPC API实现MakeNetwork12345678910111213141516171819// 本模块主要是初始化Network数据结构，然后启动一个goroutine来处理Client的Call调用请求。func MakeNetwork() *Network&#123; rn := &amp;Network&#123;&#125; rn.reliable = true rn.ends = map[interface&#123;&#125;]*ClientEnd&#123;&#125; rn.enabled = map[interface&#123;&#125;]bool&#123;&#125; rn.servers = map[interface&#123;&#125;]*Server&#123;&#125; rn.connections = map[interface&#123;&#125;](interface&#123;&#125;)&#123;&#125; rn.endCh = make(chan reqMsg) //single goroutine to handle all ClientEnd.Call()s go func()&#123; for xreq := range rn.endCh&#123; go rn.ProcessReq(xreq) &#125; &#125;() return rn&#125; MakeEnd123456789101112131415161718// 本模块主要是在Network中添加Client,并把其enabled和connections设置成空。func (rn *Network) MakeEnd(endname interface&#123;&#125;) *ClientEnd&#123; rn.mu.Lock() defer rn.mu.Unlock() if _, ok := rn.ends[endname]; ok&#123; log.Fatalf("MakeEnd: %v already exsists\n", endname) &#125; e := &amp;ClientEnd&#123;&#125; e. endname := endname e.ch = rn.endCh rn.ends[endname] = e rn.enabled[endname] = false rn.connections[endname] = nil return e&#125; MakeServer123456//本模块初始化Server结构体的service字段为空的hashmap。func MakeServer() *Server&#123; rs := &amp;Server&#123;&#125; rs.services = map[string]*Service&#123;&#125; return rs&#125; AddServer1234567// 在Network的servers中添加server。func (rn *Network) AddServer(servername interface&#123;&#125;, rs *Server)&#123; rn.mu.Lock() defer rn.mu.Unlock() rn.servers[servername] = rs&#125; MakeService12345678910111213141516171819202122232425262728293031func MakeService(rcvr interface&#123;&#125;) *Service&#123; svc := &amp;Service&#123;&#125; svc.typ = reflect.TypeOf(rcvr) svc.rcvr = reflect.ValueOf(rcvr) svc.name = reflect.Indirect(svc.rcvr).Type().Name() svc.methods = map[string]relect.Method&#123;&#125; for m := 0; m &lt; svc.typ.NumMethod(); m++ &#123; method := svc.typ.Method(m) mtype := method.Type mname := method.Name //fmt.Printf("%v pp %v ni %v 1k %v 2k %v no %v\n", // mname, method.PkgPath, mtype.NumIn(), mtype.In(1).Kind(), mtype.In(2).Kind(), mtype.NumOut()) if method.PkgPath != "" || // capitalized? mtype.NumIn() != 3 || //mtype.In(1).Kind() != reflect.Ptr || mtype.In(2).Kind() != reflect.Ptr || mtype.NumOut() != 0 &#123; // the method is not suitable for a handler //fmt.Printf("bad method: %v\n", mname) &#125; else &#123; // the method looks like a handler svc.methods[mname] = method &#125; &#125; return svc&#125; rcvr是一个golang的接口，其上定义了一系列的方法，每个方法对应RPC的一个调用函数。整个处理方式流程如下： 创建Service结构体 通过golang的reflection方式，获取结构体的所有方法，通过reflect.TypeOf(rcvr).NumMethod()来获取 检测结构体中所有的method的参数是否符合RPC的标准。 把符合的方法添加到Service中，作为handle Connect1234567// 在Network结构体中的connections设置endname的连接为servernamefunc (rn *Network) Connect(endname interface&#123;&#125;, servername interface&#123;&#125;)&#123; rn.mu.Lock() defer rn.mu.Unlock() rn.connections[endname] = servername&#125; Enable1234567// 设置此Client对应的Server是否宕机func (rn *Network) Enable(endname interface&#123;&#125;, enabled bool)&#123; rn.mu.Lock() defer rn.mu.Unlock() rn.enabled[endname] = enabled&#125; Call123456789101112131415161718192021222324252627282930313233343536// send an RPC, wait for the reply.// the return value indicates success; false means the// server couldn't be contacted.func (e *ClientEnd) Call(svcMeth string, args interface&#123;&#125;, reply interface&#123;&#125;) bool &#123; req := reqMsg&#123;&#125; req.endname = e.endname req.svcMeth = svcMeth req.argsType = reflect.TypeOf(args) req.replyCh = make(chan replyMsg) //把Client要发送的数据进行encode，即序列化 qb := new(bytes.Buffer) qe := gob.NewEncoder(qb) qe.Encode(args) req.args = qb.Bytes() //发送请求的数据到channel上，即模拟的网络上 e.ch &lt;- req //Client等待Server端返回数据 rep := &lt;-req.replyCh // Server端响应！ //Client收到数据后，按照以下流程处理。反序列化Server端的响应，最终返回结果给应用端。 if rep.ok &#123; rb := bytes.NewBuffer(rep.reply) rd := gob.NewDecoder(rb) if err := rd.Decode(reply); err != nil &#123; log.Fatalf("ClientEnd.Call(): decode reply: %v\n", err) &#125; return true &#125; else &#123; return false &#125;&#125; 在Client端发送数据后, Server端的处理流程如下:12345go func()&#123; for xreg := range rn.endCh&#123; go rn.ProcessReq(xreq) &#125;&#125; Server端检测到endCh中的数据，然后调用ProcessReq处理请求。123456789101112if enabled &amp;&amp; servername != nil &amp;&amp; server != nil &#123; if reliable == false &#123; // short delay ms := (rand.Int() % 27) time.Sleep(time.Duration(ms) * time.Millisecond) &#125; if reliable == false &amp;&amp; (rand.Int()%1000) &lt; 100 &#123; // drop the request, return as if timeout req.replyCh &lt;- replyMsg&#123;false, nil&#125; return &#125; 如果要模拟网络不是可靠的请求下，会按照如下流程处理: 随机等待一小段时间 等待完后，以一定地概率不处理结果，直接返回Client失败 接着把请求分发到相应的handle处理12345ech := make(chan replyMsg)go func()&#123; r := server.dispatch(req) ech &lt;- r&#125;() 其中server.dispatch实现如下:1234567891011121314151617181920212223242526func (rs *Server) dispatch(req reqMsg) replyMsg &#123; rs.mu.Lock() rs.count += 1 // split Raft.AppendEntries into service and method dot := strings.LastIndex(req.svcMeth, ".") serviceName := req.svcMeth[:dot] methodName := req.svcMeth[dot+1:] service, ok := rs.services[serviceName] rs.mu.Unlock() if ok &#123; return service.dispatch(methodName, req) &#125; else &#123; choices := []string&#123;&#125; for k, _ := range rs.services &#123; choices = append(choices, k) &#125; log.Fatalf("labrpc.Server.dispatch(): unknown service %v in %v.%v; expecting one of %v\n", serviceName, serviceName, methodName, choices) return replyMsg&#123;false, nil&#125; &#125;&#125; 具体到service.dispatch，定位到具体需要处理的函数并调用它，流程如下:123456789101112131415161718// decode the argument.ab := bytes.NewBuffer(req.args)ad := gob.NewDecoder(ab)ad.Decode(args.Interface()) // allocate space for the reply.replyType := method.Type.In(2)replyType = replyType.Elem()replyv := reflect.New(replyType) // call the method.function := method.Funcfunction.Call([]reflect.Value&#123;svc.rcvr, args.Elem(), replyv&#125;) // encode the reply.rb := new(bytes.Buffer)re := gob.NewEncoder(rb)re.EncodeValue(replyv) 之后server对RPC请求进行反序列化，调用对应的函数处理，最后把生成的结果序列化:12345678910111213141516171819202122232425262728293031 serverDead = rn.IsServerDead(req.endname, servername, server) if replyOK == false || serverDead == true &#123; // server was killed while we were waiting; return error. req.replyCh &lt;- replyMsg&#123;false, nil&#125; &#125; else if reliable == false &amp;&amp; (rand.Int()%1000) &lt; 100 &#123; // drop the reply, return as if timeout req.replyCh &lt;- replyMsg&#123;false, nil&#125; &#125; else if longreordering == true &amp;&amp; rand.Intn(900) &lt; 600 &#123; // delay the response for a while ms := 200 + rand.Intn(1+rand.Intn(2000)) time.Sleep(time.Duration(ms) * time.Millisecond) req.replyCh &lt;- reply &#125; else &#123; req.replyCh &lt;- reply &#125; &#125; else &#123; // simulate no reply and eventual timeout. ms := 0 if rn.longDelays &#123; // let Raft tests check that leader doesn't send // RPCs synchronously. ms = (rand.Int() % 7000) &#125; else &#123; // many kv tests require the client to try each // server in fairly rapid succession. ms = (rand.Int() % 100) &#125; time.Sleep(time.Duration(ms) * time.Millisecond) req.replyCh &lt;- replyMsg&#123;false, nil&#125;&#125; 根据一系列的配置，决定是否返回结果以及何时返回结果，用来模拟故障情况: 如果enabled为false，则模拟Server挂掉的情况，则直接返回失败。 如果reliable为false，则模拟网络不可靠情况，有概率返回失败 如果longreording为true，则以一定概率等待一定时间返回结果，以模拟网络包乱序地情况 如果是longDelays为true，则会等待一段事件再返回结果，模拟高时延的情况]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发&并行]]></title>
    <url>%2FCurrency%26Parallel%2F</url>
    <content type="text"><![CDATA[纪念当初傻傻分不清楚的并发并行。 并发&amp;并行2018/1/8 0:33 阅读 goroutine背后的系统知识与Concurrency is not parallelism有感。对并发、并行这一老生常谈的问题进行二次挖掘，但仍然停留在扩展概念和完善解释的层面。对Concurrency后面的big picture(如Actor模型等)仍认识不足。需待以后补全。 一组定义 并发(Concurrency)：一个同时具有多个活动的系统； 两个或多个事件在同一时间间隔内发生。(操作系统教材) Concurrency is the composition of independently executing processes. Concurrency is about dealing with lots of things at once. Concurrency is about structure. Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable. 一个逻辑流（一系列PC的值，唯一地对应于包含在程序的可执行目标文件中的指令/包含在运行时动态链接到程序的共享对象中的指令）的执行在时间上与另一个流重叠，这两个流称为被称为并发地运行。多个流并发地执行的一般现象成为并发。 并行(Parallelism)：用并发使一个系统运行得更快。 两个或多个事件在同一时刻发生。(操作系统教材) Parallelism is the simultaneous execution of (possibly related) computations. Parallelism is about doing lots of things at once. Parallelism is about execution. 并发是指程序的逻辑结构。非并发的程序就是一根竹竿捅到底，只有一个逻辑控制流，也就是顺序执行的(Sequential)程序，在任何时刻，程序只会处在这个逻辑控制流的某个位置。而如果某个程序有多个独立的逻辑控制流，也就是可以同时处理(deal)多件事情，我们就说这个程序是并发的。这里的“同时”，并不一定要是真正在时钟的某一时刻(那是运行状态而不是逻辑结构)，而是指：如果把这些逻辑控制流画成时序流程图，它们在时间线上是可以重叠的。 并行是指程序的运行状态。如果一个程序在某一时刻被多个CPU流水线同时进行处理，那么我们就说这个程序是以并行的形式在运行。（严格意义上讲，我们不能说某程序是“并行”的，因为“并行”不是描述程序本身，而是描述程序的运行状态。以下说到“并行”的时候，就是指代“以并行的形式运行”）显然，并行一定是需要硬件支持的。 一组推论基于上述概念，我们不难得到如下的推论： 并发是并行的必要条件，如果一个程序本身就不是并发的，也就是只有一个逻辑控制流，那么我们不可能让其被并行处理。 并发不是并行的充分条件，一个并发的程序，如果只被一个CPU流水线进行处理(通过分时)，那么它就不是并行的。 并发只是更符合现实问题本质的表达方式，并发的最初目的是简化代码逻辑，而不是使程序运行的更快； 这几段略微抽象，我们可以用一个最简单的例子来把这些概念实例化：用C语言写一个最简单的HelloWorld，它就是非并发的，如果我们建立多个线程，每个线程里打印一个HelloWorld，那么这个程序就是并发的，如果这个程序运行在老式的单核CPU上，那么这个并发程序还不是并行的，如果我们用多核多CPU且支持多任务的操作系统来运行它，那么这个并发程序就是并行的。 还有一个略微复杂的例子，更能说明并发不一定可以并行，而且并发不是为了效率，就是Go语言例子里计算素数的sieve.go。我们从小到大针对每一个因子启动一个代码片段，如果当前验证的数能被当前因子除尽，则该数不是素数，如果不能，则把该数发送给下一个因子的代码片段，直到最后一个因子也无法除尽，则该数为素数，我们再启动一个它的代码片段，用于验证更大的数字。这是符合我们计算素数的逻辑的，而且每个因子的代码处理片段都是相同的，所以程序非常的简洁，但它无法被并行，因为每个片段都依赖于前一个片段的处理结果和输出。 并发的构造方式 显式地定义并触发多个代码片段，也就是逻辑控制流，由应用程序或操作系统对它们进行调度。它们可以是独立无关的，也可以是相互依赖需要交互的。线程只是实现并发的其中一个手段，除此之外，运行库或是应用程序本身也有多种手段来实现并发。 隐式地放置多个代码片段，在系统事件发生时触发执行相应的代码片段，也就是事件驱动的方式，譬如某个端口或管道接收到了数据(多路IO的情况下)，再譬如进程接收到了某个信号(signal)。 构建并发程序的机制：进程、I/O多路复用、线程。 进程： 进程由内核自动调度，因为他们有各自独立的虚拟地址空间，所以要实现数据共享必须要有显式的的IPC机制。构造“进程”这一抽象的概念，能设计出同时执行多个程序的系统。 I/O多路复用： 事件驱动程序创建他们自己的并发逻辑流，这些逻辑流被模型化为状态机，用I/O多路复用来显示地调度这些流。因为程序运行在单一线程中，所以共享数据很快而且很容易。 线程： 线程是上述方法的综合。同基于进程的流一样，线程也是由内核自动调度的。同基于I/O多路复用的流一样，线程是运行在一个单一进程的上下文中的，因此可以更快地访问数据。构造“线程”这一概念，能够在一个进程中执行多个控制流。 从进程到线程到协程 碰着I/O访问，阻塞了后面所有的计算。空着也是空着，就把CPU切换到其他进程。 进程数高的时候，进程切换会耗费大量系统资源。于是线程的概念，大致意思就是，这个地方阻塞了，但我还有其他地方的逻辑流可以计算，这些逻辑流是共享一个地址空间的，不用特别麻烦的切换页表、刷新TLB，只要把寄存器刷新一遍就行，能比切换进程开销少点。 如果连时钟阻塞、 线程切换这些功能我们都不需要了，自己在进程里面写一个逻辑流调度的东西。那么我们即可以利用到并发优势，又可以避免反复系统调用，还有进程切换造成的开销，分分钟给你上几千个逻辑流不费力。这就是用户态线程。 从上面可以看到，实现一个用户态线程有两个必须要处理的问题：一是碰着阻塞式I\O会导致整个进程被挂起；二是由于缺乏时钟阻塞，进程需要自己拥有调度线程的能力。如果一种实现使得每个线程需要自己通过调用某个方法，主动交出控制权。那么我们就称这种用户态线程是协作式的，即是协程。本质上，协程就是用户空间下的线程。 并行的四个层面 多台机器。自然我们就有了多个CPU流水线，譬如Hadoop集群里的MapReduce任务。 多CPU。不管是真的多颗CPU还是多核还是超线程，总之我们有了多个CPU流水线。 多CPU：将多个CPU(称为”核”)集成到一个集成电路芯片上。以Intel Core i7为例，其中微处理器芯片有4个CPU核，每个核都有自己的L1和L2高速缓存，以及到主存的接口。 超线程：有时称为同时多线程(simultaneout multi-threading),是个控制流的技术。它涉及CPU某些硬件有多个备份，比如程序计数器和寄存器文件；而其他的硬件部分只有一份，比如执行浮点算数运算的单元。常规的处理器需要大约20000个时钟周期做不同线程间的转换，而超线程的处理器可以在单个周期的基础上决定要执行哪一个线程。这使得CPU能够更好地利用它的处理资源。例如，假设一个线程必须等到某些数据被装载到高速缓存中，那CPU就可以继续去执行另一个线程。举例来说，Intel Core i7处理器可以让一个核执行两个线程，所以一个4核的系统实际上可以并行地执行8个线程。 单CPU核里的ILP(Instruction-level parallelism)，指令级并行。 通过复杂的制造工艺和对指令的解析以及分支预测和乱序执行，现在的CPU可以在单个时钟周期内执行多条指令，从而，即使是非并发的程序，也可能是以并行的形式执行。早起的微处理器，如1978年的Intel 8086，需要多个(通常是3-10个)时钟周期来执行一条指令。比较先进的处理器可以保持每个时钟周期2-4条指令的执行效率。其实每条指令从开始到结束需要长得多的时间，大约要20个时间周期，为了执行多达百条的指令，通过流水线将每条指令分解成几个步骤，这些步骤可以同时进行，这样就能并行地处理不同指令的不同部分。如果处理器可以达到比一个周期一条语句更快的执行速率，就称之为超标量(superscalar)处理器。 单指令多数据(Single instruction, multiple data. SIMD)。 为了多媒体数据的处理，现在的CPU的指令集支持单条指令对多条数据进行操作。 其中，1牵涉到分布式处理，包括数据的分布和任务的同步等等，而且是基于网络的。3和4通常是编译器和CPU的开发人员需要考虑的。这里我们说的并行主要针对第2种：单台机器内的多核CPU并行。 在CMU那本著名的《Computer Systems: A Programmer’s Perspective》里的这张图也非常直观清晰：]]></content>
      <tags>
        <tag>并发模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统学习笔记（一）- MapReduce]]></title>
    <url>%2Fmapreduce%2F</url>
    <content type="text"><![CDATA[没有完美的系统，任何系统的出现都是为了解决当时最主要的问题。 走向分布式 分布式： 一个业务分拆为多个子业务，部署在不同的服务器上。各个模块间通过远程服务调用(RPC)进行通信，整个计算机集合共同对外提供服务，但对于用户来说，就像是一台计算机在提供服务一样。 一个系统走向分布式，追求的无非是： 扩展性(Scalability) 高可用(High Availability) 高性能(High Performence) 获取相应特性的同时，便注定要接受一些牺牲: 牺牲效率 牺牲AP弹性 牺牲运维的便捷性 为了对应用掩盖“分布”，产生了三大抽象: 存储(Storage) 通信(Communication) 计算(Computation) 云计算不过是分布式这个旧瓶子装的新酒：分布式技术 + 服务化技术 + 虚拟化技术(资源隔离和管理) 服务模式(Service models)美国国家标准和技术研究院的云计算定义中明确了三种服务模式： 软件即服务（SaaS - Software as a service）平台即服务（PaaS - Platform as a service）基础设施即服务（IaaS - Infrastructure as a service） 而另外两位新成员分别是：移动后端即服务(MBaaS - Mobile “backend” as a service)和Serverless computing。 虚拟化包括资源虚拟化、统一分配检测资源、向资源池中添加资源。 实现远程服务调用(RPC), 线程(threads), 并发控制(concurrency control) 性能 愿景: 彻底的可扩展性N台服务器就有N倍的吞吐量。应对更多的负载只需要堆更多的机器就好了。 事实上:随着N的增长，扩展的难度越大。 负载不均衡(Load im-balance, stragglers) 不可并行化的代码(Non-parallelizable code) 共享资源(比如网络)带来的性能瓶颈容错数以千计的机器、复杂的网络意味着总会有错误产生，而我们希望这些错误对应用是透明的。 我们想要的是: 可用性(Availability)尽管机器出了毛病，应用不至于崩溃。 耐久性(Durability)当错误修复的时候，应用又能活蹦乱跳。 一个点子诞生了: 复制服务器如果一个服务器崩溃了，用户可以转而使用其他的。 一致性 通常，基础架构需要“良好定义的行为”(well-defined behavior)。比如”Get(k)获取到的值，产生自最近的Put(k,v).” 但实现良好行为相当困难！ 备份服务器(Replica servers)之间很难保证一致。 客户端可能在多步骤上传的中途崩溃。 服务器可能在执行后、响应前崩溃。 竞争网络资源可能造成服务器的死锁。 多主服务器的风险(split brain)。 一致性和高性能不可兼得。保证一致性需要通信，强一致性会影响系统性能。高可用常常意味着应用的“弱一致性”。 实例学习: MapReduceMapReduce 概述现实需求 处理大量的原始数据比如文档抓取、Web 请求日志； 处理各种类型的衍生数据比如倒排索引、Web 文档的图结构的各种表示形式、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合。潜在问题 如何处理并行计算、如何分发数据、如何处理错误？ 有问题，试着用抽象的方法解决如何？一种新的计算模型——MapReduce应运而生。MapReduce抽象出来大多数的运算都包含的操作：在输入数据的记录上应用 Map 操作得出一个中间 key/value pair 集合。然后在所有具有相同 key 值的 value 值上应用 Reduce 操作，从而达到合并中间的数据，得到结果。值得一提的是，它的灵感来自 Lisp 和许多其他函数式语言的 Map 和 Reduce 的原语。 MapReduce 原理简图1234567input is divided into M files Input1 -&gt; Map -&gt; a,1 b,1 c,1 Input2 -&gt; Map -&gt; b,1 Input3 -&gt; Map -&gt; a,1 c,1 | | | | -&gt; Reduce -&gt; c,2 -----&gt; Reduce -&gt; b,2 用户自定义的 Map 函数接受一个输入的key/value pair值，然后产生一个中间key/value pair 值的集合。MapReduce 库把所有具有相同中间数据的值集合在一起后传递给 reduce 函数。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般地，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通过一个迭代器把中间 value 值提供给 Reduce 函数，以此处理无法全部放入内存中的大量的 value 值的集合。 例子: Word Count1234567input is thousands of text filesMap(k, v) split v into words for each word w emit(w, "1")Reduce(k, v) emit(len(v)) MapReduce 特性MapReduce 隐藏了许多令人痛苦的细节 启动服务器上的软件 追踪任务进展 数据移动 错误恢复 MapReduce 可扩展性良好 N台服务器可以有N倍的吞吐量。因为Map()操作之间并不需要交互，所以Map()可以并行处理。同理Reduce()操作也可以。 新模型解放了程序员，他们不再需要为每个应用进行专门的并行优化了。毕竟程序员的时间比机器更宝贵! 可能限制性能的地方? 我们想知道这里还有什么可优化的地方。CPU? 内存? 硬盘? 网络?论文的作者认为网络带宽是瓶颈所在，所以他们尽量避免数据在网络间的移动。 MapReduce 详述运行流程 用户程序首先调用 MapReduce 库将输入文件分成 M 个数据片，每个数据片的大小一般从16MB 到 64MB(通过可选的参数可配置每个数据片的大小)。然后在集群中启动大量用户程序的副本。 这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 或 Reduce 任务分配给一个空闲的 worker。 被分配了 Map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key/value pair，然后把 key/value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key/value pair，并缓存在内存中。 缓存中的 key/value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的key/value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给Reduce worker。 当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。 Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。 当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对MapReduce 调用才返回。 在成功完成任务之后，MapReduce 的输出存放在 R 个输出文件中（对应每个 Reduce 任务产生一个输出文件，文件名由用户指定）。一般情况下，用户不需要将这 R 个输出文件合并成一个文件–他们经常把这些文件作为另外一个 MapReduce 的输入，或者在另外一个可以处理多个分割文件的分布式应用中使用。 我们此前提到的Word_Count例子也可用该流程图表达。 如何降低缓慢网络的影响? Map 输入从 GFS replica的本地磁盘中读取，而不是网络中。 中间数据经过仅经过网络一次。 Map worker写入本地磁盘，而不是GFS。 中间数据分成多个键的文件。 大规模的网络运输更有效。 如何获得负载均衡?有些应用执行所需的时间本身就比其他应用要长。解决方案：任务数多于worker数。Master分配新任务给完成先前任务的worker。所以，理想中没有因任务太大而占据太多完成时间的情况出现。更快的服务器做更多的工作，最终在相同时间完成。 如何进行容错?例如, 如果一台服务器在MR任务中崩溃了怎么办？隐藏失败是简化编程的一大组成部分!MR只需要重新运行失败的Map和Reduce任务。 为什么不需要从头开始整个工作？究其根源，是MR要求各个任务是纯函数: 他们不在调用里保持状态 他们不读或写文件, 除了预期的 MR 输入/输出 任务之间没有隐藏的通信。 如此便保证了重新执行(re-execution)会产生相同的结果。注意事项：对纯函数的要求是MR与其他并行编程方案相比主要的限制，但这对 MR 的简洁性至关重要。 故障恢复的细节 Map worker崩溃:master 发现 worker 不再回应 pings，即意识到worker崩溃。若崩溃的worker的中间Map输出损坏，但这部分数据可能被所有Reduce任务需要。此时master重新运行，将任务分配到其他GFS副本中。有些 Reduce workers 可能已经读取失败的worker的中间数据，这里我们需要有函数式(functional)和可确定(deterministic)的Map()!如果 Reduces已经获取了所有中间数据，master 不需要重新执行 Map。尽管若之后Reduce崩溃仍会导致失败的Map的重新执行。 Reduce worker 崩溃：已完成的任务不受影响，因它们已经被存储在了GFS上，并且有副本。master 重启 worker的未完成的任务在其他worker上。 Reduce worker 在构建输出文件的中途崩溃：GFS具有原子重命名功能，可防止输出在完成之前可见。所以master在其他地方重新运行Reduce任务是安全的。 其他问题 如果master分配两个workers同样的Map()任务怎么办?大概master错以为其中一个worker宕机了，它只会告诉 Reduce workers 其中的一个. 如果master分配两个workers同样的Reduce()任务怎么办?他们都将尝试写入同样的输出文件到GFS上。GFS的原子重命名可防止混淆;先完成的文件将是可见的。 如果一个 worker 十分缓慢怎么办(straggler)?也许是由于硬件的问题，master启动前几个任务的第二个副本。 如果一个 worker 由于 h/w 或 s/w 计算出错误的输出怎么办?MR 采取失效停止(fail-stop)策略评估CPUs和软件。 如果 master 崩溃怎么办?master可以周期性地建立备份，当master宕机后，可以从这些checkpoint中恢复过来，然而必须终止当期的mapreduce活动，用户需要重新开始任务。 对于哪些应用MapReduce效果不好?不是一切应用都适合map/shuffle/reduce模式。 数据量很小的时候。 小数据更新到大数据之中。 不可预料的读操作(Map和Reduce都不能选择输入) 多重洗牌(Multiple shuffles)。比如 page-rank，虽然可以用多重MR但不是最有效的。有更灵活的系统适用于该场景，当然模型也更复杂。 观感结论MapReduce凭一己之力让集群计算广受欢迎。 或许不是最有效、最灵活的。 可扩展性良好。 易于编程。错误和数据移动对程序员透明。 启发 限定编程模型使得并行和分布式计算非常容易，也易于构造容错的计算环境； 网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽； 多次执行相同的任务可以减少性能缓慢的机器带来的负面影响（即硬件配置的不平衡），同时解决了由于机器失效导致的数据丢失问题。 继承者MapReduce 这套分布式计算框架实现的主要局限在于： 用 MapReduce 写复杂的分析 pipeline 太麻烦。 它怎么改进都还是一个基于 batch mode 的框架。 MapReduce 的计算模型特别简单，只要分析任务稍微复杂一点，你就会发现一趟 MapReduce 是没法把事情做完了，你就得设计多个互相依赖的 MapReduce 任务，这就是所谓 pipeline。在数据流复杂的分析任务中，设计好的 pipeline 达到最高运行效率很困难，至于给 pipeline 调错更是复杂。 这时就需要用到 Flume 了。Flume 提供了一个抽象层次更高的 API，然后一个 planner 把 Flume 程序转换成若干个 MapReduce 任务去跑。其实Flume的思路没有多独特，它的编程模型很像微软的LINQ，本质上是一个编译器，把一个复杂的分析程序编译成一堆基本的 MapReduce 执行单元。多说一句，DryadLINQ 的计划算法也跟 Flume 异曲同工。另外其实自从有了 Dremel, 很多分析任务都可以直接用交互式查询来完成，写分析 pipeline 的时候也少了很多。 Google 还有很多这种基于 MapReduce 的封装： Tenzing : 把复杂的 SQL 查询转换（编译）成 MapReduce. Sawzall : 直接基于 MapReduce 模型的专用语言。 MillWheel:解决流计算的问题了。 MillWheel 的所谓流计算则跟函数式编程里的懒惰求值大有渊源。比如计算：1(map (fn [x] (* x 2)) (map (fn [x] (+ x 1)) data-list)) 最笨的做法就是先把 data-list 每项加 1，输出一个列表作为每项乘 2 的 map 任务的输入，然后再输出另一个列表，这就是传统的MapReduce实现。Clojure 利用 LazySeq 实现了对 map 的懒惰求值，可以做到「要一个算一个」：当要取上述结果的第一项时，它才去取 data-list 中的第一项，作加 1 和乘 2 操作然后输出，如此类推，就不是做完一个 map 再做另一个 map 了。 MillWheel 做的则是方向正好反过来的「来一个算一个」，data-list 里来一个输入就输出一个结果，每一步都不需要等上一步全部完成（数据流往往是无限的，没有「全部完成」的概念）。 比如计算：1(reduce + 0 (map (fn [x] (* x 2)) data-stream)) 在 MillWheel 里，就可以随着 data-stream 数据的涌入，实时显示当前的数据总和，而不是到 data-stream 结束时才输出一个结果，而且这样 x * 2 的中间结果也压根用不着存储下来。可以看到，具体怎么实现上述运算，是个具体实现的底层优化的问题，在概念上计算模型还是基本的 map 和 reduce，就好比同一条 SQL 查询语句可用于不同的执行引擎。 作为常用计算模型的 MapReduce 并没有什么被淘汰的可能。当然，MapReduce 不是唯一可用的计算模型，MillWheel 可以很方便地实现其他计算模型：基于图的计算框架 Pregel。 每个系统都有自己的历史地位，一篇论文，一个系统带给我们更多的是一种思路，以及更深层次的哲学层面的东西。而不是一个具体的系统实现。具体的系统实现可能会迭代、优化和被遗弃，但其背后的思想将作为人类文明中的宝贵财富传承下去。 MIT 6.824 Lab 1Preamble: Getting familiar with the sourceMapreduce包提供了一个简单的Map / Reduce库。应用程序通常应该调用Distributed()[位于master.go中]来启动一个作业，但是也可以调用Sequential()[也在master.go中]来获得调试的顺序执行。 执行流程与论文中的运行流程无异，但有了具体的函数名看起来更亲切一些。 该应用程序提供大量输入文件, 一个Map函数, 一个Reduce函数, nReduce个reduce tasks。 RPC服务器启动, 等该worker注册(Register)至RPC中. 任务处于可执行状态时, 调度器(Schedule)决定如何把task分配给worker和如何容错。 Master把每个输入文件作为一个Map任务, 每个任务至少调用一次doMap. 这些任务或者直接执行或者由DoTask RPC发射, 每个对doMap()的调用读取合适的文件, 对每个文件调用map, 对每个map文件最后产生nReduce个文件。 Master对每个reduce人物至少调用一次doReduce(). doReduce()收集由map产生的nReduce文件, 然后对这些文件运行reduce函数. 最终产生nReduce个中间文件。 Master调用mr.merge(), 合并所有的中间文件为一个输出文件。 Master发送Shutdown RPC关闭workers, 然后关闭RPC服务器。 Part I: Map/Reduce input and output本部分要实现DoMap()和DoReduce()两个函数。 DoMap功能描述:通过inFile文件名读取文件中的内容, 将内容传入Map函数中, 返回得到Key-value对数组。将Key-value pair数组通过Split函数(实验中的Split函数为ihash()), 平均分配到nReduce个中间文件中, nReduce名字可以通过reduceName构造出来。 注意事项：Key-Value写入文件需要用Json进行序列化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445func doMap( jobName string, mapTaskNumber int, inFile string, nReduce int, mapF func(file string, contents string) []KeyValue,) &#123; file, err := os.Open(inFile) if err != nil&#123; log.Fatal("Open file error: ", err) &#125; fileInfo, err := file.Stat() if err != nil&#123; log.Fatal("Get file info error: ", err) &#125; fileSize := fileInfo.Size() buf := make([]byte, fileSize) _, err = file.Read(buf) if err != nil&#123; log.Fatal("Read error: ", err) &#125; res := mapF(inFile, string(buf)) rSize := len(res) file.Close() // Generate Intermediate files for i := 0; i &lt; nReduce; i++ &#123; fileName := reduceName(jobName, mapTaskNumber, i) file, err := os.Create(fileName) if err != nil&#123; log.Fatal("Create intermediate file error:", err) &#125; enc := json.NewEncoder(file) for r := 0; r &lt; rSize; r++ &#123; kv := res[r] if ihash(kv.Key) % nReduce == i&#123; err := enc.Encode(&amp;kv) if err != nil&#123; log.Fatal("Encode error: ", kv) &#125; &#125; &#125; file.Close() &#125;&#125; DoReduce功能描述: Map生成nReduce个中间文件后, DoReduce遍历读取这些中间文件, 通过序列化器拿出所有的Key-Value pair, 然后将Key-value放入新的数据结构。为将所有key相同的value值合并, 自然想到使用Map数据结构。对所有的key进行排序, 生成有序的key数组. 然后对key-values(注意此处key对应的值为一个数组)进行Reduce操作, 并将结果写入新的文件(文件名由mergeName获得). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func doReduce( jobName string, reduceTaskNumber int, outFile string, nMap int, reduceF func(key string, values []string) string,) &#123; keyValues := make(map[string][]string) for i := 0; i &lt; nMap; i++ &#123; fileName := reduceName(jobName, i, reduceTaskNumber) file, err := os.Open(fileName) if err != nil&#123; log.Fatal("Open Error: ",fileName) &#125; decoder := json.NewDecoder(file) for&#123; var kv KeyValue err := decoder.Decode(&amp;kv) if err != nil&#123; break &#125; _, ok := keyValues[kv.Key] if !ok&#123; keyValues[kv.Key] = make([]string, 0) &#125; keyValues[kv.Key] = append(keyValues[kv.Key], kv.Value) &#125; file.Close() &#125; var keys []string for k, _ := range keyValues &#123; keys = append(keys, k) &#125; sort.Strings(keys) mergeFileName := mergeName(jobName, reduceTaskNumber) file, err := os.Create(mergeFileName) if err != nil&#123; log.Fatal("Create file error: ", err) &#125; enc := json.NewEncoder(file) for _, k := range keys&#123; res := reduceF(k, keyValues[k]) enc.Encode(&amp;KeyValue&#123;k, res&#125;) &#125; file.Close() &#125; Part II: Single-worker word count本部分将完成一个简单的Map-Reduce词频统计的任务。任务描述：对输入文件内容进行分词, 然后将词发射出去(词频默认为1), Reduce将values进行求和即可。 1234567891011121314151617181920212223func mapF(filename string, contents string) []mapreduce.KeyValue &#123; var res []mapreduce.KeyValue values := strings.FieldsFunc(contents, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, v := range values&#123; res = append(res, mapreduce.KeyValue&#123;v,"1"&#125;) &#125; return res&#125;func reduceF(key string, values []string) string &#123; var count int = 0 for _,v := range values&#123; intValue, err := strconv.Atoi(v) if err != nil&#123; fmt.Printf("%v make error: %v\n", v, err) &#125; count += intValue &#125; return strconv.Itoa(count)&#125; Part III: Distributing MapReduce tasks本部分将实现分布式MapReduce的调度模块。执行流程:启动一个Master RPC服务器, 服务器调用schedule来调用Map/Reduce任务;启动多个Worker RPC服务器, 并将Worker的端口信息注册到Master服务器的数据结构(channel)中。 Schedule: 负责整个MapReduce任务的调度, 查找当前可用Worker, 然后通过worker来执行Map/Reduce任务。 注意事项: 应该保证Schedule中所有的goroutine全部完成后才能返回. 所以应该使函数阻塞直到所有的goroutine完成。 Part IV: Handling worker failuresMaster来处理失败的workers, 当某worker上的Map/Reduce任务失败后, 需要将这个任务转移给其他worker来执行。 在设计调度任务函数schedule()的时候考虑容错性, 判断在Worker上调用RPC是否成功, 若失败则重新分配一个新的worker服务器来处理task。整个容错逻辑可以放到一个for循环中, 只有当任务成功调用才break跳出循环。 1234567891011121314151617181920212223for i := 0; i &lt; ntasks; i++&#123; waitGroup.Add(1) go func(TaskNumber int, n_other int, phase jobPhase)&#123; defer waitGroup.Done() for &#123; worker := &lt;- registerChan var args DoTaskArgs args.JobName = jobName args.File = mapFiles[TaskNumber] args.Phase = phase args.TaskNumber = TaskNumber args.NumOtherPhase = n_other ok := call(worker, "Worker.DoTask", &amp;args, new(struct&#123;&#125;)) if ok &#123; go func()&#123; registerChan &lt;- worker &#125;() break &#125; &#125; &#125;(i, n_other, phase) &#125; Part V: Inverted index generation本部分将构建一个倒排索引Map/Reduce任务。 Map任务描述:拿到一个网页URL和URL对应网页文本, 对网页文本进行分词, 将每个词作为key, 网页URL作为value发射出去。 123456789func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; values := strings.FieldsFunc(value, func(c rune) bool&#123; return !unicode.IsLetter(c) &#125;) for _, word := range values&#123; res = append(res, mapreduce.KeyValue&#123;word, document&#125;) &#125; return res&#125; Reduce任务描述：拿到一个关键词key, 和关键词对应的URL集合, 首先对URL进行去重(可能一个URL中出现多次关键词), 然后对URL进行排序(可以不排序), 根据需要的结构对整个URL集合作拼接(URL集合的长度即为URL中出现关键词的URL个数), 最后将关键词和拼接字符串发射出去。 123456789101112131415func reduceF(key string, values []string) string &#123; var buffer bytes.Buffer values = deleteDuplicates(values) sort.Strings(values) size := len(values) for index, value := range values&#123; buffer.WriteString(value) if index != (size - 1)&#123; buffer.WriteString(",") &#125; &#125; return strconv.Itoa(size) + " " + buffer.String()&#125; 去重的算法大家当然是轻车熟路了： 12345678910111213func deleteDuplicates(values []string) []string&#123; var res []string valuesMap := make(map[string]bool) for _, v := range values&#123; if _, ok := valuesMap[v]; !ok&#123; valuesMap[v] = true res = append(res, v) &#125;else&#123; continue &#125; &#125; return res&#125;]]></content>
      <tags>
        <tag>Golang</tag>
        <tag>分布式系统</tag>
        <tag>Mit 6.824</tag>
      </tags>
  </entry>
</search>
